{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.0"
    },
    "colab": {
      "name": "Copy of Linear classifier.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "Pj_fLfPexS_z",
        "quad_68RxTAV",
        "BjGBTeDXxTAZ"
      ]
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pj_fLfPexS_z"
      },
      "source": [
        "# Задание 1.2 - Линейный классификатор (Linear classifier)\n",
        "\n",
        "В этом задании мы реализуем другую модель машинного обучения - линейный классификатор. Линейный классификатор подбирает для каждого класса веса, на которые нужно умножить значение каждого признака и потом сложить вместе.\n",
        "Тот класс, у которого эта сумма больше, и является предсказанием модели.\n",
        "\n",
        "В этом задании вы:\n",
        "- потренируетесь считать градиенты различных многомерных функций\n",
        "- реализуете подсчет градиентов через линейную модель и функцию потерь softmax\n",
        "- реализуете процесс тренировки линейного классификатора\n",
        "- подберете параметры тренировки на практике\n",
        "\n",
        "На всякий случай, еще раз ссылка на туториал по numpy:  \n",
        "http://cs231n.github.io/python-numpy-tutorial/"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nguxxJ9Jxt2r",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "87e1547e-99bf-4ac5-f72c-fff064975589"
      },
      "source": [
        "!git clone https://github.com/sim0nsays/dlcourse_ai.git\n",
        "!cp dlcourse_ai/assignments/assignment1/*.py .\n",
        "!dlcourse_ai/assignments/assignment1/download_data.sh\n"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "fatal: destination path 'dlcourse_ai' already exists and is not an empty directory.\n",
            "mkdir: cannot create directory ‘data’: File exists\n",
            "--2021-03-02 10:30:19--  http://ufldl.stanford.edu/housenumbers/train_32x32.mat\n",
            "Resolving ufldl.stanford.edu (ufldl.stanford.edu)... 171.64.68.10\n",
            "Connecting to ufldl.stanford.edu (ufldl.stanford.edu)|171.64.68.10|:80... connected.\n",
            "HTTP request sent, awaiting response... 416 Requested Range Not Satisfiable\n",
            "\n",
            "    The file is already fully retrieved; nothing to do.\n",
            "\n",
            "--2021-03-02 10:30:19--  http://ufldl.stanford.edu/housenumbers/test_32x32.mat\n",
            "Reusing existing connection to ufldl.stanford.edu:80.\n",
            "HTTP request sent, awaiting response... 416 Requested Range Not Satisfiable\n",
            "\n",
            "    The file is already fully retrieved; nothing to do.\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z0m6pQbvxTAQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4f6b092f-8a18-4d42-f5f3-a4a0760fd20c"
      },
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "%matplotlib inline\n",
        "\n",
        "%load_ext autoreload\n",
        "%autoreload 2"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The autoreload extension is already loaded. To reload it, use:\n",
            "  %reload_ext autoreload\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lYQ9yBaMxTAV"
      },
      "source": [
        "from dataset import load_svhn, random_split_train_val\n",
        "from gradient_check import check_gradient\n",
        "from metrics import multiclass_accuracy \n",
        "import linear_classifer"
      ],
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "quad_68RxTAV"
      },
      "source": [
        "# Как всегда, первым делом загружаем данные\n",
        "\n",
        "Мы будем использовать все тот же SVHN."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gvvKgHxtxTAX"
      },
      "source": [
        "def prepare_for_linear_classifier(train_X, test_X):\n",
        "    train_flat = train_X.reshape(train_X.shape[0], -1).astype(np.float) / 255.0\n",
        "    test_flat = test_X.reshape(test_X.shape[0], -1).astype(np.float) / 255.0\n",
        "    \n",
        "    # Subtract mean\n",
        "    mean_image = np.mean(train_flat, axis = 0)\n",
        "    train_flat -= mean_image\n",
        "    test_flat -= mean_image\n",
        "    \n",
        "    # Add another channel with ones as a bias term\n",
        "    train_flat_with_ones = np.hstack([train_flat, np.ones((train_X.shape[0], 1))])\n",
        "    test_flat_with_ones = np.hstack([test_flat, np.ones((test_X.shape[0], 1))])    \n",
        "    return train_flat_with_ones, test_flat_with_ones\n",
        "    \n",
        "train_X, train_y, test_X, test_y = load_svhn(\"data\", max_train=10000, max_test=1000)    \n",
        "train_X, test_X = prepare_for_linear_classifier(train_X, test_X)\n",
        "# Split train into train and val\n",
        "train_X, train_y, val_X, val_y = random_split_train_val(train_X, train_y, num_val = 1000)"
      ],
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BjGBTeDXxTAZ"
      },
      "source": [
        "# Играемся с градиентами!\n",
        "\n",
        "В этом курсе мы будем писать много функций, которые вычисляют градиенты аналитическим методом.\n",
        "\n",
        "Все функции, в которых мы будем вычислять градиенты, будут написаны по одной и той же схеме.  \n",
        "Они будут получать на вход точку, где нужно вычислить значение и градиент функции, а на выходе будут выдавать кортеж (tuple) из двух значений - собственно значения функции в этой точке (всегда одно число) и аналитического значения градиента в той же точке (той же размерности, что и вход).\n",
        "```\n",
        "def f(x):\n",
        "    \"\"\"\n",
        "    Computes function and analytic gradient at x\n",
        "    \n",
        "    x: np array of float, input to the function\n",
        "    \n",
        "    Returns:\n",
        "    value: float, value of the function \n",
        "    grad: np array of float, same shape as x\n",
        "    \"\"\"\n",
        "    ...\n",
        "    \n",
        "    return value, grad\n",
        "```\n",
        "\n",
        "Необходимым инструментом во время реализации кода, вычисляющего градиенты, является функция его проверки. Эта функция вычисляет градиент численным методом и сверяет результат с градиентом, вычисленным аналитическим методом.\n",
        "\n",
        "Мы начнем с того, чтобы реализовать вычисление численного градиента (numeric gradient) в функции `check_gradient` в `gradient_check.py`. Эта функция будет принимать на вход функции формата, заданного выше, использовать значение `value` для вычисления численного градиента и сравнит его с аналитическим - они должны сходиться.\n",
        "\n",
        "Напишите часть функции, которая вычисляет градиент с помощью численной производной для каждой координаты. Для вычисления производной используйте так называемую two-point formula (https://en.wikipedia.org/wiki/Numerical_differentiation):\n",
        "\n",
        "![image](https://wikimedia.org/api/rest_v1/media/math/render/svg/22fc2c0a66c63560a349604f8b6b39221566236d)\n",
        "\n",
        "Все функции приведенные в следующей клетке должны проходить gradient check."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "1eykGCRjxTAb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a03b6237-20f3-4466-d0bd-b3cd902ef6ff"
      },
      "source": [
        "# TODO: Implement check_gradient function in gradient_check.py\n",
        "# All the functions below should pass the gradient check\n",
        "\n",
        "def square(x):\n",
        "    return float(x*x), 2*x\n",
        "\n",
        "check_gradient(square, np.array([3.0]))\n",
        "\n",
        "def array_sum(x):\n",
        "    assert x.shape == (2,), x.shape\n",
        "    return np.sum(x), np.ones_like(x)\n",
        "\n",
        "check_gradient(array_sum, np.array([3.0, 2.0]))\n",
        "\n",
        "def array_2d_sum(x):\n",
        "    assert x.shape == (2,2)\n",
        "    return np.sum(x), np.ones_like(x)\n",
        "\n",
        "check_gradient(array_2d_sum, np.array([[3.0, 2.0], [1.0, 0.0]]))"
      ],
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Gradient check passed!\n",
            "Gradient check passed!\n",
            "Gradient check passed!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 49
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-3bwzmcCxTAc"
      },
      "source": [
        "## Начинаем писать свои функции, считающие аналитический градиент\n",
        "\n",
        "Теперь реализуем функцию softmax, которая получает на вход оценки для каждого класса и преобразует их в вероятности от 0 до 1:\n",
        "![image](https://wikimedia.org/api/rest_v1/media/math/render/svg/e348290cf48ddbb6e9a6ef4e39363568b67c09d3)\n",
        "\n",
        "**Важно:** Практический аспект вычисления этой функции заключается в том, что в ней участвует вычисление экспоненты от потенциально очень больших чисел - это может привести к очень большим значениям в числителе и знаменателе за пределами диапазона float.\n",
        "\n",
        "К счастью, у этой проблемы есть простое решение -- перед вычислением softmax вычесть из всех оценок максимальное значение среди всех оценок:\n",
        "```\n",
        "predictions -= np.max(predictions)\n",
        "```\n",
        "(подробнее здесь - http://cs231n.github.io/linear-classify/#softmax, секция `Practical issues: Numeric stability`)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C3S4_hcbxTAf"
      },
      "source": [
        "# TODO Implement softmax and cross-entropy for single sample\n",
        "probs = linear_classifer.softmax(np.array([-10, 0, 10]))\n",
        "\n",
        "# Make sure it works for big numbers too!\n",
        "probs = linear_classifer.softmax(np.array([1000, 0, 0]))\n",
        "assert np.isclose(probs[0], 1.0)"
      ],
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VXc3bQFbxTAg"
      },
      "source": [
        "Кроме этого, мы реализуем cross-entropy loss, которую мы будем использовать как функцию ошибки (error function).\n",
        "В общем виде cross-entropy определена следующим образом:\n",
        "![image](https://wikimedia.org/api/rest_v1/media/math/render/svg/0cb6da032ab424eefdca0884cd4113fe578f4293)\n",
        "\n",
        "где x - все классы, p(x) - истинная вероятность принадлежности сэмпла классу x, а q(x) - вероятность принадлежности классу x, предсказанная моделью.  \n",
        "В нашем случае сэмпл принадлежит только одному классу, индекс которого передается функции. Для него p(x) равна 1, а для остальных классов - 0. \n",
        "\n",
        "Это позволяет реализовать функцию проще!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ixWKSuvHxTAh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2e90e088-d629-4acf-cc18-b1ba32b17201"
      },
      "source": [
        "probs = linear_classifer.softmax(np.array([-5, 0, 5]))\n",
        "linear_classifer.cross_entropy_loss(probs, 1)"
      ],
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "5.006760443547122"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 51
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9C1gPAf-xTAj"
      },
      "source": [
        "После того как мы реализовали сами функции, мы можем реализовать градиент.\n",
        "\n",
        "Оказывается, что вычисление градиента становится гораздо проще, если объединить эти функции в одну, которая сначала вычисляет вероятности через softmax, а потом использует их для вычисления функции ошибки через cross-entropy loss.\n",
        "\n",
        "Эта функция `softmax_with_cross_entropy` будет возвращает и значение ошибки, и градиент по входным параметрам. Мы проверим корректность реализации с помощью `check_gradient`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4ZIJLUPaxTAk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a455cb03-06e3-4cc5-c039-759987c699bb"
      },
      "source": [
        "# TODO Implement combined function or softmax and cross entropy and produces gradient\n",
        "#loss, grad = linear_classifer.softmax_with_cross_entropy(np.array([1, 0, 0]), 1)\n",
        "check_gradient(lambda x: linear_classifer.softmax_with_cross_entropy(x, 1), np.array([1, 0, 0], np.float))"
      ],
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Gradient check passed!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 52
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MIDP5y80xTAm"
      },
      "source": [
        "В качестве метода тренировки мы будем использовать стохастический градиентный спуск (stochastic gradient descent или SGD), который работает с батчами сэмплов. \n",
        "\n",
        "Поэтому все наши фукнции будут получать не один пример, а батч, то есть входом будет не вектор из `num_classes` оценок, а матрица размерности `batch_size, num_classes`. Индекс примера в батче всегда будет первым измерением.\n",
        "\n",
        "Следующий шаг - переписать наши функции так, чтобы они поддерживали батчи.\n",
        "\n",
        "Финальное значение функции ошибки должно остаться числом, и оно равно среднему значению ошибки среди всех примеров в батче."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O6P125m3jovU",
        "outputId": "6cfead6f-7c31-486a-ed03-538f66470c37"
      },
      "source": [
        "a = np.array([[10, 30, 20], [60, 40, 50]])\n",
        "b = np.array([0, 2])\n",
        "y_true = np.zeros(a.shape)\n",
        "if a.ndim == 1:\n",
        "    y_true[target_index] = 1\n",
        "else:\n",
        "    #ai = np.expand_dims(b, axis=1)\n",
        "    ai = b.reshape(-1, 1)\n",
        "    np.put_along_axis(y_true, ai, 1, axis=1)\n",
        "    #print(ai)\n",
        "    #print(y_true)\n",
        "\n",
        "#loss = -np.sum(np.log(np.take_along_axis(a, ai, axis=1))) / probs.shape[0]\n",
        "#print(loss)\n",
        "d = np.array([[20,0,0], [1000, 0, 0]])\n",
        "normalized_predictions = d - np.max(d, axis=1)[:, None]\n",
        "pr = np.apply_along_axis(lambda x: np.exp(x) / np.sum(np.exp(x)), 1, normalized_predictions)\n",
        "print(normalized_predictions)\n",
        "print(pr)\n"
      ],
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[    0   -20   -20]\n",
            " [    0 -1000 -1000]]\n",
            "[[9.99999996e-01 2.06115361e-09 2.06115361e-09]\n",
            " [1.00000000e+00 0.00000000e+00 0.00000000e+00]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": false,
        "id": "QFKngK5rxTAo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7d96d9fe-ff34-41cc-f233-0bb69a7f9c3b"
      },
      "source": [
        "# TODO Extend combined function so it can receive a 2d array with batch of samples\n",
        "np.random.seed(42)\n",
        "# Test batch_size = 1\n",
        "num_classes = 4\n",
        "batch_size = 1\n",
        "predictions = np.random.randint(-1, 3, size=(batch_size, num_classes)).astype(np.float)\n",
        "target_index = np.random.randint(0, num_classes, size=(batch_size, 1)).astype(np.int)\n",
        "check_gradient(lambda x: linear_classifer.softmax_with_cross_entropy(x, target_index), predictions)\n",
        "\n",
        "# Test batch_size = 3\n",
        "num_classes = 4\n",
        "batch_size = 3\n",
        "predictions = np.random.randint(-1, 3, size=(batch_size, num_classes)).astype(np.float)\n",
        "target_index = np.random.randint(0, num_classes, size=(batch_size, 1)).astype(np.int)\n",
        "check_gradient(lambda x: linear_classifer.softmax_with_cross_entropy(x, target_index), predictions)\n",
        "\n",
        "# Make sure maximum subtraction for numberic stability is done separately for every sample in the batch\n",
        "probs = linear_classifer.softmax(np.array([[20,0,0], [1000, 0, 0]]))\n",
        "assert np.all(np.isclose(probs[:, 0], 1.0))"
      ],
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Gradient check passed!\n",
            "Gradient check passed!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3DXkkUI5xTAu"
      },
      "source": [
        "### Наконец, реализуем сам линейный классификатор!\n",
        "\n",
        "softmax и cross-entropy получают на вход оценки, которые выдает линейный классификатор.\n",
        "\n",
        "Он делает это очень просто: для каждого класса есть набор весов, на которые надо умножить пиксели картинки и сложить. Получившееся число и является оценкой класса, идущей на вход softmax.\n",
        "\n",
        "Таким образом, линейный классификатор можно представить как умножение вектора с пикселями на матрицу W размера `num_features, num_classes`. Такой подход легко расширяется на случай батча векторов с пикселями X размера `batch_size, num_features`:\n",
        "\n",
        "`predictions = X * W`, где `*` - матричное умножение.\n",
        "\n",
        "Реализуйте функцию подсчета линейного классификатора и градиентов по весам `linear_softmax` в файле `linear_classifer.py`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FhEpY4o8xTAv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b704517a-4040-4a91-f904-191c6b340dbe"
      },
      "source": [
        "# TODO Implement linear_softmax function that uses softmax with cross-entropy for linear classifier\n",
        "batch_size = 2\n",
        "num_classes = 2\n",
        "num_features = 3\n",
        "np.random.seed(42)\n",
        "W = np.random.randint(-1, 3, size=(num_features, num_classes)).astype(np.float)\n",
        "X = np.random.randint(-1, 3, size=(batch_size, num_features)).astype(np.float)\n",
        "target_index = np.ones(batch_size, dtype=np.int)\n",
        "\n",
        "loss, dW = linear_classifer.linear_softmax(X, W, target_index)\n",
        "check_gradient(lambda w: linear_classifer.linear_softmax(X, w, target_index), W)"
      ],
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Gradient check passed!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 55
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rVzGSjPLxTAz"
      },
      "source": [
        "### И теперь регуляризация\n",
        "\n",
        "Мы будем использовать L2 regularization для весов как часть общей функции ошибки.\n",
        "\n",
        "Напомним, L2 regularization определяется как\n",
        "\n",
        "l2_reg_loss = regularization_strength * sum<sub>ij</sub> W[i, j]<sup>2</sup>\n",
        "\n",
        "Реализуйте функцию для его вычисления и вычисления соотвествующих градиентов."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U8DwT2vDxTA0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "92d174c9-0a16-4658-a114-8e1a0be8eba1"
      },
      "source": [
        "# TODO Implement l2_regularization function that implements loss for L2 regularization\n",
        "linear_classifer.l2_regularization(W, 0.01)\n",
        "check_gradient(lambda w: linear_classifer.l2_regularization(w, 0.01), W)"
      ],
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Gradient check passed!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 56
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5mfTaJyZxTA1"
      },
      "source": [
        "# Тренировка!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VdI7GJ42xTA2"
      },
      "source": [
        "Градиенты в порядке, реализуем процесс тренировки!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": false,
        "id": "F5jWO5j7xTA2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5df560ee-8d87-480a-c6d0-8a1ed6a94d99"
      },
      "source": [
        "# TODO: Implement LinearSoftmaxClassifier.fit function\n",
        "classifier = linear_classifer.LinearSoftmaxClassifier()\n",
        "loss_history = classifier.fit(train_X, train_y, epochs=10, learning_rate=1e-3, batch_size=300, reg=1e1)"
      ],
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 0, loss: 2.397784\n",
            "Epoch 1, loss: 2.330238\n",
            "Epoch 2, loss: 2.310605\n",
            "Epoch 3, loss: 2.304842\n",
            "Epoch 4, loss: 2.302948\n",
            "Epoch 5, loss: 2.301829\n",
            "Epoch 6, loss: 2.302589\n",
            "Epoch 7, loss: 2.300861\n",
            "Epoch 8, loss: 2.302606\n",
            "Epoch 9, loss: 2.301903\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YmRkGByFxTA2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 282
        },
        "outputId": "fb1cb7ae-0c9b-494d-cae7-bd263ef56ea5"
      },
      "source": [
        "# let's look at the loss history!\n",
        "plt.plot(loss_history)"
      ],
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[<matplotlib.lines.Line2D at 0x7f7851301c50>]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 66
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uOpFYgPoxTA4"
      },
      "source": [
        "# Let's check how it performs on validation set\n",
        "pred = classifier.predict(val_X)\n",
        "accuracy = multiclass_accuracy(pred, val_y)\n",
        "print(\"Accuracy: \", accuracy)\n",
        "\n",
        "# Now, let's train more and see if it performs better\n",
        "classifier.fit(train_X, train_y, epochs=100, learning_rate=1e-3, batch_size=300, reg=1e1)\n",
        "pred = classifier.predict(val_X)\n",
        "accuracy = multiclass_accuracy(pred, val_y)\n",
        "print(\"Accuracy after training for 100 epochs: \", accuracy)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lcj8OZGIxTA4"
      },
      "source": [
        "### Как и раньше, используем кросс-валидацию для подбора гиперпараметтов.\n",
        "\n",
        "В этот раз, чтобы тренировка занимала разумное время, мы будем использовать только одно разделение на тренировочные (training) и проверочные (validation) данные.\n",
        "\n",
        "Теперь нам нужно подобрать не один, а два гиперпараметра! Не ограничивайте себя изначальными значениями в коде.  \n",
        "Добейтесь точности более чем **20%** на проверочных данных (validation data)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zZW1PLH5xTA5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c0aa91bc-2995-459b-9f3a-6c35c21725b7"
      },
      "source": [
        "num_epochs = 200\n",
        "batch_size = 300\n",
        "\n",
        "learning_rates = [1e-2, 1e-3, 1e-4, 1e-5]\n",
        "reg_strengths = [1e-3, 1e-4, 1e-5, 1e-6]\n",
        "\n",
        "best_classifier = None\n",
        "best_val_accuracy = 0\n",
        "best_rate = 0\n",
        "best_reg = 0\n",
        "\n",
        "# TODO use validation set to find the best hyperparameters\n",
        "# hint: for best results, you might need to try more values for learning rate and regularization strength \n",
        "# than provided initially\n",
        "for learning_rate in learning_rates:\n",
        "    for reg in reg_strengths:\n",
        "        classifier = linear_classifer.LinearSoftmaxClassifier()\n",
        "        loss_history = classifier.fit(train_X, train_y, epochs=num_epochs, learning_rate=learning_rate, batch_size=batch_size, reg=reg)\n",
        "        pred = classifier.predict(val_X)\n",
        "        accuracy = multiclass_accuracy(pred, val_y)\n",
        "        if (accuracy > best_val_accuracy):\n",
        "            best_rate = learning_rate\n",
        "            best_reg = reg\n",
        "            best_val_accuracy = accuracy\n",
        "            best_classifier = classifier\n",
        "\n",
        "        \n",
        "\n",
        "print('best validation accuracy achieved: %f' % best_val_accuracy)\n",
        "print('rate = ', best_rate, \", \", 'reg = ', reg)"
      ],
      "execution_count": 76,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 0, loss: 2.296225\n",
            "Epoch 1, loss: 2.285822\n",
            "Epoch 2, loss: 2.277143\n",
            "Epoch 3, loss: 2.284636\n",
            "Epoch 4, loss: 2.263372\n",
            "Epoch 5, loss: 2.264473\n",
            "Epoch 6, loss: 2.254315\n",
            "Epoch 7, loss: 2.244687\n",
            "Epoch 8, loss: 2.255019\n",
            "Epoch 9, loss: 2.237249\n",
            "Epoch 10, loss: 2.245903\n",
            "Epoch 11, loss: 2.233116\n",
            "Epoch 12, loss: 2.217045\n",
            "Epoch 13, loss: 2.218506\n",
            "Epoch 14, loss: 2.242925\n",
            "Epoch 15, loss: 2.200259\n",
            "Epoch 16, loss: 2.202640\n",
            "Epoch 17, loss: 2.188990\n",
            "Epoch 18, loss: 2.186423\n",
            "Epoch 19, loss: 2.216784\n",
            "Epoch 20, loss: 2.226010\n",
            "Epoch 21, loss: 2.215899\n",
            "Epoch 22, loss: 2.223026\n",
            "Epoch 23, loss: 2.177977\n",
            "Epoch 24, loss: 2.182215\n",
            "Epoch 25, loss: 2.187831\n",
            "Epoch 26, loss: 2.174118\n",
            "Epoch 27, loss: 2.200575\n",
            "Epoch 28, loss: 2.197892\n",
            "Epoch 29, loss: 2.196088\n",
            "Epoch 30, loss: 2.143454\n",
            "Epoch 31, loss: 2.206296\n",
            "Epoch 32, loss: 2.207268\n",
            "Epoch 33, loss: 2.161505\n",
            "Epoch 34, loss: 2.166010\n",
            "Epoch 35, loss: 2.132767\n",
            "Epoch 36, loss: 2.140164\n",
            "Epoch 37, loss: 2.197453\n",
            "Epoch 38, loss: 2.245789\n",
            "Epoch 39, loss: 2.206524\n",
            "Epoch 40, loss: 2.171785\n",
            "Epoch 41, loss: 2.134235\n",
            "Epoch 42, loss: 2.164824\n",
            "Epoch 43, loss: 2.169707\n",
            "Epoch 44, loss: 2.153875\n",
            "Epoch 45, loss: 2.195305\n",
            "Epoch 46, loss: 2.150231\n",
            "Epoch 47, loss: 2.111954\n",
            "Epoch 48, loss: 2.164145\n",
            "Epoch 49, loss: 2.190892\n",
            "Epoch 50, loss: 2.113080\n",
            "Epoch 51, loss: 2.147190\n",
            "Epoch 52, loss: 2.171885\n",
            "Epoch 53, loss: 2.150487\n",
            "Epoch 54, loss: 2.182363\n",
            "Epoch 55, loss: 2.139997\n",
            "Epoch 56, loss: 2.153395\n",
            "Epoch 57, loss: 2.150366\n",
            "Epoch 58, loss: 2.148536\n",
            "Epoch 59, loss: 2.187516\n",
            "Epoch 60, loss: 2.140212\n",
            "Epoch 61, loss: 2.147525\n",
            "Epoch 62, loss: 2.101170\n",
            "Epoch 63, loss: 2.179459\n",
            "Epoch 64, loss: 2.169921\n",
            "Epoch 65, loss: 2.185988\n",
            "Epoch 66, loss: 2.155098\n",
            "Epoch 67, loss: 2.153183\n",
            "Epoch 68, loss: 2.137140\n",
            "Epoch 69, loss: 2.082833\n",
            "Epoch 70, loss: 2.142954\n",
            "Epoch 71, loss: 2.123540\n",
            "Epoch 72, loss: 2.131719\n",
            "Epoch 73, loss: 2.128063\n",
            "Epoch 74, loss: 2.130405\n",
            "Epoch 75, loss: 2.149827\n",
            "Epoch 76, loss: 2.116323\n",
            "Epoch 77, loss: 2.138408\n",
            "Epoch 78, loss: 2.126386\n",
            "Epoch 79, loss: 2.164964\n",
            "Epoch 80, loss: 2.141103\n",
            "Epoch 81, loss: 2.152464\n",
            "Epoch 82, loss: 2.144908\n",
            "Epoch 83, loss: 2.148833\n",
            "Epoch 84, loss: 2.076838\n",
            "Epoch 85, loss: 2.118379\n",
            "Epoch 86, loss: 2.141507\n",
            "Epoch 87, loss: 2.081365\n",
            "Epoch 88, loss: 2.088322\n",
            "Epoch 89, loss: 2.134331\n",
            "Epoch 90, loss: 2.148892\n",
            "Epoch 91, loss: 2.111829\n",
            "Epoch 92, loss: 2.163128\n",
            "Epoch 93, loss: 2.059160\n",
            "Epoch 94, loss: 2.098542\n",
            "Epoch 95, loss: 2.153951\n",
            "Epoch 96, loss: 2.140928\n",
            "Epoch 97, loss: 2.166929\n",
            "Epoch 98, loss: 2.139729\n",
            "Epoch 99, loss: 2.132660\n",
            "Epoch 100, loss: 2.136182\n",
            "Epoch 101, loss: 2.176353\n",
            "Epoch 102, loss: 2.148785\n",
            "Epoch 103, loss: 2.101964\n",
            "Epoch 104, loss: 2.140672\n",
            "Epoch 105, loss: 2.181579\n",
            "Epoch 106, loss: 2.080618\n",
            "Epoch 107, loss: 2.114688\n",
            "Epoch 108, loss: 2.113998\n",
            "Epoch 109, loss: 2.126941\n",
            "Epoch 110, loss: 2.119265\n",
            "Epoch 111, loss: 2.147652\n",
            "Epoch 112, loss: 2.118139\n",
            "Epoch 113, loss: 2.120970\n",
            "Epoch 114, loss: 2.125606\n",
            "Epoch 115, loss: 2.181622\n",
            "Epoch 116, loss: 2.204016\n",
            "Epoch 117, loss: 2.119388\n",
            "Epoch 118, loss: 2.118308\n",
            "Epoch 119, loss: 2.140813\n",
            "Epoch 120, loss: 2.116436\n",
            "Epoch 121, loss: 2.110504\n",
            "Epoch 122, loss: 2.147998\n",
            "Epoch 123, loss: 2.103256\n",
            "Epoch 124, loss: 2.195500\n",
            "Epoch 125, loss: 2.160824\n",
            "Epoch 126, loss: 2.127610\n",
            "Epoch 127, loss: 2.140088\n",
            "Epoch 128, loss: 2.177569\n",
            "Epoch 129, loss: 2.110317\n",
            "Epoch 130, loss: 2.132995\n",
            "Epoch 131, loss: 2.143378\n",
            "Epoch 132, loss: 2.158131\n",
            "Epoch 133, loss: 2.118979\n",
            "Epoch 134, loss: 2.099559\n",
            "Epoch 135, loss: 2.186618\n",
            "Epoch 136, loss: 2.131994\n",
            "Epoch 137, loss: 2.125187\n",
            "Epoch 138, loss: 2.156238\n",
            "Epoch 139, loss: 2.078866\n",
            "Epoch 140, loss: 2.151215\n",
            "Epoch 141, loss: 2.154539\n",
            "Epoch 142, loss: 2.171192\n",
            "Epoch 143, loss: 2.135631\n",
            "Epoch 144, loss: 2.172431\n",
            "Epoch 145, loss: 2.087362\n",
            "Epoch 146, loss: 2.121876\n",
            "Epoch 147, loss: 2.122250\n",
            "Epoch 148, loss: 2.125459\n",
            "Epoch 149, loss: 2.079041\n",
            "Epoch 150, loss: 2.055302\n",
            "Epoch 151, loss: 2.073698\n",
            "Epoch 152, loss: 2.092730\n",
            "Epoch 153, loss: 2.089307\n",
            "Epoch 154, loss: 2.073148\n",
            "Epoch 155, loss: 2.113257\n",
            "Epoch 156, loss: 2.088315\n",
            "Epoch 157, loss: 2.172517\n",
            "Epoch 158, loss: 2.150270\n",
            "Epoch 159, loss: 2.086684\n",
            "Epoch 160, loss: 2.089186\n",
            "Epoch 161, loss: 2.077533\n",
            "Epoch 162, loss: 2.145604\n",
            "Epoch 163, loss: 2.116592\n",
            "Epoch 164, loss: 2.139325\n",
            "Epoch 165, loss: 2.075940\n",
            "Epoch 166, loss: 2.124019\n",
            "Epoch 167, loss: 2.165995\n",
            "Epoch 168, loss: 2.092075\n",
            "Epoch 169, loss: 2.138291\n",
            "Epoch 170, loss: 2.142742\n",
            "Epoch 171, loss: 2.032626\n",
            "Epoch 172, loss: 2.117556\n",
            "Epoch 173, loss: 2.054325\n",
            "Epoch 174, loss: 2.105744\n",
            "Epoch 175, loss: 2.073045\n",
            "Epoch 176, loss: 2.061047\n",
            "Epoch 177, loss: 2.094334\n",
            "Epoch 178, loss: 2.076200\n",
            "Epoch 179, loss: 2.113165\n",
            "Epoch 180, loss: 2.116197\n",
            "Epoch 181, loss: 2.102010\n",
            "Epoch 182, loss: 2.098836\n",
            "Epoch 183, loss: 2.059982\n",
            "Epoch 184, loss: 2.071252\n",
            "Epoch 185, loss: 2.159657\n",
            "Epoch 186, loss: 2.107934\n",
            "Epoch 187, loss: 2.119599\n",
            "Epoch 188, loss: 2.130150\n",
            "Epoch 189, loss: 2.064500\n",
            "Epoch 190, loss: 2.117068\n",
            "Epoch 191, loss: 2.114922\n",
            "Epoch 192, loss: 2.129115\n",
            "Epoch 193, loss: 2.116557\n",
            "Epoch 194, loss: 2.125670\n",
            "Epoch 195, loss: 1.986178\n",
            "Epoch 196, loss: 2.115144\n",
            "Epoch 197, loss: 2.086326\n",
            "Epoch 198, loss: 2.168737\n",
            "Epoch 199, loss: 2.164545\n",
            "Epoch 0, loss: 2.295740\n",
            "Epoch 1, loss: 2.286367\n",
            "Epoch 2, loss: 2.275351\n",
            "Epoch 3, loss: 2.272959\n",
            "Epoch 4, loss: 2.263021\n",
            "Epoch 5, loss: 2.255587\n",
            "Epoch 6, loss: 2.249475\n",
            "Epoch 7, loss: 2.242304\n",
            "Epoch 8, loss: 2.254580\n",
            "Epoch 9, loss: 2.225362\n",
            "Epoch 10, loss: 2.225081\n",
            "Epoch 11, loss: 2.241168\n",
            "Epoch 12, loss: 2.222860\n",
            "Epoch 13, loss: 2.228421\n",
            "Epoch 14, loss: 2.232216\n",
            "Epoch 15, loss: 2.210765\n",
            "Epoch 16, loss: 2.197178\n",
            "Epoch 17, loss: 2.216956\n",
            "Epoch 18, loss: 2.219335\n",
            "Epoch 19, loss: 2.197911\n",
            "Epoch 20, loss: 2.203872\n",
            "Epoch 21, loss: 2.188434\n",
            "Epoch 22, loss: 2.206968\n",
            "Epoch 23, loss: 2.189017\n",
            "Epoch 24, loss: 2.170645\n",
            "Epoch 25, loss: 2.169503\n",
            "Epoch 26, loss: 2.218611\n",
            "Epoch 27, loss: 2.160133\n",
            "Epoch 28, loss: 2.172398\n",
            "Epoch 29, loss: 2.162891\n",
            "Epoch 30, loss: 2.207606\n",
            "Epoch 31, loss: 2.182561\n",
            "Epoch 32, loss: 2.149063\n",
            "Epoch 33, loss: 2.187677\n",
            "Epoch 34, loss: 2.184052\n",
            "Epoch 35, loss: 2.144069\n",
            "Epoch 36, loss: 2.165558\n",
            "Epoch 37, loss: 2.159662\n",
            "Epoch 38, loss: 2.153238\n",
            "Epoch 39, loss: 2.153681\n",
            "Epoch 40, loss: 2.208226\n",
            "Epoch 41, loss: 2.166956\n",
            "Epoch 42, loss: 2.194028\n",
            "Epoch 43, loss: 2.185005\n",
            "Epoch 44, loss: 2.197307\n",
            "Epoch 45, loss: 2.161270\n",
            "Epoch 46, loss: 2.169007\n",
            "Epoch 47, loss: 2.122946\n",
            "Epoch 48, loss: 2.137713\n",
            "Epoch 49, loss: 2.142133\n",
            "Epoch 50, loss: 2.166527\n",
            "Epoch 51, loss: 2.208997\n",
            "Epoch 52, loss: 2.200100\n",
            "Epoch 53, loss: 2.165657\n",
            "Epoch 54, loss: 2.098592\n",
            "Epoch 55, loss: 2.104744\n",
            "Epoch 56, loss: 2.176402\n",
            "Epoch 57, loss: 2.072561\n",
            "Epoch 58, loss: 2.149596\n",
            "Epoch 59, loss: 2.103721\n",
            "Epoch 60, loss: 2.142533\n",
            "Epoch 61, loss: 2.159171\n",
            "Epoch 62, loss: 2.108329\n",
            "Epoch 63, loss: 2.111621\n",
            "Epoch 64, loss: 2.201635\n",
            "Epoch 65, loss: 2.072293\n",
            "Epoch 66, loss: 2.130641\n",
            "Epoch 67, loss: 2.120353\n",
            "Epoch 68, loss: 2.101529\n",
            "Epoch 69, loss: 2.157373\n",
            "Epoch 70, loss: 2.121014\n",
            "Epoch 71, loss: 2.090331\n",
            "Epoch 72, loss: 2.147390\n",
            "Epoch 73, loss: 2.159378\n",
            "Epoch 74, loss: 2.132334\n",
            "Epoch 75, loss: 2.090201\n",
            "Epoch 76, loss: 2.108433\n",
            "Epoch 77, loss: 2.162500\n",
            "Epoch 78, loss: 2.173483\n",
            "Epoch 79, loss: 2.145618\n",
            "Epoch 80, loss: 2.097554\n",
            "Epoch 81, loss: 2.142882\n",
            "Epoch 82, loss: 2.138067\n",
            "Epoch 83, loss: 2.087743\n",
            "Epoch 84, loss: 2.098026\n",
            "Epoch 85, loss: 2.154688\n",
            "Epoch 86, loss: 2.146561\n",
            "Epoch 87, loss: 2.123299\n",
            "Epoch 88, loss: 2.174730\n",
            "Epoch 89, loss: 2.150809\n",
            "Epoch 90, loss: 2.177958\n",
            "Epoch 91, loss: 2.152450\n",
            "Epoch 92, loss: 2.090383\n",
            "Epoch 93, loss: 2.164900\n",
            "Epoch 94, loss: 2.140628\n",
            "Epoch 95, loss: 2.192155\n",
            "Epoch 96, loss: 2.165715\n",
            "Epoch 97, loss: 2.101889\n",
            "Epoch 98, loss: 2.134276\n",
            "Epoch 99, loss: 2.121254\n",
            "Epoch 100, loss: 2.129495\n",
            "Epoch 101, loss: 2.077108\n",
            "Epoch 102, loss: 2.117788\n",
            "Epoch 103, loss: 2.133570\n",
            "Epoch 104, loss: 2.121332\n",
            "Epoch 105, loss: 2.197790\n",
            "Epoch 106, loss: 2.097987\n",
            "Epoch 107, loss: 2.141154\n",
            "Epoch 108, loss: 2.154525\n",
            "Epoch 109, loss: 2.084366\n",
            "Epoch 110, loss: 2.099175\n",
            "Epoch 111, loss: 2.137767\n",
            "Epoch 112, loss: 2.137397\n",
            "Epoch 113, loss: 2.159324\n",
            "Epoch 114, loss: 2.115021\n",
            "Epoch 115, loss: 2.118011\n",
            "Epoch 116, loss: 2.132710\n",
            "Epoch 117, loss: 2.120845\n",
            "Epoch 118, loss: 2.105437\n",
            "Epoch 119, loss: 2.118905\n",
            "Epoch 120, loss: 2.084948\n",
            "Epoch 121, loss: 2.119653\n",
            "Epoch 122, loss: 2.167468\n",
            "Epoch 123, loss: 2.111708\n",
            "Epoch 124, loss: 2.132577\n",
            "Epoch 125, loss: 2.069027\n",
            "Epoch 126, loss: 2.085017\n",
            "Epoch 127, loss: 2.093742\n",
            "Epoch 128, loss: 2.147427\n",
            "Epoch 129, loss: 2.141732\n",
            "Epoch 130, loss: 2.116886\n",
            "Epoch 131, loss: 2.078486\n",
            "Epoch 132, loss: 2.083800\n",
            "Epoch 133, loss: 2.105221\n",
            "Epoch 134, loss: 2.087198\n",
            "Epoch 135, loss: 2.117608\n",
            "Epoch 136, loss: 2.169992\n",
            "Epoch 137, loss: 2.108625\n",
            "Epoch 138, loss: 2.111658\n",
            "Epoch 139, loss: 2.110581\n",
            "Epoch 140, loss: 2.134756\n",
            "Epoch 141, loss: 2.085462\n",
            "Epoch 142, loss: 2.125511\n",
            "Epoch 143, loss: 2.133343\n",
            "Epoch 144, loss: 2.120623\n",
            "Epoch 145, loss: 2.080448\n",
            "Epoch 146, loss: 2.097963\n",
            "Epoch 147, loss: 2.132022\n",
            "Epoch 148, loss: 2.123042\n",
            "Epoch 149, loss: 2.123874\n",
            "Epoch 150, loss: 2.151036\n",
            "Epoch 151, loss: 2.147301\n",
            "Epoch 152, loss: 2.080090\n",
            "Epoch 153, loss: 2.083859\n",
            "Epoch 154, loss: 2.109961\n",
            "Epoch 155, loss: 2.098473\n",
            "Epoch 156, loss: 2.124462\n",
            "Epoch 157, loss: 2.063164\n",
            "Epoch 158, loss: 2.120418\n",
            "Epoch 159, loss: 2.158617\n",
            "Epoch 160, loss: 2.124709\n",
            "Epoch 161, loss: 2.141764\n",
            "Epoch 162, loss: 2.092045\n",
            "Epoch 163, loss: 2.162614\n",
            "Epoch 164, loss: 2.089738\n",
            "Epoch 165, loss: 2.093438\n",
            "Epoch 166, loss: 2.072114\n",
            "Epoch 167, loss: 2.095504\n",
            "Epoch 168, loss: 2.080098\n",
            "Epoch 169, loss: 2.151188\n",
            "Epoch 170, loss: 2.136813\n",
            "Epoch 171, loss: 2.170905\n",
            "Epoch 172, loss: 2.096711\n",
            "Epoch 173, loss: 2.135496\n",
            "Epoch 174, loss: 2.129674\n",
            "Epoch 175, loss: 2.136691\n",
            "Epoch 176, loss: 2.048621\n",
            "Epoch 177, loss: 2.124861\n",
            "Epoch 178, loss: 2.168574\n",
            "Epoch 179, loss: 2.127594\n",
            "Epoch 180, loss: 2.048796\n",
            "Epoch 181, loss: 2.128371\n",
            "Epoch 182, loss: 2.141031\n",
            "Epoch 183, loss: 2.185797\n",
            "Epoch 184, loss: 2.112468\n",
            "Epoch 185, loss: 2.065589\n",
            "Epoch 186, loss: 2.080318\n",
            "Epoch 187, loss: 2.092087\n",
            "Epoch 188, loss: 2.102873\n",
            "Epoch 189, loss: 2.123979\n",
            "Epoch 190, loss: 2.114329\n",
            "Epoch 191, loss: 2.128794\n",
            "Epoch 192, loss: 2.097588\n",
            "Epoch 193, loss: 2.175131\n",
            "Epoch 194, loss: 2.096338\n",
            "Epoch 195, loss: 2.074038\n",
            "Epoch 196, loss: 2.139269\n",
            "Epoch 197, loss: 2.173164\n",
            "Epoch 198, loss: 2.137625\n",
            "Epoch 199, loss: 2.120115\n",
            "Epoch 0, loss: 2.298042\n",
            "Epoch 1, loss: 2.290139\n",
            "Epoch 2, loss: 2.273704\n",
            "Epoch 3, loss: 2.273855\n",
            "Epoch 4, loss: 2.277105\n",
            "Epoch 5, loss: 2.246927\n",
            "Epoch 6, loss: 2.249825\n",
            "Epoch 7, loss: 2.247799\n",
            "Epoch 8, loss: 2.247784\n",
            "Epoch 9, loss: 2.238500\n",
            "Epoch 10, loss: 2.226005\n",
            "Epoch 11, loss: 2.220281\n",
            "Epoch 12, loss: 2.242449\n",
            "Epoch 13, loss: 2.211370\n",
            "Epoch 14, loss: 2.228113\n",
            "Epoch 15, loss: 2.195745\n",
            "Epoch 16, loss: 2.215457\n",
            "Epoch 17, loss: 2.182634\n",
            "Epoch 18, loss: 2.179214\n",
            "Epoch 19, loss: 2.222834\n",
            "Epoch 20, loss: 2.192344\n",
            "Epoch 21, loss: 2.186425\n",
            "Epoch 22, loss: 2.179616\n",
            "Epoch 23, loss: 2.202990\n",
            "Epoch 24, loss: 2.185158\n",
            "Epoch 25, loss: 2.212799\n",
            "Epoch 26, loss: 2.202486\n",
            "Epoch 27, loss: 2.186741\n",
            "Epoch 28, loss: 2.199944\n",
            "Epoch 29, loss: 2.166260\n",
            "Epoch 30, loss: 2.184843\n",
            "Epoch 31, loss: 2.200780\n",
            "Epoch 32, loss: 2.151745\n",
            "Epoch 33, loss: 2.181341\n",
            "Epoch 34, loss: 2.160098\n",
            "Epoch 35, loss: 2.146652\n",
            "Epoch 36, loss: 2.129851\n",
            "Epoch 37, loss: 2.183740\n",
            "Epoch 38, loss: 2.180710\n",
            "Epoch 39, loss: 2.176957\n",
            "Epoch 40, loss: 2.170340\n",
            "Epoch 41, loss: 2.179169\n",
            "Epoch 42, loss: 2.190725\n",
            "Epoch 43, loss: 2.134375\n",
            "Epoch 44, loss: 2.151180\n",
            "Epoch 45, loss: 2.173870\n",
            "Epoch 46, loss: 2.145669\n",
            "Epoch 47, loss: 2.128000\n",
            "Epoch 48, loss: 2.147321\n",
            "Epoch 49, loss: 2.195732\n",
            "Epoch 50, loss: 2.112326\n",
            "Epoch 51, loss: 2.168952\n",
            "Epoch 52, loss: 2.082149\n",
            "Epoch 53, loss: 2.161573\n",
            "Epoch 54, loss: 2.199135\n",
            "Epoch 55, loss: 2.154712\n",
            "Epoch 56, loss: 2.180774\n",
            "Epoch 57, loss: 2.167642\n",
            "Epoch 58, loss: 2.182421\n",
            "Epoch 59, loss: 2.158594\n",
            "Epoch 60, loss: 2.182137\n",
            "Epoch 61, loss: 2.180565\n",
            "Epoch 62, loss: 2.138382\n",
            "Epoch 63, loss: 2.106137\n",
            "Epoch 64, loss: 2.135482\n",
            "Epoch 65, loss: 2.133591\n",
            "Epoch 66, loss: 2.142829\n",
            "Epoch 67, loss: 2.168192\n",
            "Epoch 68, loss: 2.119139\n",
            "Epoch 69, loss: 2.145171\n",
            "Epoch 70, loss: 2.159528\n",
            "Epoch 71, loss: 2.143982\n",
            "Epoch 72, loss: 2.180753\n",
            "Epoch 73, loss: 2.118455\n",
            "Epoch 74, loss: 2.099267\n",
            "Epoch 75, loss: 2.131606\n",
            "Epoch 76, loss: 2.148654\n",
            "Epoch 77, loss: 2.124436\n",
            "Epoch 78, loss: 2.175807\n",
            "Epoch 79, loss: 2.133989\n",
            "Epoch 80, loss: 2.187183\n",
            "Epoch 81, loss: 2.127726\n",
            "Epoch 82, loss: 2.129563\n",
            "Epoch 83, loss: 2.185969\n",
            "Epoch 84, loss: 2.127583\n",
            "Epoch 85, loss: 2.095594\n",
            "Epoch 86, loss: 2.127428\n",
            "Epoch 87, loss: 2.125227\n",
            "Epoch 88, loss: 2.103040\n",
            "Epoch 89, loss: 2.127839\n",
            "Epoch 90, loss: 2.119304\n",
            "Epoch 91, loss: 2.089372\n",
            "Epoch 92, loss: 2.150470\n",
            "Epoch 93, loss: 2.160165\n",
            "Epoch 94, loss: 2.134001\n",
            "Epoch 95, loss: 2.087750\n",
            "Epoch 96, loss: 2.121163\n",
            "Epoch 97, loss: 2.128117\n",
            "Epoch 98, loss: 2.147986\n",
            "Epoch 99, loss: 2.133272\n",
            "Epoch 100, loss: 2.141949\n",
            "Epoch 101, loss: 2.085617\n",
            "Epoch 102, loss: 2.175439\n",
            "Epoch 103, loss: 2.097310\n",
            "Epoch 104, loss: 2.085211\n",
            "Epoch 105, loss: 2.138011\n",
            "Epoch 106, loss: 2.177118\n",
            "Epoch 107, loss: 2.164411\n",
            "Epoch 108, loss: 2.089460\n",
            "Epoch 109, loss: 2.115158\n",
            "Epoch 110, loss: 2.170331\n",
            "Epoch 111, loss: 2.094925\n",
            "Epoch 112, loss: 2.074361\n",
            "Epoch 113, loss: 2.089721\n",
            "Epoch 114, loss: 2.125198\n",
            "Epoch 115, loss: 2.110838\n",
            "Epoch 116, loss: 2.141513\n",
            "Epoch 117, loss: 2.138850\n",
            "Epoch 118, loss: 2.133486\n",
            "Epoch 119, loss: 2.137909\n",
            "Epoch 120, loss: 2.115812\n",
            "Epoch 121, loss: 2.131575\n",
            "Epoch 122, loss: 2.112033\n",
            "Epoch 123, loss: 2.104830\n",
            "Epoch 124, loss: 2.205695\n",
            "Epoch 125, loss: 2.128854\n",
            "Epoch 126, loss: 2.162717\n",
            "Epoch 127, loss: 2.176161\n",
            "Epoch 128, loss: 2.116662\n",
            "Epoch 129, loss: 2.093278\n",
            "Epoch 130, loss: 2.144587\n",
            "Epoch 131, loss: 2.105977\n",
            "Epoch 132, loss: 2.123352\n",
            "Epoch 133, loss: 2.079536\n",
            "Epoch 134, loss: 2.118187\n",
            "Epoch 135, loss: 2.146372\n",
            "Epoch 136, loss: 2.135416\n",
            "Epoch 137, loss: 2.104067\n",
            "Epoch 138, loss: 2.138181\n",
            "Epoch 139, loss: 2.120022\n",
            "Epoch 140, loss: 2.147581\n",
            "Epoch 141, loss: 2.157083\n",
            "Epoch 142, loss: 2.121386\n",
            "Epoch 143, loss: 2.094188\n",
            "Epoch 144, loss: 2.120293\n",
            "Epoch 145, loss: 2.137399\n",
            "Epoch 146, loss: 2.098994\n",
            "Epoch 147, loss: 2.064063\n",
            "Epoch 148, loss: 2.069737\n",
            "Epoch 149, loss: 2.108515\n",
            "Epoch 150, loss: 2.120383\n",
            "Epoch 151, loss: 2.105633\n",
            "Epoch 152, loss: 2.116116\n",
            "Epoch 153, loss: 2.146768\n",
            "Epoch 154, loss: 2.124051\n",
            "Epoch 155, loss: 2.139710\n",
            "Epoch 156, loss: 2.078649\n",
            "Epoch 157, loss: 2.130850\n",
            "Epoch 158, loss: 2.095850\n",
            "Epoch 159, loss: 2.035344\n",
            "Epoch 160, loss: 2.099177\n",
            "Epoch 161, loss: 2.074855\n",
            "Epoch 162, loss: 2.120095\n",
            "Epoch 163, loss: 2.097900\n",
            "Epoch 164, loss: 2.041502\n",
            "Epoch 165, loss: 2.137676\n",
            "Epoch 166, loss: 2.095151\n",
            "Epoch 167, loss: 2.140167\n",
            "Epoch 168, loss: 2.109228\n",
            "Epoch 169, loss: 2.188014\n",
            "Epoch 170, loss: 2.106066\n",
            "Epoch 171, loss: 2.083817\n",
            "Epoch 172, loss: 2.126886\n",
            "Epoch 173, loss: 2.088980\n",
            "Epoch 174, loss: 2.105452\n",
            "Epoch 175, loss: 2.115589\n",
            "Epoch 176, loss: 2.120485\n",
            "Epoch 177, loss: 2.043904\n",
            "Epoch 178, loss: 2.011554\n",
            "Epoch 179, loss: 2.094253\n",
            "Epoch 180, loss: 2.084575\n",
            "Epoch 181, loss: 2.185048\n",
            "Epoch 182, loss: 2.157767\n",
            "Epoch 183, loss: 2.145134\n",
            "Epoch 184, loss: 2.046080\n",
            "Epoch 185, loss: 2.079264\n",
            "Epoch 186, loss: 2.124476\n",
            "Epoch 187, loss: 2.053688\n",
            "Epoch 188, loss: 2.141602\n",
            "Epoch 189, loss: 2.167209\n",
            "Epoch 190, loss: 2.081715\n",
            "Epoch 191, loss: 2.157548\n",
            "Epoch 192, loss: 2.124090\n",
            "Epoch 193, loss: 2.134523\n",
            "Epoch 194, loss: 2.130929\n",
            "Epoch 195, loss: 2.139135\n",
            "Epoch 196, loss: 2.107109\n",
            "Epoch 197, loss: 2.102532\n",
            "Epoch 198, loss: 2.049159\n",
            "Epoch 199, loss: 2.053227\n",
            "Epoch 0, loss: 2.295466\n",
            "Epoch 1, loss: 2.290229\n",
            "Epoch 2, loss: 2.287600\n",
            "Epoch 3, loss: 2.280718\n",
            "Epoch 4, loss: 2.267491\n",
            "Epoch 5, loss: 2.267193\n",
            "Epoch 6, loss: 2.261902\n",
            "Epoch 7, loss: 2.253610\n",
            "Epoch 8, loss: 2.243436\n",
            "Epoch 9, loss: 2.248006\n",
            "Epoch 10, loss: 2.241197\n",
            "Epoch 11, loss: 2.216559\n",
            "Epoch 12, loss: 2.201234\n",
            "Epoch 13, loss: 2.226532\n",
            "Epoch 14, loss: 2.196368\n",
            "Epoch 15, loss: 2.206434\n",
            "Epoch 16, loss: 2.205290\n",
            "Epoch 17, loss: 2.194635\n",
            "Epoch 18, loss: 2.193069\n",
            "Epoch 19, loss: 2.181797\n",
            "Epoch 20, loss: 2.190870\n",
            "Epoch 21, loss: 2.171603\n",
            "Epoch 22, loss: 2.199686\n",
            "Epoch 23, loss: 2.188312\n",
            "Epoch 24, loss: 2.179433\n",
            "Epoch 25, loss: 2.182609\n",
            "Epoch 26, loss: 2.213987\n",
            "Epoch 27, loss: 2.158015\n",
            "Epoch 28, loss: 2.218568\n",
            "Epoch 29, loss: 2.162928\n",
            "Epoch 30, loss: 2.186902\n",
            "Epoch 31, loss: 2.172776\n",
            "Epoch 32, loss: 2.196079\n",
            "Epoch 33, loss: 2.171184\n",
            "Epoch 34, loss: 2.192373\n",
            "Epoch 35, loss: 2.183251\n",
            "Epoch 36, loss: 2.191965\n",
            "Epoch 37, loss: 2.197812\n",
            "Epoch 38, loss: 2.167495\n",
            "Epoch 39, loss: 2.159359\n",
            "Epoch 40, loss: 2.157167\n",
            "Epoch 41, loss: 2.155678\n",
            "Epoch 42, loss: 2.147828\n",
            "Epoch 43, loss: 2.170286\n",
            "Epoch 44, loss: 2.128369\n",
            "Epoch 45, loss: 2.198036\n",
            "Epoch 46, loss: 2.145752\n",
            "Epoch 47, loss: 2.130189\n",
            "Epoch 48, loss: 2.131210\n",
            "Epoch 49, loss: 2.121733\n",
            "Epoch 50, loss: 2.153107\n",
            "Epoch 51, loss: 2.127560\n",
            "Epoch 52, loss: 2.143591\n",
            "Epoch 53, loss: 2.105853\n",
            "Epoch 54, loss: 2.152757\n",
            "Epoch 55, loss: 2.156436\n",
            "Epoch 56, loss: 2.134974\n",
            "Epoch 57, loss: 2.173002\n",
            "Epoch 58, loss: 2.161204\n",
            "Epoch 59, loss: 2.172280\n",
            "Epoch 60, loss: 2.175036\n",
            "Epoch 61, loss: 2.127354\n",
            "Epoch 62, loss: 2.194024\n",
            "Epoch 63, loss: 2.120394\n",
            "Epoch 64, loss: 2.147647\n",
            "Epoch 65, loss: 2.091320\n",
            "Epoch 66, loss: 2.146893\n",
            "Epoch 67, loss: 2.174603\n",
            "Epoch 68, loss: 2.113013\n",
            "Epoch 69, loss: 2.147402\n",
            "Epoch 70, loss: 2.156938\n",
            "Epoch 71, loss: 2.102569\n",
            "Epoch 72, loss: 2.131781\n",
            "Epoch 73, loss: 2.098725\n",
            "Epoch 74, loss: 2.139690\n",
            "Epoch 75, loss: 2.160084\n",
            "Epoch 76, loss: 2.130230\n",
            "Epoch 77, loss: 2.092297\n",
            "Epoch 78, loss: 2.116791\n",
            "Epoch 79, loss: 2.100692\n",
            "Epoch 80, loss: 2.135329\n",
            "Epoch 81, loss: 2.112125\n",
            "Epoch 82, loss: 2.158104\n",
            "Epoch 83, loss: 2.177170\n",
            "Epoch 84, loss: 2.126367\n",
            "Epoch 85, loss: 2.120649\n",
            "Epoch 86, loss: 2.147243\n",
            "Epoch 87, loss: 2.174533\n",
            "Epoch 88, loss: 2.154814\n",
            "Epoch 89, loss: 2.102354\n",
            "Epoch 90, loss: 2.140049\n",
            "Epoch 91, loss: 2.145522\n",
            "Epoch 92, loss: 2.140813\n",
            "Epoch 93, loss: 2.118236\n",
            "Epoch 94, loss: 2.144872\n",
            "Epoch 95, loss: 2.140390\n",
            "Epoch 96, loss: 2.174951\n",
            "Epoch 97, loss: 2.111649\n",
            "Epoch 98, loss: 2.082807\n",
            "Epoch 99, loss: 2.131892\n",
            "Epoch 100, loss: 2.162045\n",
            "Epoch 101, loss: 2.086559\n",
            "Epoch 102, loss: 2.116686\n",
            "Epoch 103, loss: 2.133897\n",
            "Epoch 104, loss: 2.133603\n",
            "Epoch 105, loss: 2.120218\n",
            "Epoch 106, loss: 2.105617\n",
            "Epoch 107, loss: 2.187938\n",
            "Epoch 108, loss: 2.171386\n",
            "Epoch 109, loss: 2.148669\n",
            "Epoch 110, loss: 2.098251\n",
            "Epoch 111, loss: 2.064883\n",
            "Epoch 112, loss: 2.138037\n",
            "Epoch 113, loss: 2.156586\n",
            "Epoch 114, loss: 2.148795\n",
            "Epoch 115, loss: 2.104648\n",
            "Epoch 116, loss: 2.130425\n",
            "Epoch 117, loss: 2.115771\n",
            "Epoch 118, loss: 2.099407\n",
            "Epoch 119, loss: 2.120677\n",
            "Epoch 120, loss: 2.171478\n",
            "Epoch 121, loss: 2.129554\n",
            "Epoch 122, loss: 2.131920\n",
            "Epoch 123, loss: 2.143765\n",
            "Epoch 124, loss: 2.108367\n",
            "Epoch 125, loss: 2.103571\n",
            "Epoch 126, loss: 2.116951\n",
            "Epoch 127, loss: 2.105502\n",
            "Epoch 128, loss: 2.079338\n",
            "Epoch 129, loss: 2.103696\n",
            "Epoch 130, loss: 2.073349\n",
            "Epoch 131, loss: 2.130798\n",
            "Epoch 132, loss: 2.099801\n",
            "Epoch 133, loss: 2.165323\n",
            "Epoch 134, loss: 2.125365\n",
            "Epoch 135, loss: 2.078689\n",
            "Epoch 136, loss: 2.074424\n",
            "Epoch 137, loss: 2.110205\n",
            "Epoch 138, loss: 2.098042\n",
            "Epoch 139, loss: 2.105391\n",
            "Epoch 140, loss: 2.179129\n",
            "Epoch 141, loss: 2.092377\n",
            "Epoch 142, loss: 2.105706\n",
            "Epoch 143, loss: 2.073232\n",
            "Epoch 144, loss: 2.112629\n",
            "Epoch 145, loss: 2.123290\n",
            "Epoch 146, loss: 2.073347\n",
            "Epoch 147, loss: 2.131439\n",
            "Epoch 148, loss: 2.179913\n",
            "Epoch 149, loss: 2.124330\n",
            "Epoch 150, loss: 2.133798\n",
            "Epoch 151, loss: 2.068144\n",
            "Epoch 152, loss: 2.085800\n",
            "Epoch 153, loss: 2.116114\n",
            "Epoch 154, loss: 2.120417\n",
            "Epoch 155, loss: 2.145857\n",
            "Epoch 156, loss: 2.160305\n",
            "Epoch 157, loss: 2.109301\n",
            "Epoch 158, loss: 2.001393\n",
            "Epoch 159, loss: 2.134572\n",
            "Epoch 160, loss: 2.141531\n",
            "Epoch 161, loss: 2.122988\n",
            "Epoch 162, loss: 2.074033\n",
            "Epoch 163, loss: 2.097346\n",
            "Epoch 164, loss: 2.042399\n",
            "Epoch 165, loss: 2.111806\n",
            "Epoch 166, loss: 2.092986\n",
            "Epoch 167, loss: 2.137516\n",
            "Epoch 168, loss: 2.057376\n",
            "Epoch 169, loss: 2.121738\n",
            "Epoch 170, loss: 2.141018\n",
            "Epoch 171, loss: 2.080106\n",
            "Epoch 172, loss: 2.122950\n",
            "Epoch 173, loss: 2.036258\n",
            "Epoch 174, loss: 2.121849\n",
            "Epoch 175, loss: 2.088557\n",
            "Epoch 176, loss: 2.116740\n",
            "Epoch 177, loss: 2.093667\n",
            "Epoch 178, loss: 2.138071\n",
            "Epoch 179, loss: 2.101445\n",
            "Epoch 180, loss: 2.174709\n",
            "Epoch 181, loss: 2.097219\n",
            "Epoch 182, loss: 2.102247\n",
            "Epoch 183, loss: 2.172269\n",
            "Epoch 184, loss: 2.108975\n",
            "Epoch 185, loss: 2.148238\n",
            "Epoch 186, loss: 2.134473\n",
            "Epoch 187, loss: 2.153020\n",
            "Epoch 188, loss: 2.109693\n",
            "Epoch 189, loss: 2.120339\n",
            "Epoch 190, loss: 2.088213\n",
            "Epoch 191, loss: 2.115858\n",
            "Epoch 192, loss: 2.144748\n",
            "Epoch 193, loss: 2.089061\n",
            "Epoch 194, loss: 2.128546\n",
            "Epoch 195, loss: 2.103781\n",
            "Epoch 196, loss: 2.123692\n",
            "Epoch 197, loss: 2.092106\n",
            "Epoch 198, loss: 2.134926\n",
            "Epoch 199, loss: 2.077767\n",
            "Epoch 0, loss: 2.301463\n",
            "Epoch 1, loss: 2.300995\n",
            "Epoch 2, loss: 2.300543\n",
            "Epoch 3, loss: 2.298960\n",
            "Epoch 4, loss: 2.297301\n",
            "Epoch 5, loss: 2.295249\n",
            "Epoch 6, loss: 2.295613\n",
            "Epoch 7, loss: 2.295336\n",
            "Epoch 8, loss: 2.295706\n",
            "Epoch 9, loss: 2.297833\n",
            "Epoch 10, loss: 2.293739\n",
            "Epoch 11, loss: 2.290333\n",
            "Epoch 12, loss: 2.296800\n",
            "Epoch 13, loss: 2.287668\n",
            "Epoch 14, loss: 2.286921\n",
            "Epoch 15, loss: 2.287760\n",
            "Epoch 16, loss: 2.292550\n",
            "Epoch 17, loss: 2.281760\n",
            "Epoch 18, loss: 2.286975\n",
            "Epoch 19, loss: 2.287125\n",
            "Epoch 20, loss: 2.283508\n",
            "Epoch 21, loss: 2.285109\n",
            "Epoch 22, loss: 2.280376\n",
            "Epoch 23, loss: 2.282998\n",
            "Epoch 24, loss: 2.279477\n",
            "Epoch 25, loss: 2.283135\n",
            "Epoch 26, loss: 2.282302\n",
            "Epoch 27, loss: 2.281868\n",
            "Epoch 28, loss: 2.274372\n",
            "Epoch 29, loss: 2.277539\n",
            "Epoch 30, loss: 2.278294\n",
            "Epoch 31, loss: 2.277771\n",
            "Epoch 32, loss: 2.276898\n",
            "Epoch 33, loss: 2.278296\n",
            "Epoch 34, loss: 2.269183\n",
            "Epoch 35, loss: 2.272390\n",
            "Epoch 36, loss: 2.274645\n",
            "Epoch 37, loss: 2.277100\n",
            "Epoch 38, loss: 2.275116\n",
            "Epoch 39, loss: 2.267500\n",
            "Epoch 40, loss: 2.267470\n",
            "Epoch 41, loss: 2.268796\n",
            "Epoch 42, loss: 2.267524\n",
            "Epoch 43, loss: 2.268106\n",
            "Epoch 44, loss: 2.263913\n",
            "Epoch 45, loss: 2.266836\n",
            "Epoch 46, loss: 2.268354\n",
            "Epoch 47, loss: 2.265059\n",
            "Epoch 48, loss: 2.267045\n",
            "Epoch 49, loss: 2.268986\n",
            "Epoch 50, loss: 2.259240\n",
            "Epoch 51, loss: 2.258783\n",
            "Epoch 52, loss: 2.255453\n",
            "Epoch 53, loss: 2.264993\n",
            "Epoch 54, loss: 2.258685\n",
            "Epoch 55, loss: 2.259353\n",
            "Epoch 56, loss: 2.255781\n",
            "Epoch 57, loss: 2.266193\n",
            "Epoch 58, loss: 2.257175\n",
            "Epoch 59, loss: 2.250195\n",
            "Epoch 60, loss: 2.246542\n",
            "Epoch 61, loss: 2.239908\n",
            "Epoch 62, loss: 2.267585\n",
            "Epoch 63, loss: 2.257139\n",
            "Epoch 64, loss: 2.253167\n",
            "Epoch 65, loss: 2.253309\n",
            "Epoch 66, loss: 2.263836\n",
            "Epoch 67, loss: 2.242471\n",
            "Epoch 68, loss: 2.254037\n",
            "Epoch 69, loss: 2.248996\n",
            "Epoch 70, loss: 2.250111\n",
            "Epoch 71, loss: 2.252427\n",
            "Epoch 72, loss: 2.261650\n",
            "Epoch 73, loss: 2.247764\n",
            "Epoch 74, loss: 2.274827\n",
            "Epoch 75, loss: 2.251394\n",
            "Epoch 76, loss: 2.256313\n",
            "Epoch 77, loss: 2.249061\n",
            "Epoch 78, loss: 2.243910\n",
            "Epoch 79, loss: 2.254458\n",
            "Epoch 80, loss: 2.257809\n",
            "Epoch 81, loss: 2.240902\n",
            "Epoch 82, loss: 2.246857\n",
            "Epoch 83, loss: 2.253799\n",
            "Epoch 84, loss: 2.231419\n",
            "Epoch 85, loss: 2.247403\n",
            "Epoch 86, loss: 2.251315\n",
            "Epoch 87, loss: 2.240709\n",
            "Epoch 88, loss: 2.246959\n",
            "Epoch 89, loss: 2.248712\n",
            "Epoch 90, loss: 2.236283\n",
            "Epoch 91, loss: 2.227699\n",
            "Epoch 92, loss: 2.245625\n",
            "Epoch 93, loss: 2.250105\n",
            "Epoch 94, loss: 2.239395\n",
            "Epoch 95, loss: 2.243110\n",
            "Epoch 96, loss: 2.241967\n",
            "Epoch 97, loss: 2.256331\n",
            "Epoch 98, loss: 2.212853\n",
            "Epoch 99, loss: 2.247774\n",
            "Epoch 100, loss: 2.237066\n",
            "Epoch 101, loss: 2.218970\n",
            "Epoch 102, loss: 2.223109\n",
            "Epoch 103, loss: 2.229828\n",
            "Epoch 104, loss: 2.244603\n",
            "Epoch 105, loss: 2.229888\n",
            "Epoch 106, loss: 2.231822\n",
            "Epoch 107, loss: 2.235581\n",
            "Epoch 108, loss: 2.226253\n",
            "Epoch 109, loss: 2.222553\n",
            "Epoch 110, loss: 2.239470\n",
            "Epoch 111, loss: 2.226618\n",
            "Epoch 112, loss: 2.233780\n",
            "Epoch 113, loss: 2.251227\n",
            "Epoch 114, loss: 2.233194\n",
            "Epoch 115, loss: 2.223630\n",
            "Epoch 116, loss: 2.229321\n",
            "Epoch 117, loss: 2.239361\n",
            "Epoch 118, loss: 2.217568\n",
            "Epoch 119, loss: 2.243579\n",
            "Epoch 120, loss: 2.221437\n",
            "Epoch 121, loss: 2.231438\n",
            "Epoch 122, loss: 2.239220\n",
            "Epoch 123, loss: 2.225982\n",
            "Epoch 124, loss: 2.231905\n",
            "Epoch 125, loss: 2.242820\n",
            "Epoch 126, loss: 2.232538\n",
            "Epoch 127, loss: 2.236106\n",
            "Epoch 128, loss: 2.225980\n",
            "Epoch 129, loss: 2.246276\n",
            "Epoch 130, loss: 2.219308\n",
            "Epoch 131, loss: 2.234953\n",
            "Epoch 132, loss: 2.241631\n",
            "Epoch 133, loss: 2.227971\n",
            "Epoch 134, loss: 2.193068\n",
            "Epoch 135, loss: 2.230584\n",
            "Epoch 136, loss: 2.216454\n",
            "Epoch 137, loss: 2.222177\n",
            "Epoch 138, loss: 2.225400\n",
            "Epoch 139, loss: 2.222078\n",
            "Epoch 140, loss: 2.221393\n",
            "Epoch 141, loss: 2.223524\n",
            "Epoch 142, loss: 2.223889\n",
            "Epoch 143, loss: 2.239772\n",
            "Epoch 144, loss: 2.220350\n",
            "Epoch 145, loss: 2.244851\n",
            "Epoch 146, loss: 2.234099\n",
            "Epoch 147, loss: 2.219144\n",
            "Epoch 148, loss: 2.222830\n",
            "Epoch 149, loss: 2.186518\n",
            "Epoch 150, loss: 2.191097\n",
            "Epoch 151, loss: 2.237300\n",
            "Epoch 152, loss: 2.190544\n",
            "Epoch 153, loss: 2.231929\n",
            "Epoch 154, loss: 2.203193\n",
            "Epoch 155, loss: 2.214505\n",
            "Epoch 156, loss: 2.219200\n",
            "Epoch 157, loss: 2.205690\n",
            "Epoch 158, loss: 2.201419\n",
            "Epoch 159, loss: 2.214601\n",
            "Epoch 160, loss: 2.232757\n",
            "Epoch 161, loss: 2.234536\n",
            "Epoch 162, loss: 2.219422\n",
            "Epoch 163, loss: 2.202361\n",
            "Epoch 164, loss: 2.187972\n",
            "Epoch 165, loss: 2.200760\n",
            "Epoch 166, loss: 2.211581\n",
            "Epoch 167, loss: 2.205682\n",
            "Epoch 168, loss: 2.214487\n",
            "Epoch 169, loss: 2.214930\n",
            "Epoch 170, loss: 2.204750\n",
            "Epoch 171, loss: 2.193294\n",
            "Epoch 172, loss: 2.212022\n",
            "Epoch 173, loss: 2.228168\n",
            "Epoch 174, loss: 2.236269\n",
            "Epoch 175, loss: 2.197949\n",
            "Epoch 176, loss: 2.215847\n",
            "Epoch 177, loss: 2.223163\n",
            "Epoch 178, loss: 2.197859\n",
            "Epoch 179, loss: 2.202178\n",
            "Epoch 180, loss: 2.203441\n",
            "Epoch 181, loss: 2.212575\n",
            "Epoch 182, loss: 2.233672\n",
            "Epoch 183, loss: 2.199589\n",
            "Epoch 184, loss: 2.196841\n",
            "Epoch 185, loss: 2.218945\n",
            "Epoch 186, loss: 2.216761\n",
            "Epoch 187, loss: 2.207077\n",
            "Epoch 188, loss: 2.185685\n",
            "Epoch 189, loss: 2.175905\n",
            "Epoch 190, loss: 2.215356\n",
            "Epoch 191, loss: 2.208986\n",
            "Epoch 192, loss: 2.185145\n",
            "Epoch 193, loss: 2.205752\n",
            "Epoch 194, loss: 2.181333\n",
            "Epoch 195, loss: 2.207053\n",
            "Epoch 196, loss: 2.198799\n",
            "Epoch 197, loss: 2.181561\n",
            "Epoch 198, loss: 2.194080\n",
            "Epoch 199, loss: 2.199565\n",
            "Epoch 0, loss: 2.301491\n",
            "Epoch 1, loss: 2.301603\n",
            "Epoch 2, loss: 2.300416\n",
            "Epoch 3, loss: 2.299595\n",
            "Epoch 4, loss: 2.297281\n",
            "Epoch 5, loss: 2.295416\n",
            "Epoch 6, loss: 2.296843\n",
            "Epoch 7, loss: 2.297707\n",
            "Epoch 8, loss: 2.292512\n",
            "Epoch 9, loss: 2.293815\n",
            "Epoch 10, loss: 2.294618\n",
            "Epoch 11, loss: 2.293075\n",
            "Epoch 12, loss: 2.293466\n",
            "Epoch 13, loss: 2.291411\n",
            "Epoch 14, loss: 2.292932\n",
            "Epoch 15, loss: 2.285615\n",
            "Epoch 16, loss: 2.284296\n",
            "Epoch 17, loss: 2.289585\n",
            "Epoch 18, loss: 2.285922\n",
            "Epoch 19, loss: 2.287130\n",
            "Epoch 20, loss: 2.287612\n",
            "Epoch 21, loss: 2.283393\n",
            "Epoch 22, loss: 2.279069\n",
            "Epoch 23, loss: 2.285479\n",
            "Epoch 24, loss: 2.276690\n",
            "Epoch 25, loss: 2.280085\n",
            "Epoch 26, loss: 2.276150\n",
            "Epoch 27, loss: 2.282118\n",
            "Epoch 28, loss: 2.285036\n",
            "Epoch 29, loss: 2.280164\n",
            "Epoch 30, loss: 2.276633\n",
            "Epoch 31, loss: 2.278077\n",
            "Epoch 32, loss: 2.271758\n",
            "Epoch 33, loss: 2.273806\n",
            "Epoch 34, loss: 2.283903\n",
            "Epoch 35, loss: 2.276437\n",
            "Epoch 36, loss: 2.266796\n",
            "Epoch 37, loss: 2.268153\n",
            "Epoch 38, loss: 2.262499\n",
            "Epoch 39, loss: 2.272736\n",
            "Epoch 40, loss: 2.275928\n",
            "Epoch 41, loss: 2.261924\n",
            "Epoch 42, loss: 2.272717\n",
            "Epoch 43, loss: 2.271448\n",
            "Epoch 44, loss: 2.264174\n",
            "Epoch 45, loss: 2.271503\n",
            "Epoch 46, loss: 2.270805\n",
            "Epoch 47, loss: 2.263286\n",
            "Epoch 48, loss: 2.265762\n",
            "Epoch 49, loss: 2.271446\n",
            "Epoch 50, loss: 2.262981\n",
            "Epoch 51, loss: 2.266986\n",
            "Epoch 52, loss: 2.254592\n",
            "Epoch 53, loss: 2.255711\n",
            "Epoch 54, loss: 2.262630\n",
            "Epoch 55, loss: 2.258895\n",
            "Epoch 56, loss: 2.259920\n",
            "Epoch 57, loss: 2.263409\n",
            "Epoch 58, loss: 2.270693\n",
            "Epoch 59, loss: 2.259141\n",
            "Epoch 60, loss: 2.258719\n",
            "Epoch 61, loss: 2.256208\n",
            "Epoch 62, loss: 2.252313\n",
            "Epoch 63, loss: 2.260369\n",
            "Epoch 64, loss: 2.253912\n",
            "Epoch 65, loss: 2.252093\n",
            "Epoch 66, loss: 2.255419\n",
            "Epoch 67, loss: 2.257366\n",
            "Epoch 68, loss: 2.256159\n",
            "Epoch 69, loss: 2.265253\n",
            "Epoch 70, loss: 2.257779\n",
            "Epoch 71, loss: 2.261223\n",
            "Epoch 72, loss: 2.252929\n",
            "Epoch 73, loss: 2.257273\n",
            "Epoch 74, loss: 2.248455\n",
            "Epoch 75, loss: 2.255771\n",
            "Epoch 76, loss: 2.252429\n",
            "Epoch 77, loss: 2.241439\n",
            "Epoch 78, loss: 2.249005\n",
            "Epoch 79, loss: 2.237391\n",
            "Epoch 80, loss: 2.241726\n",
            "Epoch 81, loss: 2.247647\n",
            "Epoch 82, loss: 2.255051\n",
            "Epoch 83, loss: 2.245429\n",
            "Epoch 84, loss: 2.255657\n",
            "Epoch 85, loss: 2.226495\n",
            "Epoch 86, loss: 2.249306\n",
            "Epoch 87, loss: 2.250087\n",
            "Epoch 88, loss: 2.242554\n",
            "Epoch 89, loss: 2.227021\n",
            "Epoch 90, loss: 2.234766\n",
            "Epoch 91, loss: 2.215339\n",
            "Epoch 92, loss: 2.253670\n",
            "Epoch 93, loss: 2.244834\n",
            "Epoch 94, loss: 2.239506\n",
            "Epoch 95, loss: 2.229278\n",
            "Epoch 96, loss: 2.225737\n",
            "Epoch 97, loss: 2.241055\n",
            "Epoch 98, loss: 2.235744\n",
            "Epoch 99, loss: 2.240341\n",
            "Epoch 100, loss: 2.254929\n",
            "Epoch 101, loss: 2.250035\n",
            "Epoch 102, loss: 2.231356\n",
            "Epoch 103, loss: 2.252015\n",
            "Epoch 104, loss: 2.238755\n",
            "Epoch 105, loss: 2.219052\n",
            "Epoch 106, loss: 2.243187\n",
            "Epoch 107, loss: 2.232341\n",
            "Epoch 108, loss: 2.245302\n",
            "Epoch 109, loss: 2.221083\n",
            "Epoch 110, loss: 2.229546\n",
            "Epoch 111, loss: 2.247201\n",
            "Epoch 112, loss: 2.232607\n",
            "Epoch 113, loss: 2.251877\n",
            "Epoch 114, loss: 2.232246\n",
            "Epoch 115, loss: 2.234459\n",
            "Epoch 116, loss: 2.247805\n",
            "Epoch 117, loss: 2.236468\n",
            "Epoch 118, loss: 2.210658\n",
            "Epoch 119, loss: 2.238486\n",
            "Epoch 120, loss: 2.244291\n",
            "Epoch 121, loss: 2.243280\n",
            "Epoch 122, loss: 2.211382\n",
            "Epoch 123, loss: 2.226242\n",
            "Epoch 124, loss: 2.222366\n",
            "Epoch 125, loss: 2.225541\n",
            "Epoch 126, loss: 2.236109\n",
            "Epoch 127, loss: 2.223625\n",
            "Epoch 128, loss: 2.234202\n",
            "Epoch 129, loss: 2.227704\n",
            "Epoch 130, loss: 2.219406\n",
            "Epoch 131, loss: 2.234472\n",
            "Epoch 132, loss: 2.230022\n",
            "Epoch 133, loss: 2.218036\n",
            "Epoch 134, loss: 2.226472\n",
            "Epoch 135, loss: 2.211738\n",
            "Epoch 136, loss: 2.222384\n",
            "Epoch 137, loss: 2.223950\n",
            "Epoch 138, loss: 2.224024\n",
            "Epoch 139, loss: 2.215658\n",
            "Epoch 140, loss: 2.241858\n",
            "Epoch 141, loss: 2.218226\n",
            "Epoch 142, loss: 2.235825\n",
            "Epoch 143, loss: 2.209137\n",
            "Epoch 144, loss: 2.237464\n",
            "Epoch 145, loss: 2.221097\n",
            "Epoch 146, loss: 2.210239\n",
            "Epoch 147, loss: 2.237499\n",
            "Epoch 148, loss: 2.232765\n",
            "Epoch 149, loss: 2.213131\n",
            "Epoch 150, loss: 2.217488\n",
            "Epoch 151, loss: 2.223019\n",
            "Epoch 152, loss: 2.231092\n",
            "Epoch 153, loss: 2.223509\n",
            "Epoch 154, loss: 2.216252\n",
            "Epoch 155, loss: 2.211915\n",
            "Epoch 156, loss: 2.205531\n",
            "Epoch 157, loss: 2.219656\n",
            "Epoch 158, loss: 2.179767\n",
            "Epoch 159, loss: 2.170197\n",
            "Epoch 160, loss: 2.219944\n",
            "Epoch 161, loss: 2.193302\n",
            "Epoch 162, loss: 2.224525\n",
            "Epoch 163, loss: 2.202314\n",
            "Epoch 164, loss: 2.211398\n",
            "Epoch 165, loss: 2.214534\n",
            "Epoch 166, loss: 2.221501\n",
            "Epoch 167, loss: 2.209977\n",
            "Epoch 168, loss: 2.221622\n",
            "Epoch 169, loss: 2.207988\n",
            "Epoch 170, loss: 2.219285\n",
            "Epoch 171, loss: 2.214300\n",
            "Epoch 172, loss: 2.211654\n",
            "Epoch 173, loss: 2.199921\n",
            "Epoch 174, loss: 2.217291\n",
            "Epoch 175, loss: 2.224263\n",
            "Epoch 176, loss: 2.209858\n",
            "Epoch 177, loss: 2.220872\n",
            "Epoch 178, loss: 2.215289\n",
            "Epoch 179, loss: 2.203791\n",
            "Epoch 180, loss: 2.215353\n",
            "Epoch 181, loss: 2.230945\n",
            "Epoch 182, loss: 2.224745\n",
            "Epoch 183, loss: 2.190688\n",
            "Epoch 184, loss: 2.207049\n",
            "Epoch 185, loss: 2.207966\n",
            "Epoch 186, loss: 2.221713\n",
            "Epoch 187, loss: 2.206436\n",
            "Epoch 188, loss: 2.200169\n",
            "Epoch 189, loss: 2.190824\n",
            "Epoch 190, loss: 2.204164\n",
            "Epoch 191, loss: 2.191450\n",
            "Epoch 192, loss: 2.217543\n",
            "Epoch 193, loss: 2.232098\n",
            "Epoch 194, loss: 2.209632\n",
            "Epoch 195, loss: 2.201863\n",
            "Epoch 196, loss: 2.205435\n",
            "Epoch 197, loss: 2.195971\n",
            "Epoch 198, loss: 2.218424\n",
            "Epoch 199, loss: 2.198291\n",
            "Epoch 0, loss: 2.302427\n",
            "Epoch 1, loss: 2.300701\n",
            "Epoch 2, loss: 2.301355\n",
            "Epoch 3, loss: 2.298534\n",
            "Epoch 4, loss: 2.299134\n",
            "Epoch 5, loss: 2.297273\n",
            "Epoch 6, loss: 2.294575\n",
            "Epoch 7, loss: 2.296559\n",
            "Epoch 8, loss: 2.293251\n",
            "Epoch 9, loss: 2.296020\n",
            "Epoch 10, loss: 2.291516\n",
            "Epoch 11, loss: 2.290571\n",
            "Epoch 12, loss: 2.291535\n",
            "Epoch 13, loss: 2.292400\n",
            "Epoch 14, loss: 2.296264\n",
            "Epoch 15, loss: 2.287134\n",
            "Epoch 16, loss: 2.294436\n",
            "Epoch 17, loss: 2.292207\n",
            "Epoch 18, loss: 2.286961\n",
            "Epoch 19, loss: 2.284543\n",
            "Epoch 20, loss: 2.284162\n",
            "Epoch 21, loss: 2.279368\n",
            "Epoch 22, loss: 2.284044\n",
            "Epoch 23, loss: 2.280415\n",
            "Epoch 24, loss: 2.276084\n",
            "Epoch 25, loss: 2.279528\n",
            "Epoch 26, loss: 2.274324\n",
            "Epoch 27, loss: 2.288563\n",
            "Epoch 28, loss: 2.281128\n",
            "Epoch 29, loss: 2.275500\n",
            "Epoch 30, loss: 2.280491\n",
            "Epoch 31, loss: 2.280402\n",
            "Epoch 32, loss: 2.280325\n",
            "Epoch 33, loss: 2.284109\n",
            "Epoch 34, loss: 2.278703\n",
            "Epoch 35, loss: 2.281077\n",
            "Epoch 36, loss: 2.270681\n",
            "Epoch 37, loss: 2.266006\n",
            "Epoch 38, loss: 2.269385\n",
            "Epoch 39, loss: 2.263841\n",
            "Epoch 40, loss: 2.278874\n",
            "Epoch 41, loss: 2.270231\n",
            "Epoch 42, loss: 2.260984\n",
            "Epoch 43, loss: 2.260043\n",
            "Epoch 44, loss: 2.263148\n",
            "Epoch 45, loss: 2.271996\n",
            "Epoch 46, loss: 2.269921\n",
            "Epoch 47, loss: 2.260037\n",
            "Epoch 48, loss: 2.262699\n",
            "Epoch 49, loss: 2.269304\n",
            "Epoch 50, loss: 2.266067\n",
            "Epoch 51, loss: 2.261796\n",
            "Epoch 52, loss: 2.265335\n",
            "Epoch 53, loss: 2.257463\n",
            "Epoch 54, loss: 2.249076\n",
            "Epoch 55, loss: 2.255700\n",
            "Epoch 56, loss: 2.266988\n",
            "Epoch 57, loss: 2.259563\n",
            "Epoch 58, loss: 2.251537\n",
            "Epoch 59, loss: 2.249242\n",
            "Epoch 60, loss: 2.258023\n",
            "Epoch 61, loss: 2.244399\n",
            "Epoch 62, loss: 2.252666\n",
            "Epoch 63, loss: 2.258136\n",
            "Epoch 64, loss: 2.270943\n",
            "Epoch 65, loss: 2.254609\n",
            "Epoch 66, loss: 2.252811\n",
            "Epoch 67, loss: 2.258253\n",
            "Epoch 68, loss: 2.250792\n",
            "Epoch 69, loss: 2.247658\n",
            "Epoch 70, loss: 2.237781\n",
            "Epoch 71, loss: 2.248116\n",
            "Epoch 72, loss: 2.266187\n",
            "Epoch 73, loss: 2.251125\n",
            "Epoch 74, loss: 2.253252\n",
            "Epoch 75, loss: 2.252863\n",
            "Epoch 76, loss: 2.250485\n",
            "Epoch 77, loss: 2.251020\n",
            "Epoch 78, loss: 2.252965\n",
            "Epoch 79, loss: 2.251347\n",
            "Epoch 80, loss: 2.243708\n",
            "Epoch 81, loss: 2.253880\n",
            "Epoch 82, loss: 2.264101\n",
            "Epoch 83, loss: 2.249787\n",
            "Epoch 84, loss: 2.243165\n",
            "Epoch 85, loss: 2.239480\n",
            "Epoch 86, loss: 2.238081\n",
            "Epoch 87, loss: 2.221183\n",
            "Epoch 88, loss: 2.236637\n",
            "Epoch 89, loss: 2.252708\n",
            "Epoch 90, loss: 2.248783\n",
            "Epoch 91, loss: 2.234402\n",
            "Epoch 92, loss: 2.230496\n",
            "Epoch 93, loss: 2.245415\n",
            "Epoch 94, loss: 2.227802\n",
            "Epoch 95, loss: 2.258843\n",
            "Epoch 96, loss: 2.238755\n",
            "Epoch 97, loss: 2.250386\n",
            "Epoch 98, loss: 2.251906\n",
            "Epoch 99, loss: 2.242574\n",
            "Epoch 100, loss: 2.234551\n",
            "Epoch 101, loss: 2.247459\n",
            "Epoch 102, loss: 2.250018\n",
            "Epoch 103, loss: 2.234474\n",
            "Epoch 104, loss: 2.239584\n",
            "Epoch 105, loss: 2.231614\n",
            "Epoch 106, loss: 2.225875\n",
            "Epoch 107, loss: 2.216307\n",
            "Epoch 108, loss: 2.246184\n",
            "Epoch 109, loss: 2.241041\n",
            "Epoch 110, loss: 2.239520\n",
            "Epoch 111, loss: 2.224718\n",
            "Epoch 112, loss: 2.248198\n",
            "Epoch 113, loss: 2.200468\n",
            "Epoch 114, loss: 2.236843\n",
            "Epoch 115, loss: 2.249986\n",
            "Epoch 116, loss: 2.234979\n",
            "Epoch 117, loss: 2.215696\n",
            "Epoch 118, loss: 2.234640\n",
            "Epoch 119, loss: 2.240076\n",
            "Epoch 120, loss: 2.211473\n",
            "Epoch 121, loss: 2.230951\n",
            "Epoch 122, loss: 2.232376\n",
            "Epoch 123, loss: 2.216036\n",
            "Epoch 124, loss: 2.220324\n",
            "Epoch 125, loss: 2.232761\n",
            "Epoch 126, loss: 2.223226\n",
            "Epoch 127, loss: 2.216268\n",
            "Epoch 128, loss: 2.243963\n",
            "Epoch 129, loss: 2.213042\n",
            "Epoch 130, loss: 2.236828\n",
            "Epoch 131, loss: 2.230334\n",
            "Epoch 132, loss: 2.243816\n",
            "Epoch 133, loss: 2.229486\n",
            "Epoch 134, loss: 2.226404\n",
            "Epoch 135, loss: 2.222076\n",
            "Epoch 136, loss: 2.228438\n",
            "Epoch 137, loss: 2.220837\n",
            "Epoch 138, loss: 2.186768\n",
            "Epoch 139, loss: 2.231370\n",
            "Epoch 140, loss: 2.220876\n",
            "Epoch 141, loss: 2.204421\n",
            "Epoch 142, loss: 2.209317\n",
            "Epoch 143, loss: 2.205910\n",
            "Epoch 144, loss: 2.224003\n",
            "Epoch 145, loss: 2.241936\n",
            "Epoch 146, loss: 2.225337\n",
            "Epoch 147, loss: 2.228995\n",
            "Epoch 148, loss: 2.222886\n",
            "Epoch 149, loss: 2.218300\n",
            "Epoch 150, loss: 2.208458\n",
            "Epoch 151, loss: 2.206152\n",
            "Epoch 152, loss: 2.208597\n",
            "Epoch 153, loss: 2.227380\n",
            "Epoch 154, loss: 2.216807\n",
            "Epoch 155, loss: 2.202754\n",
            "Epoch 156, loss: 2.203414\n",
            "Epoch 157, loss: 2.207860\n",
            "Epoch 158, loss: 2.231727\n",
            "Epoch 159, loss: 2.204767\n",
            "Epoch 160, loss: 2.215275\n",
            "Epoch 161, loss: 2.216567\n",
            "Epoch 162, loss: 2.234968\n",
            "Epoch 163, loss: 2.218536\n",
            "Epoch 164, loss: 2.220040\n",
            "Epoch 165, loss: 2.207721\n",
            "Epoch 166, loss: 2.211549\n",
            "Epoch 167, loss: 2.209842\n",
            "Epoch 168, loss: 2.194738\n",
            "Epoch 169, loss: 2.216232\n",
            "Epoch 170, loss: 2.205849\n",
            "Epoch 171, loss: 2.217896\n",
            "Epoch 172, loss: 2.208171\n",
            "Epoch 173, loss: 2.212569\n",
            "Epoch 174, loss: 2.220414\n",
            "Epoch 175, loss: 2.197454\n",
            "Epoch 176, loss: 2.217608\n",
            "Epoch 177, loss: 2.236699\n",
            "Epoch 178, loss: 2.225669\n",
            "Epoch 179, loss: 2.217441\n",
            "Epoch 180, loss: 2.215080\n",
            "Epoch 181, loss: 2.202595\n",
            "Epoch 182, loss: 2.220675\n",
            "Epoch 183, loss: 2.196968\n",
            "Epoch 184, loss: 2.200015\n",
            "Epoch 185, loss: 2.220762\n",
            "Epoch 186, loss: 2.197017\n",
            "Epoch 187, loss: 2.211586\n",
            "Epoch 188, loss: 2.197416\n",
            "Epoch 189, loss: 2.184679\n",
            "Epoch 190, loss: 2.183116\n",
            "Epoch 191, loss: 2.224916\n",
            "Epoch 192, loss: 2.180363\n",
            "Epoch 193, loss: 2.201764\n",
            "Epoch 194, loss: 2.206266\n",
            "Epoch 195, loss: 2.204288\n",
            "Epoch 196, loss: 2.206320\n",
            "Epoch 197, loss: 2.207122\n",
            "Epoch 198, loss: 2.222478\n",
            "Epoch 199, loss: 2.197178\n",
            "Epoch 0, loss: 2.300818\n",
            "Epoch 1, loss: 2.300272\n",
            "Epoch 2, loss: 2.300899\n",
            "Epoch 3, loss: 2.297256\n",
            "Epoch 4, loss: 2.298826\n",
            "Epoch 5, loss: 2.297321\n",
            "Epoch 6, loss: 2.293755\n",
            "Epoch 7, loss: 2.295841\n",
            "Epoch 8, loss: 2.297524\n",
            "Epoch 9, loss: 2.293328\n",
            "Epoch 10, loss: 2.290036\n",
            "Epoch 11, loss: 2.290142\n",
            "Epoch 12, loss: 2.287597\n",
            "Epoch 13, loss: 2.285600\n",
            "Epoch 14, loss: 2.292098\n",
            "Epoch 15, loss: 2.288150\n",
            "Epoch 16, loss: 2.290678\n",
            "Epoch 17, loss: 2.287654\n",
            "Epoch 18, loss: 2.283378\n",
            "Epoch 19, loss: 2.281811\n",
            "Epoch 20, loss: 2.283027\n",
            "Epoch 21, loss: 2.273254\n",
            "Epoch 22, loss: 2.283605\n",
            "Epoch 23, loss: 2.282152\n",
            "Epoch 24, loss: 2.282062\n",
            "Epoch 25, loss: 2.286213\n",
            "Epoch 26, loss: 2.279170\n",
            "Epoch 27, loss: 2.279595\n",
            "Epoch 28, loss: 2.269597\n",
            "Epoch 29, loss: 2.285784\n",
            "Epoch 30, loss: 2.277940\n",
            "Epoch 31, loss: 2.273508\n",
            "Epoch 32, loss: 2.269667\n",
            "Epoch 33, loss: 2.278005\n",
            "Epoch 34, loss: 2.275554\n",
            "Epoch 35, loss: 2.276435\n",
            "Epoch 36, loss: 2.268147\n",
            "Epoch 37, loss: 2.274326\n",
            "Epoch 38, loss: 2.266549\n",
            "Epoch 39, loss: 2.274328\n",
            "Epoch 40, loss: 2.274757\n",
            "Epoch 41, loss: 2.267767\n",
            "Epoch 42, loss: 2.259617\n",
            "Epoch 43, loss: 2.267873\n",
            "Epoch 44, loss: 2.272168\n",
            "Epoch 45, loss: 2.266836\n",
            "Epoch 46, loss: 2.268960\n",
            "Epoch 47, loss: 2.269357\n",
            "Epoch 48, loss: 2.264782\n",
            "Epoch 49, loss: 2.265961\n",
            "Epoch 50, loss: 2.280483\n",
            "Epoch 51, loss: 2.259533\n",
            "Epoch 52, loss: 2.264892\n",
            "Epoch 53, loss: 2.256609\n",
            "Epoch 54, loss: 2.248879\n",
            "Epoch 55, loss: 2.251376\n",
            "Epoch 56, loss: 2.253678\n",
            "Epoch 57, loss: 2.267231\n",
            "Epoch 58, loss: 2.257284\n",
            "Epoch 59, loss: 2.268975\n",
            "Epoch 60, loss: 2.250073\n",
            "Epoch 61, loss: 2.261777\n",
            "Epoch 62, loss: 2.262873\n",
            "Epoch 63, loss: 2.254307\n",
            "Epoch 64, loss: 2.259389\n",
            "Epoch 65, loss: 2.258523\n",
            "Epoch 66, loss: 2.261560\n",
            "Epoch 67, loss: 2.255161\n",
            "Epoch 68, loss: 2.254355\n",
            "Epoch 69, loss: 2.254181\n",
            "Epoch 70, loss: 2.248065\n",
            "Epoch 71, loss: 2.243616\n",
            "Epoch 72, loss: 2.260203\n",
            "Epoch 73, loss: 2.239066\n",
            "Epoch 74, loss: 2.256707\n",
            "Epoch 75, loss: 2.250055\n",
            "Epoch 76, loss: 2.237469\n",
            "Epoch 77, loss: 2.267700\n",
            "Epoch 78, loss: 2.239461\n",
            "Epoch 79, loss: 2.239326\n",
            "Epoch 80, loss: 2.246172\n",
            "Epoch 81, loss: 2.241654\n",
            "Epoch 82, loss: 2.237945\n",
            "Epoch 83, loss: 2.240389\n",
            "Epoch 84, loss: 2.243187\n",
            "Epoch 85, loss: 2.230957\n",
            "Epoch 86, loss: 2.239925\n",
            "Epoch 87, loss: 2.226493\n",
            "Epoch 88, loss: 2.244319\n",
            "Epoch 89, loss: 2.239898\n",
            "Epoch 90, loss: 2.245543\n",
            "Epoch 91, loss: 2.245799\n",
            "Epoch 92, loss: 2.236727\n",
            "Epoch 93, loss: 2.228863\n",
            "Epoch 94, loss: 2.236924\n",
            "Epoch 95, loss: 2.264591\n",
            "Epoch 96, loss: 2.248150\n",
            "Epoch 97, loss: 2.240473\n",
            "Epoch 98, loss: 2.242029\n",
            "Epoch 99, loss: 2.247833\n",
            "Epoch 100, loss: 2.252075\n",
            "Epoch 101, loss: 2.239898\n",
            "Epoch 102, loss: 2.248158\n",
            "Epoch 103, loss: 2.230224\n",
            "Epoch 104, loss: 2.241661\n",
            "Epoch 105, loss: 2.248522\n",
            "Epoch 106, loss: 2.217385\n",
            "Epoch 107, loss: 2.263631\n",
            "Epoch 108, loss: 2.231181\n",
            "Epoch 109, loss: 2.226563\n",
            "Epoch 110, loss: 2.231417\n",
            "Epoch 111, loss: 2.243922\n",
            "Epoch 112, loss: 2.226485\n",
            "Epoch 113, loss: 2.247807\n",
            "Epoch 114, loss: 2.242212\n",
            "Epoch 115, loss: 2.239068\n",
            "Epoch 116, loss: 2.221787\n",
            "Epoch 117, loss: 2.212549\n",
            "Epoch 118, loss: 2.230725\n",
            "Epoch 119, loss: 2.238382\n",
            "Epoch 120, loss: 2.243643\n",
            "Epoch 121, loss: 2.230391\n",
            "Epoch 122, loss: 2.242836\n",
            "Epoch 123, loss: 2.221439\n",
            "Epoch 124, loss: 2.248753\n",
            "Epoch 125, loss: 2.229638\n",
            "Epoch 126, loss: 2.215462\n",
            "Epoch 127, loss: 2.214519\n",
            "Epoch 128, loss: 2.241230\n",
            "Epoch 129, loss: 2.241674\n",
            "Epoch 130, loss: 2.219345\n",
            "Epoch 131, loss: 2.223787\n",
            "Epoch 132, loss: 2.240341\n",
            "Epoch 133, loss: 2.213194\n",
            "Epoch 134, loss: 2.221916\n",
            "Epoch 135, loss: 2.222288\n",
            "Epoch 136, loss: 2.222591\n",
            "Epoch 137, loss: 2.219120\n",
            "Epoch 138, loss: 2.242575\n",
            "Epoch 139, loss: 2.233543\n",
            "Epoch 140, loss: 2.215047\n",
            "Epoch 141, loss: 2.221552\n",
            "Epoch 142, loss: 2.206979\n",
            "Epoch 143, loss: 2.205286\n",
            "Epoch 144, loss: 2.220486\n",
            "Epoch 145, loss: 2.206848\n",
            "Epoch 146, loss: 2.234461\n",
            "Epoch 147, loss: 2.215316\n",
            "Epoch 148, loss: 2.206100\n",
            "Epoch 149, loss: 2.243272\n",
            "Epoch 150, loss: 2.223789\n",
            "Epoch 151, loss: 2.226691\n",
            "Epoch 152, loss: 2.236993\n",
            "Epoch 153, loss: 2.229229\n",
            "Epoch 154, loss: 2.196413\n",
            "Epoch 155, loss: 2.232582\n",
            "Epoch 156, loss: 2.212957\n",
            "Epoch 157, loss: 2.202608\n",
            "Epoch 158, loss: 2.205973\n",
            "Epoch 159, loss: 2.206659\n",
            "Epoch 160, loss: 2.212186\n",
            "Epoch 161, loss: 2.195376\n",
            "Epoch 162, loss: 2.223637\n",
            "Epoch 163, loss: 2.209418\n",
            "Epoch 164, loss: 2.219684\n",
            "Epoch 165, loss: 2.180035\n",
            "Epoch 166, loss: 2.204521\n",
            "Epoch 167, loss: 2.197706\n",
            "Epoch 168, loss: 2.206338\n",
            "Epoch 169, loss: 2.239864\n",
            "Epoch 170, loss: 2.202578\n",
            "Epoch 171, loss: 2.214328\n",
            "Epoch 172, loss: 2.184319\n",
            "Epoch 173, loss: 2.227850\n",
            "Epoch 174, loss: 2.212667\n",
            "Epoch 175, loss: 2.199791\n",
            "Epoch 176, loss: 2.210378\n",
            "Epoch 177, loss: 2.197990\n",
            "Epoch 178, loss: 2.200535\n",
            "Epoch 179, loss: 2.189248\n",
            "Epoch 180, loss: 2.233714\n",
            "Epoch 181, loss: 2.210409\n",
            "Epoch 182, loss: 2.229704\n",
            "Epoch 183, loss: 2.228276\n",
            "Epoch 184, loss: 2.209957\n",
            "Epoch 185, loss: 2.210465\n",
            "Epoch 186, loss: 2.191730\n",
            "Epoch 187, loss: 2.198213\n",
            "Epoch 188, loss: 2.196020\n",
            "Epoch 189, loss: 2.228921\n",
            "Epoch 190, loss: 2.187653\n",
            "Epoch 191, loss: 2.187670\n",
            "Epoch 192, loss: 2.189648\n",
            "Epoch 193, loss: 2.190677\n",
            "Epoch 194, loss: 2.183412\n",
            "Epoch 195, loss: 2.196507\n",
            "Epoch 196, loss: 2.198137\n",
            "Epoch 197, loss: 2.205464\n",
            "Epoch 198, loss: 2.204786\n",
            "Epoch 199, loss: 2.214541\n",
            "Epoch 0, loss: 2.303008\n",
            "Epoch 1, loss: 2.302986\n",
            "Epoch 2, loss: 2.302174\n",
            "Epoch 3, loss: 2.301433\n",
            "Epoch 4, loss: 2.301682\n",
            "Epoch 5, loss: 2.302898\n",
            "Epoch 6, loss: 2.301718\n",
            "Epoch 7, loss: 2.301674\n",
            "Epoch 8, loss: 2.301008\n",
            "Epoch 9, loss: 2.300771\n",
            "Epoch 10, loss: 2.302072\n",
            "Epoch 11, loss: 2.301260\n",
            "Epoch 12, loss: 2.300881\n",
            "Epoch 13, loss: 2.300476\n",
            "Epoch 14, loss: 2.301154\n",
            "Epoch 15, loss: 2.300557\n",
            "Epoch 16, loss: 2.300947\n",
            "Epoch 17, loss: 2.299492\n",
            "Epoch 18, loss: 2.300797\n",
            "Epoch 19, loss: 2.300014\n",
            "Epoch 20, loss: 2.299767\n",
            "Epoch 21, loss: 2.300987\n",
            "Epoch 22, loss: 2.299714\n",
            "Epoch 23, loss: 2.300134\n",
            "Epoch 24, loss: 2.300307\n",
            "Epoch 25, loss: 2.300562\n",
            "Epoch 26, loss: 2.300476\n",
            "Epoch 27, loss: 2.300907\n",
            "Epoch 28, loss: 2.299912\n",
            "Epoch 29, loss: 2.298995\n",
            "Epoch 30, loss: 2.298817\n",
            "Epoch 31, loss: 2.297874\n",
            "Epoch 32, loss: 2.298905\n",
            "Epoch 33, loss: 2.300113\n",
            "Epoch 34, loss: 2.299327\n",
            "Epoch 35, loss: 2.297480\n",
            "Epoch 36, loss: 2.298330\n",
            "Epoch 37, loss: 2.300180\n",
            "Epoch 38, loss: 2.297322\n",
            "Epoch 39, loss: 2.298547\n",
            "Epoch 40, loss: 2.298685\n",
            "Epoch 41, loss: 2.298658\n",
            "Epoch 42, loss: 2.297646\n",
            "Epoch 43, loss: 2.296399\n",
            "Epoch 44, loss: 2.298055\n",
            "Epoch 45, loss: 2.296944\n",
            "Epoch 46, loss: 2.298838\n",
            "Epoch 47, loss: 2.298899\n",
            "Epoch 48, loss: 2.297544\n",
            "Epoch 49, loss: 2.297933\n",
            "Epoch 50, loss: 2.295444\n",
            "Epoch 51, loss: 2.297082\n",
            "Epoch 52, loss: 2.296035\n",
            "Epoch 53, loss: 2.298298\n",
            "Epoch 54, loss: 2.298542\n",
            "Epoch 55, loss: 2.297188\n",
            "Epoch 56, loss: 2.297900\n",
            "Epoch 57, loss: 2.293779\n",
            "Epoch 58, loss: 2.294472\n",
            "Epoch 59, loss: 2.296945\n",
            "Epoch 60, loss: 2.295507\n",
            "Epoch 61, loss: 2.293736\n",
            "Epoch 62, loss: 2.298125\n",
            "Epoch 63, loss: 2.294875\n",
            "Epoch 64, loss: 2.294894\n",
            "Epoch 65, loss: 2.298464\n",
            "Epoch 66, loss: 2.293667\n",
            "Epoch 67, loss: 2.298375\n",
            "Epoch 68, loss: 2.291483\n",
            "Epoch 69, loss: 2.296862\n",
            "Epoch 70, loss: 2.295886\n",
            "Epoch 71, loss: 2.294960\n",
            "Epoch 72, loss: 2.296278\n",
            "Epoch 73, loss: 2.294464\n",
            "Epoch 74, loss: 2.299248\n",
            "Epoch 75, loss: 2.295850\n",
            "Epoch 76, loss: 2.294265\n",
            "Epoch 77, loss: 2.299228\n",
            "Epoch 78, loss: 2.291558\n",
            "Epoch 79, loss: 2.293140\n",
            "Epoch 80, loss: 2.291949\n",
            "Epoch 81, loss: 2.294972\n",
            "Epoch 82, loss: 2.295619\n",
            "Epoch 83, loss: 2.294269\n",
            "Epoch 84, loss: 2.293339\n",
            "Epoch 85, loss: 2.293932\n",
            "Epoch 86, loss: 2.294926\n",
            "Epoch 87, loss: 2.293932\n",
            "Epoch 88, loss: 2.293488\n",
            "Epoch 89, loss: 2.299836\n",
            "Epoch 90, loss: 2.297509\n",
            "Epoch 91, loss: 2.294945\n",
            "Epoch 92, loss: 2.294188\n",
            "Epoch 93, loss: 2.293943\n",
            "Epoch 94, loss: 2.292604\n",
            "Epoch 95, loss: 2.296334\n",
            "Epoch 96, loss: 2.290747\n",
            "Epoch 97, loss: 2.289339\n",
            "Epoch 98, loss: 2.290850\n",
            "Epoch 99, loss: 2.293432\n",
            "Epoch 100, loss: 2.291416\n",
            "Epoch 101, loss: 2.291350\n",
            "Epoch 102, loss: 2.295070\n",
            "Epoch 103, loss: 2.293535\n",
            "Epoch 104, loss: 2.293822\n",
            "Epoch 105, loss: 2.293257\n",
            "Epoch 106, loss: 2.294453\n",
            "Epoch 107, loss: 2.292458\n",
            "Epoch 108, loss: 2.290445\n",
            "Epoch 109, loss: 2.297621\n",
            "Epoch 110, loss: 2.289806\n",
            "Epoch 111, loss: 2.292442\n",
            "Epoch 112, loss: 2.290939\n",
            "Epoch 113, loss: 2.290117\n",
            "Epoch 114, loss: 2.288286\n",
            "Epoch 115, loss: 2.294150\n",
            "Epoch 116, loss: 2.290343\n",
            "Epoch 117, loss: 2.292262\n",
            "Epoch 118, loss: 2.296696\n",
            "Epoch 119, loss: 2.291718\n",
            "Epoch 120, loss: 2.288620\n",
            "Epoch 121, loss: 2.290605\n",
            "Epoch 122, loss: 2.292929\n",
            "Epoch 123, loss: 2.292960\n",
            "Epoch 124, loss: 2.289448\n",
            "Epoch 125, loss: 2.290269\n",
            "Epoch 126, loss: 2.288915\n",
            "Epoch 127, loss: 2.296764\n",
            "Epoch 128, loss: 2.289981\n",
            "Epoch 129, loss: 2.289996\n",
            "Epoch 130, loss: 2.289984\n",
            "Epoch 131, loss: 2.291470\n",
            "Epoch 132, loss: 2.292003\n",
            "Epoch 133, loss: 2.293736\n",
            "Epoch 134, loss: 2.289773\n",
            "Epoch 135, loss: 2.288635\n",
            "Epoch 136, loss: 2.293831\n",
            "Epoch 137, loss: 2.291090\n",
            "Epoch 138, loss: 2.296927\n",
            "Epoch 139, loss: 2.287966\n",
            "Epoch 140, loss: 2.293047\n",
            "Epoch 141, loss: 2.288882\n",
            "Epoch 142, loss: 2.292127\n",
            "Epoch 143, loss: 2.291004\n",
            "Epoch 144, loss: 2.288105\n",
            "Epoch 145, loss: 2.290879\n",
            "Epoch 146, loss: 2.288591\n",
            "Epoch 147, loss: 2.290608\n",
            "Epoch 148, loss: 2.292839\n",
            "Epoch 149, loss: 2.290500\n",
            "Epoch 150, loss: 2.288704\n",
            "Epoch 151, loss: 2.291409\n",
            "Epoch 152, loss: 2.290649\n",
            "Epoch 153, loss: 2.287914\n",
            "Epoch 154, loss: 2.287748\n",
            "Epoch 155, loss: 2.290089\n",
            "Epoch 156, loss: 2.291389\n",
            "Epoch 157, loss: 2.287588\n",
            "Epoch 158, loss: 2.284145\n",
            "Epoch 159, loss: 2.287027\n",
            "Epoch 160, loss: 2.285536\n",
            "Epoch 161, loss: 2.284337\n",
            "Epoch 162, loss: 2.285338\n",
            "Epoch 163, loss: 2.288964\n",
            "Epoch 164, loss: 2.289558\n",
            "Epoch 165, loss: 2.290696\n",
            "Epoch 166, loss: 2.291374\n",
            "Epoch 167, loss: 2.286063\n",
            "Epoch 168, loss: 2.288461\n",
            "Epoch 169, loss: 2.290816\n",
            "Epoch 170, loss: 2.288438\n",
            "Epoch 171, loss: 2.289743\n",
            "Epoch 172, loss: 2.289418\n",
            "Epoch 173, loss: 2.287167\n",
            "Epoch 174, loss: 2.286050\n",
            "Epoch 175, loss: 2.286895\n",
            "Epoch 176, loss: 2.286165\n",
            "Epoch 177, loss: 2.282299\n",
            "Epoch 178, loss: 2.287970\n",
            "Epoch 179, loss: 2.285995\n",
            "Epoch 180, loss: 2.282196\n",
            "Epoch 181, loss: 2.286841\n",
            "Epoch 182, loss: 2.290611\n",
            "Epoch 183, loss: 2.284722\n",
            "Epoch 184, loss: 2.285767\n",
            "Epoch 185, loss: 2.286656\n",
            "Epoch 186, loss: 2.284294\n",
            "Epoch 187, loss: 2.290389\n",
            "Epoch 188, loss: 2.291727\n",
            "Epoch 189, loss: 2.278220\n",
            "Epoch 190, loss: 2.286288\n",
            "Epoch 191, loss: 2.289638\n",
            "Epoch 192, loss: 2.285994\n",
            "Epoch 193, loss: 2.286406\n",
            "Epoch 194, loss: 2.283501\n",
            "Epoch 195, loss: 2.285561\n",
            "Epoch 196, loss: 2.282750\n",
            "Epoch 197, loss: 2.284612\n",
            "Epoch 198, loss: 2.289075\n",
            "Epoch 199, loss: 2.284348\n",
            "Epoch 0, loss: 2.302828\n",
            "Epoch 1, loss: 2.302799\n",
            "Epoch 2, loss: 2.302156\n",
            "Epoch 3, loss: 2.301943\n",
            "Epoch 4, loss: 2.302024\n",
            "Epoch 5, loss: 2.301272\n",
            "Epoch 6, loss: 2.302669\n",
            "Epoch 7, loss: 2.300680\n",
            "Epoch 8, loss: 2.301294\n",
            "Epoch 9, loss: 2.300517\n",
            "Epoch 10, loss: 2.302264\n",
            "Epoch 11, loss: 2.301141\n",
            "Epoch 12, loss: 2.300815\n",
            "Epoch 13, loss: 2.301857\n",
            "Epoch 14, loss: 2.301177\n",
            "Epoch 15, loss: 2.301148\n",
            "Epoch 16, loss: 2.301079\n",
            "Epoch 17, loss: 2.300813\n",
            "Epoch 18, loss: 2.299761\n",
            "Epoch 19, loss: 2.299676\n",
            "Epoch 20, loss: 2.299129\n",
            "Epoch 21, loss: 2.299052\n",
            "Epoch 22, loss: 2.298944\n",
            "Epoch 23, loss: 2.299898\n",
            "Epoch 24, loss: 2.299585\n",
            "Epoch 25, loss: 2.300027\n",
            "Epoch 26, loss: 2.299878\n",
            "Epoch 27, loss: 2.299902\n",
            "Epoch 28, loss: 2.299557\n",
            "Epoch 29, loss: 2.297768\n",
            "Epoch 30, loss: 2.301344\n",
            "Epoch 31, loss: 2.302431\n",
            "Epoch 32, loss: 2.297655\n",
            "Epoch 33, loss: 2.299662\n",
            "Epoch 34, loss: 2.297433\n",
            "Epoch 35, loss: 2.298330\n",
            "Epoch 36, loss: 2.296972\n",
            "Epoch 37, loss: 2.300661\n",
            "Epoch 38, loss: 2.298616\n",
            "Epoch 39, loss: 2.299506\n",
            "Epoch 40, loss: 2.300898\n",
            "Epoch 41, loss: 2.298902\n",
            "Epoch 42, loss: 2.298752\n",
            "Epoch 43, loss: 2.299937\n",
            "Epoch 44, loss: 2.298698\n",
            "Epoch 45, loss: 2.295172\n",
            "Epoch 46, loss: 2.295489\n",
            "Epoch 47, loss: 2.296347\n",
            "Epoch 48, loss: 2.296284\n",
            "Epoch 49, loss: 2.297325\n",
            "Epoch 50, loss: 2.296464\n",
            "Epoch 51, loss: 2.297360\n",
            "Epoch 52, loss: 2.296969\n",
            "Epoch 53, loss: 2.298289\n",
            "Epoch 54, loss: 2.301361\n",
            "Epoch 55, loss: 2.297648\n",
            "Epoch 56, loss: 2.296656\n",
            "Epoch 57, loss: 2.294404\n",
            "Epoch 58, loss: 2.295288\n",
            "Epoch 59, loss: 2.295756\n",
            "Epoch 60, loss: 2.296587\n",
            "Epoch 61, loss: 2.295804\n",
            "Epoch 62, loss: 2.296711\n",
            "Epoch 63, loss: 2.294163\n",
            "Epoch 64, loss: 2.297983\n",
            "Epoch 65, loss: 2.296682\n",
            "Epoch 66, loss: 2.294269\n",
            "Epoch 67, loss: 2.295577\n",
            "Epoch 68, loss: 2.295062\n",
            "Epoch 69, loss: 2.298860\n",
            "Epoch 70, loss: 2.294213\n",
            "Epoch 71, loss: 2.298939\n",
            "Epoch 72, loss: 2.295815\n",
            "Epoch 73, loss: 2.297017\n",
            "Epoch 74, loss: 2.298140\n",
            "Epoch 75, loss: 2.292651\n",
            "Epoch 76, loss: 2.294143\n",
            "Epoch 77, loss: 2.295140\n",
            "Epoch 78, loss: 2.296371\n",
            "Epoch 79, loss: 2.296632\n",
            "Epoch 80, loss: 2.296793\n",
            "Epoch 81, loss: 2.293169\n",
            "Epoch 82, loss: 2.293327\n",
            "Epoch 83, loss: 2.296761\n",
            "Epoch 84, loss: 2.294282\n",
            "Epoch 85, loss: 2.292702\n",
            "Epoch 86, loss: 2.297552\n",
            "Epoch 87, loss: 2.294562\n",
            "Epoch 88, loss: 2.291506\n",
            "Epoch 89, loss: 2.295135\n",
            "Epoch 90, loss: 2.293400\n",
            "Epoch 91, loss: 2.298090\n",
            "Epoch 92, loss: 2.293019\n",
            "Epoch 93, loss: 2.294427\n",
            "Epoch 94, loss: 2.292211\n",
            "Epoch 95, loss: 2.297226\n",
            "Epoch 96, loss: 2.292640\n",
            "Epoch 97, loss: 2.292847\n",
            "Epoch 98, loss: 2.295084\n",
            "Epoch 99, loss: 2.294173\n",
            "Epoch 100, loss: 2.296848\n",
            "Epoch 101, loss: 2.291048\n",
            "Epoch 102, loss: 2.296759\n",
            "Epoch 103, loss: 2.296911\n",
            "Epoch 104, loss: 2.294402\n",
            "Epoch 105, loss: 2.292616\n",
            "Epoch 106, loss: 2.293371\n",
            "Epoch 107, loss: 2.294466\n",
            "Epoch 108, loss: 2.292977\n",
            "Epoch 109, loss: 2.289781\n",
            "Epoch 110, loss: 2.293754\n",
            "Epoch 111, loss: 2.293109\n",
            "Epoch 112, loss: 2.293873\n",
            "Epoch 113, loss: 2.292549\n",
            "Epoch 114, loss: 2.290998\n",
            "Epoch 115, loss: 2.294281\n",
            "Epoch 116, loss: 2.288655\n",
            "Epoch 117, loss: 2.292656\n",
            "Epoch 118, loss: 2.290483\n",
            "Epoch 119, loss: 2.292820\n",
            "Epoch 120, loss: 2.292146\n",
            "Epoch 121, loss: 2.292456\n",
            "Epoch 122, loss: 2.291609\n",
            "Epoch 123, loss: 2.292076\n",
            "Epoch 124, loss: 2.292688\n",
            "Epoch 125, loss: 2.290205\n",
            "Epoch 126, loss: 2.291537\n",
            "Epoch 127, loss: 2.290361\n",
            "Epoch 128, loss: 2.285506\n",
            "Epoch 129, loss: 2.289402\n",
            "Epoch 130, loss: 2.289051\n",
            "Epoch 131, loss: 2.289834\n",
            "Epoch 132, loss: 2.292114\n",
            "Epoch 133, loss: 2.287646\n",
            "Epoch 134, loss: 2.293062\n",
            "Epoch 135, loss: 2.290243\n",
            "Epoch 136, loss: 2.292893\n",
            "Epoch 137, loss: 2.290444\n",
            "Epoch 138, loss: 2.289579\n",
            "Epoch 139, loss: 2.287978\n",
            "Epoch 140, loss: 2.292172\n",
            "Epoch 141, loss: 2.287628\n",
            "Epoch 142, loss: 2.291972\n",
            "Epoch 143, loss: 2.290488\n",
            "Epoch 144, loss: 2.289548\n",
            "Epoch 145, loss: 2.288559\n",
            "Epoch 146, loss: 2.291836\n",
            "Epoch 147, loss: 2.289035\n",
            "Epoch 148, loss: 2.292004\n",
            "Epoch 149, loss: 2.292784\n",
            "Epoch 150, loss: 2.292833\n",
            "Epoch 151, loss: 2.290508\n",
            "Epoch 152, loss: 2.294513\n",
            "Epoch 153, loss: 2.286770\n",
            "Epoch 154, loss: 2.288837\n",
            "Epoch 155, loss: 2.286939\n",
            "Epoch 156, loss: 2.287610\n",
            "Epoch 157, loss: 2.285251\n",
            "Epoch 158, loss: 2.292266\n",
            "Epoch 159, loss: 2.292369\n",
            "Epoch 160, loss: 2.285813\n",
            "Epoch 161, loss: 2.285703\n",
            "Epoch 162, loss: 2.289878\n",
            "Epoch 163, loss: 2.292157\n",
            "Epoch 164, loss: 2.288801\n",
            "Epoch 165, loss: 2.289031\n",
            "Epoch 166, loss: 2.289768\n",
            "Epoch 167, loss: 2.287233\n",
            "Epoch 168, loss: 2.289051\n",
            "Epoch 169, loss: 2.286500\n",
            "Epoch 170, loss: 2.285023\n",
            "Epoch 171, loss: 2.288280\n",
            "Epoch 172, loss: 2.292102\n",
            "Epoch 173, loss: 2.290869\n",
            "Epoch 174, loss: 2.287234\n",
            "Epoch 175, loss: 2.287745\n",
            "Epoch 176, loss: 2.280051\n",
            "Epoch 177, loss: 2.286870\n",
            "Epoch 178, loss: 2.290439\n",
            "Epoch 179, loss: 2.292073\n",
            "Epoch 180, loss: 2.287308\n",
            "Epoch 181, loss: 2.285830\n",
            "Epoch 182, loss: 2.289566\n",
            "Epoch 183, loss: 2.289733\n",
            "Epoch 184, loss: 2.287026\n",
            "Epoch 185, loss: 2.287754\n",
            "Epoch 186, loss: 2.286353\n",
            "Epoch 187, loss: 2.285631\n",
            "Epoch 188, loss: 2.287211\n",
            "Epoch 189, loss: 2.281717\n",
            "Epoch 190, loss: 2.289977\n",
            "Epoch 191, loss: 2.285811\n",
            "Epoch 192, loss: 2.279041\n",
            "Epoch 193, loss: 2.285391\n",
            "Epoch 194, loss: 2.284726\n",
            "Epoch 195, loss: 2.288797\n",
            "Epoch 196, loss: 2.288313\n",
            "Epoch 197, loss: 2.279564\n",
            "Epoch 198, loss: 2.286280\n",
            "Epoch 199, loss: 2.281329\n",
            "Epoch 0, loss: 2.301555\n",
            "Epoch 1, loss: 2.302832\n",
            "Epoch 2, loss: 2.303225\n",
            "Epoch 3, loss: 2.302475\n",
            "Epoch 4, loss: 2.302507\n",
            "Epoch 5, loss: 2.302345\n",
            "Epoch 6, loss: 2.302675\n",
            "Epoch 7, loss: 2.301375\n",
            "Epoch 8, loss: 2.302268\n",
            "Epoch 9, loss: 2.302885\n",
            "Epoch 10, loss: 2.301243\n",
            "Epoch 11, loss: 2.301214\n",
            "Epoch 12, loss: 2.301579\n",
            "Epoch 13, loss: 2.300856\n",
            "Epoch 14, loss: 2.301623\n",
            "Epoch 15, loss: 2.299552\n",
            "Epoch 16, loss: 2.302378\n",
            "Epoch 17, loss: 2.299976\n",
            "Epoch 18, loss: 2.300661\n",
            "Epoch 19, loss: 2.300213\n",
            "Epoch 20, loss: 2.300873\n",
            "Epoch 21, loss: 2.300752\n",
            "Epoch 22, loss: 2.299351\n",
            "Epoch 23, loss: 2.300629\n",
            "Epoch 24, loss: 2.300218\n",
            "Epoch 25, loss: 2.299485\n",
            "Epoch 26, loss: 2.300545\n",
            "Epoch 27, loss: 2.297596\n",
            "Epoch 28, loss: 2.299680\n",
            "Epoch 29, loss: 2.299788\n",
            "Epoch 30, loss: 2.300038\n",
            "Epoch 31, loss: 2.300159\n",
            "Epoch 32, loss: 2.299634\n",
            "Epoch 33, loss: 2.300374\n",
            "Epoch 34, loss: 2.298631\n",
            "Epoch 35, loss: 2.299173\n",
            "Epoch 36, loss: 2.297427\n",
            "Epoch 37, loss: 2.298439\n",
            "Epoch 38, loss: 2.299349\n",
            "Epoch 39, loss: 2.297579\n",
            "Epoch 40, loss: 2.296651\n",
            "Epoch 41, loss: 2.298139\n",
            "Epoch 42, loss: 2.299259\n",
            "Epoch 43, loss: 2.297156\n",
            "Epoch 44, loss: 2.297925\n",
            "Epoch 45, loss: 2.299244\n",
            "Epoch 46, loss: 2.298310\n",
            "Epoch 47, loss: 2.297356\n",
            "Epoch 48, loss: 2.296898\n",
            "Epoch 49, loss: 2.297072\n",
            "Epoch 50, loss: 2.296222\n",
            "Epoch 51, loss: 2.296923\n",
            "Epoch 52, loss: 2.296295\n",
            "Epoch 53, loss: 2.297683\n",
            "Epoch 54, loss: 2.297405\n",
            "Epoch 55, loss: 2.296109\n",
            "Epoch 56, loss: 2.300488\n",
            "Epoch 57, loss: 2.298617\n",
            "Epoch 58, loss: 2.297364\n",
            "Epoch 59, loss: 2.298289\n",
            "Epoch 60, loss: 2.295545\n",
            "Epoch 61, loss: 2.296438\n",
            "Epoch 62, loss: 2.294708\n",
            "Epoch 63, loss: 2.293583\n",
            "Epoch 64, loss: 2.294702\n",
            "Epoch 65, loss: 2.297363\n",
            "Epoch 66, loss: 2.297318\n",
            "Epoch 67, loss: 2.296748\n",
            "Epoch 68, loss: 2.297330\n",
            "Epoch 69, loss: 2.293482\n",
            "Epoch 70, loss: 2.294396\n",
            "Epoch 71, loss: 2.294938\n",
            "Epoch 72, loss: 2.293054\n",
            "Epoch 73, loss: 2.296365\n",
            "Epoch 74, loss: 2.295856\n",
            "Epoch 75, loss: 2.295420\n",
            "Epoch 76, loss: 2.295700\n",
            "Epoch 77, loss: 2.295725\n",
            "Epoch 78, loss: 2.292394\n",
            "Epoch 79, loss: 2.292240\n",
            "Epoch 80, loss: 2.295559\n",
            "Epoch 81, loss: 2.296166\n",
            "Epoch 82, loss: 2.294610\n",
            "Epoch 83, loss: 2.292272\n",
            "Epoch 84, loss: 2.292527\n",
            "Epoch 85, loss: 2.294681\n",
            "Epoch 86, loss: 2.293327\n",
            "Epoch 87, loss: 2.293719\n",
            "Epoch 88, loss: 2.294151\n",
            "Epoch 89, loss: 2.293124\n",
            "Epoch 90, loss: 2.293318\n",
            "Epoch 91, loss: 2.292411\n",
            "Epoch 92, loss: 2.295394\n",
            "Epoch 93, loss: 2.289551\n",
            "Epoch 94, loss: 2.295827\n",
            "Epoch 95, loss: 2.290073\n",
            "Epoch 96, loss: 2.292523\n",
            "Epoch 97, loss: 2.292879\n",
            "Epoch 98, loss: 2.288883\n",
            "Epoch 99, loss: 2.294289\n",
            "Epoch 100, loss: 2.292829\n",
            "Epoch 101, loss: 2.294361\n",
            "Epoch 102, loss: 2.293682\n",
            "Epoch 103, loss: 2.294887\n",
            "Epoch 104, loss: 2.295750\n",
            "Epoch 105, loss: 2.291867\n",
            "Epoch 106, loss: 2.294793\n",
            "Epoch 107, loss: 2.289769\n",
            "Epoch 108, loss: 2.297445\n",
            "Epoch 109, loss: 2.290846\n",
            "Epoch 110, loss: 2.296720\n",
            "Epoch 111, loss: 2.293156\n",
            "Epoch 112, loss: 2.290434\n",
            "Epoch 113, loss: 2.294129\n",
            "Epoch 114, loss: 2.291709\n",
            "Epoch 115, loss: 2.290304\n",
            "Epoch 116, loss: 2.289596\n",
            "Epoch 117, loss: 2.293922\n",
            "Epoch 118, loss: 2.290386\n",
            "Epoch 119, loss: 2.295130\n",
            "Epoch 120, loss: 2.293314\n",
            "Epoch 121, loss: 2.295273\n",
            "Epoch 122, loss: 2.291159\n",
            "Epoch 123, loss: 2.290275\n",
            "Epoch 124, loss: 2.290100\n",
            "Epoch 125, loss: 2.290754\n",
            "Epoch 126, loss: 2.292126\n",
            "Epoch 127, loss: 2.292374\n",
            "Epoch 128, loss: 2.290367\n",
            "Epoch 129, loss: 2.294225\n",
            "Epoch 130, loss: 2.290231\n",
            "Epoch 131, loss: 2.294135\n",
            "Epoch 132, loss: 2.289415\n",
            "Epoch 133, loss: 2.290096\n",
            "Epoch 134, loss: 2.290567\n",
            "Epoch 135, loss: 2.288197\n",
            "Epoch 136, loss: 2.289303\n",
            "Epoch 137, loss: 2.293200\n",
            "Epoch 138, loss: 2.287374\n",
            "Epoch 139, loss: 2.287558\n",
            "Epoch 140, loss: 2.292712\n",
            "Epoch 141, loss: 2.287908\n",
            "Epoch 142, loss: 2.293674\n",
            "Epoch 143, loss: 2.285390\n",
            "Epoch 144, loss: 2.286573\n",
            "Epoch 145, loss: 2.295387\n",
            "Epoch 146, loss: 2.285240\n",
            "Epoch 147, loss: 2.293741\n",
            "Epoch 148, loss: 2.293473\n",
            "Epoch 149, loss: 2.289395\n",
            "Epoch 150, loss: 2.294772\n",
            "Epoch 151, loss: 2.292821\n",
            "Epoch 152, loss: 2.287476\n",
            "Epoch 153, loss: 2.289208\n",
            "Epoch 154, loss: 2.284243\n",
            "Epoch 155, loss: 2.292788\n",
            "Epoch 156, loss: 2.289537\n",
            "Epoch 157, loss: 2.285228\n",
            "Epoch 158, loss: 2.288086\n",
            "Epoch 159, loss: 2.289554\n",
            "Epoch 160, loss: 2.289377\n",
            "Epoch 161, loss: 2.288030\n",
            "Epoch 162, loss: 2.282756\n",
            "Epoch 163, loss: 2.288320\n",
            "Epoch 164, loss: 2.292714\n",
            "Epoch 165, loss: 2.289615\n",
            "Epoch 166, loss: 2.288533\n",
            "Epoch 167, loss: 2.289396\n",
            "Epoch 168, loss: 2.283705\n",
            "Epoch 169, loss: 2.289309\n",
            "Epoch 170, loss: 2.288938\n",
            "Epoch 171, loss: 2.289968\n",
            "Epoch 172, loss: 2.288544\n",
            "Epoch 173, loss: 2.289140\n",
            "Epoch 174, loss: 2.287107\n",
            "Epoch 175, loss: 2.283441\n",
            "Epoch 176, loss: 2.284302\n",
            "Epoch 177, loss: 2.292349\n",
            "Epoch 178, loss: 2.283575\n",
            "Epoch 179, loss: 2.288706\n",
            "Epoch 180, loss: 2.286612\n",
            "Epoch 181, loss: 2.288286\n",
            "Epoch 182, loss: 2.282992\n",
            "Epoch 183, loss: 2.284957\n",
            "Epoch 184, loss: 2.287497\n",
            "Epoch 185, loss: 2.288968\n",
            "Epoch 186, loss: 2.285725\n",
            "Epoch 187, loss: 2.289961\n",
            "Epoch 188, loss: 2.282678\n",
            "Epoch 189, loss: 2.279339\n",
            "Epoch 190, loss: 2.286137\n",
            "Epoch 191, loss: 2.285886\n",
            "Epoch 192, loss: 2.288735\n",
            "Epoch 193, loss: 2.283007\n",
            "Epoch 194, loss: 2.289210\n",
            "Epoch 195, loss: 2.281718\n",
            "Epoch 196, loss: 2.285106\n",
            "Epoch 197, loss: 2.285674\n",
            "Epoch 198, loss: 2.282427\n",
            "Epoch 199, loss: 2.279673\n",
            "Epoch 0, loss: 2.303046\n",
            "Epoch 1, loss: 2.302565\n",
            "Epoch 2, loss: 2.302698\n",
            "Epoch 3, loss: 2.302307\n",
            "Epoch 4, loss: 2.301857\n",
            "Epoch 5, loss: 2.301303\n",
            "Epoch 6, loss: 2.302553\n",
            "Epoch 7, loss: 2.301686\n",
            "Epoch 8, loss: 2.301432\n",
            "Epoch 9, loss: 2.302604\n",
            "Epoch 10, loss: 2.300840\n",
            "Epoch 11, loss: 2.300768\n",
            "Epoch 12, loss: 2.302488\n",
            "Epoch 13, loss: 2.299884\n",
            "Epoch 14, loss: 2.300329\n",
            "Epoch 15, loss: 2.301872\n",
            "Epoch 16, loss: 2.300221\n",
            "Epoch 17, loss: 2.300405\n",
            "Epoch 18, loss: 2.302305\n",
            "Epoch 19, loss: 2.301686\n",
            "Epoch 20, loss: 2.299029\n",
            "Epoch 21, loss: 2.300018\n",
            "Epoch 22, loss: 2.301106\n",
            "Epoch 23, loss: 2.299528\n",
            "Epoch 24, loss: 2.298947\n",
            "Epoch 25, loss: 2.299630\n",
            "Epoch 26, loss: 2.299602\n",
            "Epoch 27, loss: 2.300585\n",
            "Epoch 28, loss: 2.300946\n",
            "Epoch 29, loss: 2.298334\n",
            "Epoch 30, loss: 2.298424\n",
            "Epoch 31, loss: 2.298918\n",
            "Epoch 32, loss: 2.298870\n",
            "Epoch 33, loss: 2.297651\n",
            "Epoch 34, loss: 2.299594\n",
            "Epoch 35, loss: 2.299133\n",
            "Epoch 36, loss: 2.300760\n",
            "Epoch 37, loss: 2.300296\n",
            "Epoch 38, loss: 2.299899\n",
            "Epoch 39, loss: 2.298096\n",
            "Epoch 40, loss: 2.299336\n",
            "Epoch 41, loss: 2.297499\n",
            "Epoch 42, loss: 2.298268\n",
            "Epoch 43, loss: 2.300811\n",
            "Epoch 44, loss: 2.295691\n",
            "Epoch 45, loss: 2.296907\n",
            "Epoch 46, loss: 2.297298\n",
            "Epoch 47, loss: 2.296427\n",
            "Epoch 48, loss: 2.299838\n",
            "Epoch 49, loss: 2.299020\n",
            "Epoch 50, loss: 2.296709\n",
            "Epoch 51, loss: 2.295889\n",
            "Epoch 52, loss: 2.298133\n",
            "Epoch 53, loss: 2.295464\n",
            "Epoch 54, loss: 2.298450\n",
            "Epoch 55, loss: 2.296838\n",
            "Epoch 56, loss: 2.294390\n",
            "Epoch 57, loss: 2.297114\n",
            "Epoch 58, loss: 2.298224\n",
            "Epoch 59, loss: 2.296849\n",
            "Epoch 60, loss: 2.297419\n",
            "Epoch 61, loss: 2.296241\n",
            "Epoch 62, loss: 2.295356\n",
            "Epoch 63, loss: 2.295153\n",
            "Epoch 64, loss: 2.299265\n",
            "Epoch 65, loss: 2.295350\n",
            "Epoch 66, loss: 2.295968\n",
            "Epoch 67, loss: 2.298121\n",
            "Epoch 68, loss: 2.294102\n",
            "Epoch 69, loss: 2.296722\n",
            "Epoch 70, loss: 2.296468\n",
            "Epoch 71, loss: 2.294141\n",
            "Epoch 72, loss: 2.295741\n",
            "Epoch 73, loss: 2.294057\n",
            "Epoch 74, loss: 2.291728\n",
            "Epoch 75, loss: 2.293751\n",
            "Epoch 76, loss: 2.294174\n",
            "Epoch 77, loss: 2.293314\n",
            "Epoch 78, loss: 2.293483\n",
            "Epoch 79, loss: 2.295233\n",
            "Epoch 80, loss: 2.294728\n",
            "Epoch 81, loss: 2.296199\n",
            "Epoch 82, loss: 2.295704\n",
            "Epoch 83, loss: 2.292861\n",
            "Epoch 84, loss: 2.292943\n",
            "Epoch 85, loss: 2.293564\n",
            "Epoch 86, loss: 2.294083\n",
            "Epoch 87, loss: 2.292839\n",
            "Epoch 88, loss: 2.298280\n",
            "Epoch 89, loss: 2.298014\n",
            "Epoch 90, loss: 2.293503\n",
            "Epoch 91, loss: 2.293881\n",
            "Epoch 92, loss: 2.298362\n",
            "Epoch 93, loss: 2.294302\n",
            "Epoch 94, loss: 2.293808\n",
            "Epoch 95, loss: 2.294860\n",
            "Epoch 96, loss: 2.292805\n",
            "Epoch 97, loss: 2.294994\n",
            "Epoch 98, loss: 2.293517\n",
            "Epoch 99, loss: 2.291920\n",
            "Epoch 100, loss: 2.293893\n",
            "Epoch 101, loss: 2.296882\n",
            "Epoch 102, loss: 2.295381\n",
            "Epoch 103, loss: 2.291060\n",
            "Epoch 104, loss: 2.293758\n",
            "Epoch 105, loss: 2.291867\n",
            "Epoch 106, loss: 2.292810\n",
            "Epoch 107, loss: 2.294182\n",
            "Epoch 108, loss: 2.294109\n",
            "Epoch 109, loss: 2.295418\n",
            "Epoch 110, loss: 2.290964\n",
            "Epoch 111, loss: 2.289737\n",
            "Epoch 112, loss: 2.291550\n",
            "Epoch 113, loss: 2.292459\n",
            "Epoch 114, loss: 2.288664\n",
            "Epoch 115, loss: 2.295426\n",
            "Epoch 116, loss: 2.290591\n",
            "Epoch 117, loss: 2.293382\n",
            "Epoch 118, loss: 2.291953\n",
            "Epoch 119, loss: 2.296448\n",
            "Epoch 120, loss: 2.289505\n",
            "Epoch 121, loss: 2.291820\n",
            "Epoch 122, loss: 2.289743\n",
            "Epoch 123, loss: 2.291405\n",
            "Epoch 124, loss: 2.290771\n",
            "Epoch 125, loss: 2.290737\n",
            "Epoch 126, loss: 2.292687\n",
            "Epoch 127, loss: 2.288136\n",
            "Epoch 128, loss: 2.290830\n",
            "Epoch 129, loss: 2.289255\n",
            "Epoch 130, loss: 2.288763\n",
            "Epoch 131, loss: 2.292490\n",
            "Epoch 132, loss: 2.290313\n",
            "Epoch 133, loss: 2.287058\n",
            "Epoch 134, loss: 2.291402\n",
            "Epoch 135, loss: 2.290792\n",
            "Epoch 136, loss: 2.290716\n",
            "Epoch 137, loss: 2.287352\n",
            "Epoch 138, loss: 2.292307\n",
            "Epoch 139, loss: 2.287254\n",
            "Epoch 140, loss: 2.289771\n",
            "Epoch 141, loss: 2.290021\n",
            "Epoch 142, loss: 2.291414\n",
            "Epoch 143, loss: 2.294259\n",
            "Epoch 144, loss: 2.290392\n",
            "Epoch 145, loss: 2.291895\n",
            "Epoch 146, loss: 2.287676\n",
            "Epoch 147, loss: 2.287970\n",
            "Epoch 148, loss: 2.284926\n",
            "Epoch 149, loss: 2.292989\n",
            "Epoch 150, loss: 2.290737\n",
            "Epoch 151, loss: 2.286666\n",
            "Epoch 152, loss: 2.290626\n",
            "Epoch 153, loss: 2.285781\n",
            "Epoch 154, loss: 2.286064\n",
            "Epoch 155, loss: 2.286821\n",
            "Epoch 156, loss: 2.289063\n",
            "Epoch 157, loss: 2.287128\n",
            "Epoch 158, loss: 2.286512\n",
            "Epoch 159, loss: 2.284687\n",
            "Epoch 160, loss: 2.289204\n",
            "Epoch 161, loss: 2.289003\n",
            "Epoch 162, loss: 2.289630\n",
            "Epoch 163, loss: 2.289296\n",
            "Epoch 164, loss: 2.289685\n",
            "Epoch 165, loss: 2.287956\n",
            "Epoch 166, loss: 2.283906\n",
            "Epoch 167, loss: 2.285754\n",
            "Epoch 168, loss: 2.290567\n",
            "Epoch 169, loss: 2.290587\n",
            "Epoch 170, loss: 2.290119\n",
            "Epoch 171, loss: 2.286635\n",
            "Epoch 172, loss: 2.286386\n",
            "Epoch 173, loss: 2.283006\n",
            "Epoch 174, loss: 2.284041\n",
            "Epoch 175, loss: 2.289368\n",
            "Epoch 176, loss: 2.289040\n",
            "Epoch 177, loss: 2.288280\n",
            "Epoch 178, loss: 2.285966\n",
            "Epoch 179, loss: 2.288560\n",
            "Epoch 180, loss: 2.286273\n",
            "Epoch 181, loss: 2.285031\n",
            "Epoch 182, loss: 2.287616\n",
            "Epoch 183, loss: 2.284552\n",
            "Epoch 184, loss: 2.287017\n",
            "Epoch 185, loss: 2.287705\n",
            "Epoch 186, loss: 2.285876\n",
            "Epoch 187, loss: 2.290271\n",
            "Epoch 188, loss: 2.291123\n",
            "Epoch 189, loss: 2.287313\n",
            "Epoch 190, loss: 2.285588\n",
            "Epoch 191, loss: 2.290920\n",
            "Epoch 192, loss: 2.283979\n",
            "Epoch 193, loss: 2.284557\n",
            "Epoch 194, loss: 2.284251\n",
            "Epoch 195, loss: 2.290601\n",
            "Epoch 196, loss: 2.287306\n",
            "Epoch 197, loss: 2.281717\n",
            "Epoch 198, loss: 2.282996\n",
            "Epoch 199, loss: 2.284814\n",
            "Epoch 0, loss: 2.302803\n",
            "Epoch 1, loss: 2.302594\n",
            "Epoch 2, loss: 2.302751\n",
            "Epoch 3, loss: 2.303780\n",
            "Epoch 4, loss: 2.302108\n",
            "Epoch 5, loss: 2.301491\n",
            "Epoch 6, loss: 2.301260\n",
            "Epoch 7, loss: 2.302273\n",
            "Epoch 8, loss: 2.302870\n",
            "Epoch 9, loss: 2.302546\n",
            "Epoch 10, loss: 2.302646\n",
            "Epoch 11, loss: 2.302390\n",
            "Epoch 12, loss: 2.303705\n",
            "Epoch 13, loss: 2.302790\n",
            "Epoch 14, loss: 2.301983\n",
            "Epoch 15, loss: 2.301771\n",
            "Epoch 16, loss: 2.301608\n",
            "Epoch 17, loss: 2.303347\n",
            "Epoch 18, loss: 2.303206\n",
            "Epoch 19, loss: 2.302300\n",
            "Epoch 20, loss: 2.302651\n",
            "Epoch 21, loss: 2.303191\n",
            "Epoch 22, loss: 2.302033\n",
            "Epoch 23, loss: 2.303938\n",
            "Epoch 24, loss: 2.302751\n",
            "Epoch 25, loss: 2.302367\n",
            "Epoch 26, loss: 2.301513\n",
            "Epoch 27, loss: 2.301224\n",
            "Epoch 28, loss: 2.302540\n",
            "Epoch 29, loss: 2.301615\n",
            "Epoch 30, loss: 2.302679\n",
            "Epoch 31, loss: 2.302660\n",
            "Epoch 32, loss: 2.303497\n",
            "Epoch 33, loss: 2.302423\n",
            "Epoch 34, loss: 2.301730\n",
            "Epoch 35, loss: 2.302876\n",
            "Epoch 36, loss: 2.302362\n",
            "Epoch 37, loss: 2.302204\n",
            "Epoch 38, loss: 2.302250\n",
            "Epoch 39, loss: 2.301884\n",
            "Epoch 40, loss: 2.302955\n",
            "Epoch 41, loss: 2.301685\n",
            "Epoch 42, loss: 2.302018\n",
            "Epoch 43, loss: 2.301574\n",
            "Epoch 44, loss: 2.302409\n",
            "Epoch 45, loss: 2.302501\n",
            "Epoch 46, loss: 2.301952\n",
            "Epoch 47, loss: 2.302515\n",
            "Epoch 48, loss: 2.302663\n",
            "Epoch 49, loss: 2.301773\n",
            "Epoch 50, loss: 2.302004\n",
            "Epoch 51, loss: 2.302309\n",
            "Epoch 52, loss: 2.302377\n",
            "Epoch 53, loss: 2.301919\n",
            "Epoch 54, loss: 2.302390\n",
            "Epoch 55, loss: 2.301761\n",
            "Epoch 56, loss: 2.302064\n",
            "Epoch 57, loss: 2.301307\n",
            "Epoch 58, loss: 2.301571\n",
            "Epoch 59, loss: 2.301962\n",
            "Epoch 60, loss: 2.302274\n",
            "Epoch 61, loss: 2.302381\n",
            "Epoch 62, loss: 2.301145\n",
            "Epoch 63, loss: 2.301849\n",
            "Epoch 64, loss: 2.302045\n",
            "Epoch 65, loss: 2.301613\n",
            "Epoch 66, loss: 2.301814\n",
            "Epoch 67, loss: 2.302223\n",
            "Epoch 68, loss: 2.301580\n",
            "Epoch 69, loss: 2.301765\n",
            "Epoch 70, loss: 2.301757\n",
            "Epoch 71, loss: 2.301405\n",
            "Epoch 72, loss: 2.300816\n",
            "Epoch 73, loss: 2.301316\n",
            "Epoch 74, loss: 2.301242\n",
            "Epoch 75, loss: 2.301061\n",
            "Epoch 76, loss: 2.303086\n",
            "Epoch 77, loss: 2.300540\n",
            "Epoch 78, loss: 2.301503\n",
            "Epoch 79, loss: 2.301328\n",
            "Epoch 80, loss: 2.301369\n",
            "Epoch 81, loss: 2.301975\n",
            "Epoch 82, loss: 2.301981\n",
            "Epoch 83, loss: 2.302388\n",
            "Epoch 84, loss: 2.301267\n",
            "Epoch 85, loss: 2.301894\n",
            "Epoch 86, loss: 2.300835\n",
            "Epoch 87, loss: 2.301576\n",
            "Epoch 88, loss: 2.300677\n",
            "Epoch 89, loss: 2.300712\n",
            "Epoch 90, loss: 2.301559\n",
            "Epoch 91, loss: 2.302051\n",
            "Epoch 92, loss: 2.301784\n",
            "Epoch 93, loss: 2.302146\n",
            "Epoch 94, loss: 2.302250\n",
            "Epoch 95, loss: 2.301422\n",
            "Epoch 96, loss: 2.301493\n",
            "Epoch 97, loss: 2.301665\n",
            "Epoch 98, loss: 2.300616\n",
            "Epoch 99, loss: 2.301983\n",
            "Epoch 100, loss: 2.302065\n",
            "Epoch 101, loss: 2.302030\n",
            "Epoch 102, loss: 2.302090\n",
            "Epoch 103, loss: 2.301081\n",
            "Epoch 104, loss: 2.301371\n",
            "Epoch 105, loss: 2.301413\n",
            "Epoch 106, loss: 2.301553\n",
            "Epoch 107, loss: 2.301350\n",
            "Epoch 108, loss: 2.301381\n",
            "Epoch 109, loss: 2.301389\n",
            "Epoch 110, loss: 2.301491\n",
            "Epoch 111, loss: 2.301092\n",
            "Epoch 112, loss: 2.300809\n",
            "Epoch 113, loss: 2.301165\n",
            "Epoch 114, loss: 2.301554\n",
            "Epoch 115, loss: 2.301033\n",
            "Epoch 116, loss: 2.300903\n",
            "Epoch 117, loss: 2.300801\n",
            "Epoch 118, loss: 2.301473\n",
            "Epoch 119, loss: 2.302086\n",
            "Epoch 120, loss: 2.301334\n",
            "Epoch 121, loss: 2.300982\n",
            "Epoch 122, loss: 2.301535\n",
            "Epoch 123, loss: 2.299407\n",
            "Epoch 124, loss: 2.301854\n",
            "Epoch 125, loss: 2.302193\n",
            "Epoch 126, loss: 2.300996\n",
            "Epoch 127, loss: 2.300786\n",
            "Epoch 128, loss: 2.301599\n",
            "Epoch 129, loss: 2.301506\n",
            "Epoch 130, loss: 2.300932\n",
            "Epoch 131, loss: 2.301677\n",
            "Epoch 132, loss: 2.300185\n",
            "Epoch 133, loss: 2.301807\n",
            "Epoch 134, loss: 2.301442\n",
            "Epoch 135, loss: 2.302087\n",
            "Epoch 136, loss: 2.302052\n",
            "Epoch 137, loss: 2.301268\n",
            "Epoch 138, loss: 2.300457\n",
            "Epoch 139, loss: 2.300715\n",
            "Epoch 140, loss: 2.300728\n",
            "Epoch 141, loss: 2.300661\n",
            "Epoch 142, loss: 2.299935\n",
            "Epoch 143, loss: 2.300856\n",
            "Epoch 144, loss: 2.301625\n",
            "Epoch 145, loss: 2.301271\n",
            "Epoch 146, loss: 2.301503\n",
            "Epoch 147, loss: 2.301819\n",
            "Epoch 148, loss: 2.299639\n",
            "Epoch 149, loss: 2.301827\n",
            "Epoch 150, loss: 2.301234\n",
            "Epoch 151, loss: 2.302961\n",
            "Epoch 152, loss: 2.300694\n",
            "Epoch 153, loss: 2.301290\n",
            "Epoch 154, loss: 2.301304\n",
            "Epoch 155, loss: 2.300572\n",
            "Epoch 156, loss: 2.301117\n",
            "Epoch 157, loss: 2.302026\n",
            "Epoch 158, loss: 2.300293\n",
            "Epoch 159, loss: 2.300088\n",
            "Epoch 160, loss: 2.300636\n",
            "Epoch 161, loss: 2.302064\n",
            "Epoch 162, loss: 2.301281\n",
            "Epoch 163, loss: 2.300769\n",
            "Epoch 164, loss: 2.300858\n",
            "Epoch 165, loss: 2.300172\n",
            "Epoch 166, loss: 2.300794\n",
            "Epoch 167, loss: 2.300555\n",
            "Epoch 168, loss: 2.301319\n",
            "Epoch 169, loss: 2.300023\n",
            "Epoch 170, loss: 2.300941\n",
            "Epoch 171, loss: 2.301746\n",
            "Epoch 172, loss: 2.301511\n",
            "Epoch 173, loss: 2.300218\n",
            "Epoch 174, loss: 2.301017\n",
            "Epoch 175, loss: 2.300137\n",
            "Epoch 176, loss: 2.300083\n",
            "Epoch 177, loss: 2.301609\n",
            "Epoch 178, loss: 2.300760\n",
            "Epoch 179, loss: 2.301131\n",
            "Epoch 180, loss: 2.300423\n",
            "Epoch 181, loss: 2.300176\n",
            "Epoch 182, loss: 2.299792\n",
            "Epoch 183, loss: 2.300303\n",
            "Epoch 184, loss: 2.300477\n",
            "Epoch 185, loss: 2.299818\n",
            "Epoch 186, loss: 2.301489\n",
            "Epoch 187, loss: 2.300481\n",
            "Epoch 188, loss: 2.300169\n",
            "Epoch 189, loss: 2.300491\n",
            "Epoch 190, loss: 2.302833\n",
            "Epoch 191, loss: 2.301012\n",
            "Epoch 192, loss: 2.301430\n",
            "Epoch 193, loss: 2.299235\n",
            "Epoch 194, loss: 2.300417\n",
            "Epoch 195, loss: 2.300367\n",
            "Epoch 196, loss: 2.299358\n",
            "Epoch 197, loss: 2.300591\n",
            "Epoch 198, loss: 2.300284\n",
            "Epoch 199, loss: 2.301891\n",
            "Epoch 0, loss: 2.302584\n",
            "Epoch 1, loss: 2.301974\n",
            "Epoch 2, loss: 2.302792\n",
            "Epoch 3, loss: 2.302491\n",
            "Epoch 4, loss: 2.301453\n",
            "Epoch 5, loss: 2.301931\n",
            "Epoch 6, loss: 2.302460\n",
            "Epoch 7, loss: 2.302214\n",
            "Epoch 8, loss: 2.301881\n",
            "Epoch 9, loss: 2.301647\n",
            "Epoch 10, loss: 2.302135\n",
            "Epoch 11, loss: 2.302367\n",
            "Epoch 12, loss: 2.301237\n",
            "Epoch 13, loss: 2.302903\n",
            "Epoch 14, loss: 2.302333\n",
            "Epoch 15, loss: 2.302544\n",
            "Epoch 16, loss: 2.301369\n",
            "Epoch 17, loss: 2.303362\n",
            "Epoch 18, loss: 2.302609\n",
            "Epoch 19, loss: 2.302332\n",
            "Epoch 20, loss: 2.302906\n",
            "Epoch 21, loss: 2.302389\n",
            "Epoch 22, loss: 2.302323\n",
            "Epoch 23, loss: 2.302649\n",
            "Epoch 24, loss: 2.301810\n",
            "Epoch 25, loss: 2.301850\n",
            "Epoch 26, loss: 2.302137\n",
            "Epoch 27, loss: 2.302667\n",
            "Epoch 28, loss: 2.301484\n",
            "Epoch 29, loss: 2.303082\n",
            "Epoch 30, loss: 2.302042\n",
            "Epoch 31, loss: 2.302673\n",
            "Epoch 32, loss: 2.302009\n",
            "Epoch 33, loss: 2.302002\n",
            "Epoch 34, loss: 2.302958\n",
            "Epoch 35, loss: 2.302861\n",
            "Epoch 36, loss: 2.302288\n",
            "Epoch 37, loss: 2.303162\n",
            "Epoch 38, loss: 2.302226\n",
            "Epoch 39, loss: 2.301722\n",
            "Epoch 40, loss: 2.301492\n",
            "Epoch 41, loss: 2.300809\n",
            "Epoch 42, loss: 2.302641\n",
            "Epoch 43, loss: 2.303214\n",
            "Epoch 44, loss: 2.302801\n",
            "Epoch 45, loss: 2.303221\n",
            "Epoch 46, loss: 2.302631\n",
            "Epoch 47, loss: 2.301028\n",
            "Epoch 48, loss: 2.301996\n",
            "Epoch 49, loss: 2.302661\n",
            "Epoch 50, loss: 2.302513\n",
            "Epoch 51, loss: 2.300624\n",
            "Epoch 52, loss: 2.301742\n",
            "Epoch 53, loss: 2.302113\n",
            "Epoch 54, loss: 2.301381\n",
            "Epoch 55, loss: 2.301704\n",
            "Epoch 56, loss: 2.301770\n",
            "Epoch 57, loss: 2.301813\n",
            "Epoch 58, loss: 2.301683\n",
            "Epoch 59, loss: 2.301073\n",
            "Epoch 60, loss: 2.301059\n",
            "Epoch 61, loss: 2.300398\n",
            "Epoch 62, loss: 2.302015\n",
            "Epoch 63, loss: 2.302068\n",
            "Epoch 64, loss: 2.303458\n",
            "Epoch 65, loss: 2.301331\n",
            "Epoch 66, loss: 2.302415\n",
            "Epoch 67, loss: 2.301107\n",
            "Epoch 68, loss: 2.301664\n",
            "Epoch 69, loss: 2.302400\n",
            "Epoch 70, loss: 2.301830\n",
            "Epoch 71, loss: 2.302734\n",
            "Epoch 72, loss: 2.300978\n",
            "Epoch 73, loss: 2.301639\n",
            "Epoch 74, loss: 2.302073\n",
            "Epoch 75, loss: 2.301323\n",
            "Epoch 76, loss: 2.301759\n",
            "Epoch 77, loss: 2.302425\n",
            "Epoch 78, loss: 2.303074\n",
            "Epoch 79, loss: 2.301265\n",
            "Epoch 80, loss: 2.301658\n",
            "Epoch 81, loss: 2.300703\n",
            "Epoch 82, loss: 2.301813\n",
            "Epoch 83, loss: 2.301330\n",
            "Epoch 84, loss: 2.302015\n",
            "Epoch 85, loss: 2.301091\n",
            "Epoch 86, loss: 2.301525\n",
            "Epoch 87, loss: 2.301335\n",
            "Epoch 88, loss: 2.301388\n",
            "Epoch 89, loss: 2.302241\n",
            "Epoch 90, loss: 2.301860\n",
            "Epoch 91, loss: 2.300051\n",
            "Epoch 92, loss: 2.301364\n",
            "Epoch 93, loss: 2.301914\n",
            "Epoch 94, loss: 2.302105\n",
            "Epoch 95, loss: 2.300463\n",
            "Epoch 96, loss: 2.302067\n",
            "Epoch 97, loss: 2.301268\n",
            "Epoch 98, loss: 2.301609\n",
            "Epoch 99, loss: 2.301084\n",
            "Epoch 100, loss: 2.301233\n",
            "Epoch 101, loss: 2.302215\n",
            "Epoch 102, loss: 2.301794\n",
            "Epoch 103, loss: 2.301253\n",
            "Epoch 104, loss: 2.301744\n",
            "Epoch 105, loss: 2.301686\n",
            "Epoch 106, loss: 2.301695\n",
            "Epoch 107, loss: 2.302015\n",
            "Epoch 108, loss: 2.301870\n",
            "Epoch 109, loss: 2.301315\n",
            "Epoch 110, loss: 2.301286\n",
            "Epoch 111, loss: 2.302390\n",
            "Epoch 112, loss: 2.300951\n",
            "Epoch 113, loss: 2.302002\n",
            "Epoch 114, loss: 2.302123\n",
            "Epoch 115, loss: 2.300556\n",
            "Epoch 116, loss: 2.300435\n",
            "Epoch 117, loss: 2.300653\n",
            "Epoch 118, loss: 2.301531\n",
            "Epoch 119, loss: 2.300408\n",
            "Epoch 120, loss: 2.301797\n",
            "Epoch 121, loss: 2.299960\n",
            "Epoch 122, loss: 2.302234\n",
            "Epoch 123, loss: 2.301742\n",
            "Epoch 124, loss: 2.300935\n",
            "Epoch 125, loss: 2.301327\n",
            "Epoch 126, loss: 2.301576\n",
            "Epoch 127, loss: 2.299835\n",
            "Epoch 128, loss: 2.301344\n",
            "Epoch 129, loss: 2.301517\n",
            "Epoch 130, loss: 2.301089\n",
            "Epoch 131, loss: 2.301350\n",
            "Epoch 132, loss: 2.300635\n",
            "Epoch 133, loss: 2.301314\n",
            "Epoch 134, loss: 2.301786\n",
            "Epoch 135, loss: 2.300157\n",
            "Epoch 136, loss: 2.301552\n",
            "Epoch 137, loss: 2.300670\n",
            "Epoch 138, loss: 2.300961\n",
            "Epoch 139, loss: 2.301321\n",
            "Epoch 140, loss: 2.300050\n",
            "Epoch 141, loss: 2.301174\n",
            "Epoch 142, loss: 2.300723\n",
            "Epoch 143, loss: 2.301886\n",
            "Epoch 144, loss: 2.300357\n",
            "Epoch 145, loss: 2.300361\n",
            "Epoch 146, loss: 2.301696\n",
            "Epoch 147, loss: 2.300543\n",
            "Epoch 148, loss: 2.301338\n",
            "Epoch 149, loss: 2.300032\n",
            "Epoch 150, loss: 2.301102\n",
            "Epoch 151, loss: 2.300889\n",
            "Epoch 152, loss: 2.300514\n",
            "Epoch 153, loss: 2.302258\n",
            "Epoch 154, loss: 2.301655\n",
            "Epoch 155, loss: 2.301232\n",
            "Epoch 156, loss: 2.301407\n",
            "Epoch 157, loss: 2.300659\n",
            "Epoch 158, loss: 2.301027\n",
            "Epoch 159, loss: 2.300027\n",
            "Epoch 160, loss: 2.301028\n",
            "Epoch 161, loss: 2.299306\n",
            "Epoch 162, loss: 2.300407\n",
            "Epoch 163, loss: 2.301746\n",
            "Epoch 164, loss: 2.301744\n",
            "Epoch 165, loss: 2.300972\n",
            "Epoch 166, loss: 2.299376\n",
            "Epoch 167, loss: 2.301374\n",
            "Epoch 168, loss: 2.300273\n",
            "Epoch 169, loss: 2.301019\n",
            "Epoch 170, loss: 2.299936\n",
            "Epoch 171, loss: 2.301465\n",
            "Epoch 172, loss: 2.299051\n",
            "Epoch 173, loss: 2.299776\n",
            "Epoch 174, loss: 2.302094\n",
            "Epoch 175, loss: 2.299153\n",
            "Epoch 176, loss: 2.299948\n",
            "Epoch 177, loss: 2.300700\n",
            "Epoch 178, loss: 2.298112\n",
            "Epoch 179, loss: 2.301768\n",
            "Epoch 180, loss: 2.301056\n",
            "Epoch 181, loss: 2.299289\n",
            "Epoch 182, loss: 2.300308\n",
            "Epoch 183, loss: 2.301520\n",
            "Epoch 184, loss: 2.302139\n",
            "Epoch 185, loss: 2.300929\n",
            "Epoch 186, loss: 2.300015\n",
            "Epoch 187, loss: 2.301209\n",
            "Epoch 188, loss: 2.301683\n",
            "Epoch 189, loss: 2.299557\n",
            "Epoch 190, loss: 2.300300\n",
            "Epoch 191, loss: 2.299719\n",
            "Epoch 192, loss: 2.301978\n",
            "Epoch 193, loss: 2.299863\n",
            "Epoch 194, loss: 2.298418\n",
            "Epoch 195, loss: 2.299860\n",
            "Epoch 196, loss: 2.300305\n",
            "Epoch 197, loss: 2.301402\n",
            "Epoch 198, loss: 2.299859\n",
            "Epoch 199, loss: 2.300950\n",
            "Epoch 0, loss: 2.303523\n",
            "Epoch 1, loss: 2.304027\n",
            "Epoch 2, loss: 2.302532\n",
            "Epoch 3, loss: 2.301476\n",
            "Epoch 4, loss: 2.303271\n",
            "Epoch 5, loss: 2.302493\n",
            "Epoch 6, loss: 2.302565\n",
            "Epoch 7, loss: 2.301952\n",
            "Epoch 8, loss: 2.302376\n",
            "Epoch 9, loss: 2.302205\n",
            "Epoch 10, loss: 2.302702\n",
            "Epoch 11, loss: 2.302681\n",
            "Epoch 12, loss: 2.303173\n",
            "Epoch 13, loss: 2.302401\n",
            "Epoch 14, loss: 2.302250\n",
            "Epoch 15, loss: 2.303565\n",
            "Epoch 16, loss: 2.302809\n",
            "Epoch 17, loss: 2.303120\n",
            "Epoch 18, loss: 2.303200\n",
            "Epoch 19, loss: 2.302048\n",
            "Epoch 20, loss: 2.302913\n",
            "Epoch 21, loss: 2.302610\n",
            "Epoch 22, loss: 2.303046\n",
            "Epoch 23, loss: 2.304246\n",
            "Epoch 24, loss: 2.301012\n",
            "Epoch 25, loss: 2.302527\n",
            "Epoch 26, loss: 2.301687\n",
            "Epoch 27, loss: 2.303663\n",
            "Epoch 28, loss: 2.302890\n",
            "Epoch 29, loss: 2.303433\n",
            "Epoch 30, loss: 2.301736\n",
            "Epoch 31, loss: 2.302544\n",
            "Epoch 32, loss: 2.301395\n",
            "Epoch 33, loss: 2.302044\n",
            "Epoch 34, loss: 2.302426\n",
            "Epoch 35, loss: 2.301868\n",
            "Epoch 36, loss: 2.301907\n",
            "Epoch 37, loss: 2.301434\n",
            "Epoch 38, loss: 2.302530\n",
            "Epoch 39, loss: 2.302490\n",
            "Epoch 40, loss: 2.302170\n",
            "Epoch 41, loss: 2.301683\n",
            "Epoch 42, loss: 2.302658\n",
            "Epoch 43, loss: 2.302106\n",
            "Epoch 44, loss: 2.302558\n",
            "Epoch 45, loss: 2.302549\n",
            "Epoch 46, loss: 2.302254\n",
            "Epoch 47, loss: 2.301782\n",
            "Epoch 48, loss: 2.303038\n",
            "Epoch 49, loss: 2.301246\n",
            "Epoch 50, loss: 2.301444\n",
            "Epoch 51, loss: 2.302374\n",
            "Epoch 52, loss: 2.303319\n",
            "Epoch 53, loss: 2.302199\n",
            "Epoch 54, loss: 2.302731\n",
            "Epoch 55, loss: 2.301641\n",
            "Epoch 56, loss: 2.301976\n",
            "Epoch 57, loss: 2.301222\n",
            "Epoch 58, loss: 2.302680\n",
            "Epoch 59, loss: 2.302208\n",
            "Epoch 60, loss: 2.302188\n",
            "Epoch 61, loss: 2.301533\n",
            "Epoch 62, loss: 2.303977\n",
            "Epoch 63, loss: 2.302321\n",
            "Epoch 64, loss: 2.301443\n",
            "Epoch 65, loss: 2.302196\n",
            "Epoch 66, loss: 2.301197\n",
            "Epoch 67, loss: 2.301202\n",
            "Epoch 68, loss: 2.303026\n",
            "Epoch 69, loss: 2.302435\n",
            "Epoch 70, loss: 2.300011\n",
            "Epoch 71, loss: 2.302495\n",
            "Epoch 72, loss: 2.301465\n",
            "Epoch 73, loss: 2.302918\n",
            "Epoch 74, loss: 2.301998\n",
            "Epoch 75, loss: 2.303163\n",
            "Epoch 76, loss: 2.302322\n",
            "Epoch 77, loss: 2.299815\n",
            "Epoch 78, loss: 2.302621\n",
            "Epoch 79, loss: 2.303084\n",
            "Epoch 80, loss: 2.301654\n",
            "Epoch 81, loss: 2.300915\n",
            "Epoch 82, loss: 2.301368\n",
            "Epoch 83, loss: 2.302118\n",
            "Epoch 84, loss: 2.301520\n",
            "Epoch 85, loss: 2.301486\n",
            "Epoch 86, loss: 2.302191\n",
            "Epoch 87, loss: 2.302317\n",
            "Epoch 88, loss: 2.301877\n",
            "Epoch 89, loss: 2.301614\n",
            "Epoch 90, loss: 2.301931\n",
            "Epoch 91, loss: 2.301785\n",
            "Epoch 92, loss: 2.301547\n",
            "Epoch 93, loss: 2.301683\n",
            "Epoch 94, loss: 2.301262\n",
            "Epoch 95, loss: 2.301276\n",
            "Epoch 96, loss: 2.301882\n",
            "Epoch 97, loss: 2.302285\n",
            "Epoch 98, loss: 2.301993\n",
            "Epoch 99, loss: 2.302717\n",
            "Epoch 100, loss: 2.301156\n",
            "Epoch 101, loss: 2.301472\n",
            "Epoch 102, loss: 2.301085\n",
            "Epoch 103, loss: 2.301017\n",
            "Epoch 104, loss: 2.301402\n",
            "Epoch 105, loss: 2.301363\n",
            "Epoch 106, loss: 2.301478\n",
            "Epoch 107, loss: 2.302216\n",
            "Epoch 108, loss: 2.300449\n",
            "Epoch 109, loss: 2.303131\n",
            "Epoch 110, loss: 2.302862\n",
            "Epoch 111, loss: 2.301238\n",
            "Epoch 112, loss: 2.300846\n",
            "Epoch 113, loss: 2.301395\n",
            "Epoch 114, loss: 2.302408\n",
            "Epoch 115, loss: 2.300922\n",
            "Epoch 116, loss: 2.301899\n",
            "Epoch 117, loss: 2.301785\n",
            "Epoch 118, loss: 2.300728\n",
            "Epoch 119, loss: 2.302052\n",
            "Epoch 120, loss: 2.302043\n",
            "Epoch 121, loss: 2.301017\n",
            "Epoch 122, loss: 2.301797\n",
            "Epoch 123, loss: 2.300542\n",
            "Epoch 124, loss: 2.302000\n",
            "Epoch 125, loss: 2.300669\n",
            "Epoch 126, loss: 2.302173\n",
            "Epoch 127, loss: 2.301151\n",
            "Epoch 128, loss: 2.301734\n",
            "Epoch 129, loss: 2.301295\n",
            "Epoch 130, loss: 2.302068\n",
            "Epoch 131, loss: 2.300395\n",
            "Epoch 132, loss: 2.301209\n",
            "Epoch 133, loss: 2.299941\n",
            "Epoch 134, loss: 2.301531\n",
            "Epoch 135, loss: 2.301278\n",
            "Epoch 136, loss: 2.300893\n",
            "Epoch 137, loss: 2.299789\n",
            "Epoch 138, loss: 2.301314\n",
            "Epoch 139, loss: 2.301479\n",
            "Epoch 140, loss: 2.301851\n",
            "Epoch 141, loss: 2.302287\n",
            "Epoch 142, loss: 2.300479\n",
            "Epoch 143, loss: 2.300374\n",
            "Epoch 144, loss: 2.302897\n",
            "Epoch 145, loss: 2.301354\n",
            "Epoch 146, loss: 2.301218\n",
            "Epoch 147, loss: 2.302189\n",
            "Epoch 148, loss: 2.302478\n",
            "Epoch 149, loss: 2.301413\n",
            "Epoch 150, loss: 2.299985\n",
            "Epoch 151, loss: 2.301271\n",
            "Epoch 152, loss: 2.300542\n",
            "Epoch 153, loss: 2.302062\n",
            "Epoch 154, loss: 2.300894\n",
            "Epoch 155, loss: 2.300713\n",
            "Epoch 156, loss: 2.302529\n",
            "Epoch 157, loss: 2.301105\n",
            "Epoch 158, loss: 2.301448\n",
            "Epoch 159, loss: 2.302636\n",
            "Epoch 160, loss: 2.301984\n",
            "Epoch 161, loss: 2.299217\n",
            "Epoch 162, loss: 2.302336\n",
            "Epoch 163, loss: 2.300352\n",
            "Epoch 164, loss: 2.301686\n",
            "Epoch 165, loss: 2.299003\n",
            "Epoch 166, loss: 2.300949\n",
            "Epoch 167, loss: 2.300088\n",
            "Epoch 168, loss: 2.301714\n",
            "Epoch 169, loss: 2.301313\n",
            "Epoch 170, loss: 2.300962\n",
            "Epoch 171, loss: 2.301734\n",
            "Epoch 172, loss: 2.301139\n",
            "Epoch 173, loss: 2.300949\n",
            "Epoch 174, loss: 2.299778\n",
            "Epoch 175, loss: 2.301569\n",
            "Epoch 176, loss: 2.302068\n",
            "Epoch 177, loss: 2.299584\n",
            "Epoch 178, loss: 2.300937\n",
            "Epoch 179, loss: 2.301649\n",
            "Epoch 180, loss: 2.302057\n",
            "Epoch 181, loss: 2.300303\n",
            "Epoch 182, loss: 2.299590\n",
            "Epoch 183, loss: 2.300329\n",
            "Epoch 184, loss: 2.300164\n",
            "Epoch 185, loss: 2.301085\n",
            "Epoch 186, loss: 2.300929\n",
            "Epoch 187, loss: 2.301238\n",
            "Epoch 188, loss: 2.300587\n",
            "Epoch 189, loss: 2.301534\n",
            "Epoch 190, loss: 2.299653\n",
            "Epoch 191, loss: 2.299719\n",
            "Epoch 192, loss: 2.300591\n",
            "Epoch 193, loss: 2.299462\n",
            "Epoch 194, loss: 2.301148\n",
            "Epoch 195, loss: 2.299867\n",
            "Epoch 196, loss: 2.301811\n",
            "Epoch 197, loss: 2.301838\n",
            "Epoch 198, loss: 2.300660\n",
            "Epoch 199, loss: 2.301507\n",
            "Epoch 0, loss: 2.303080\n",
            "Epoch 1, loss: 2.303109\n",
            "Epoch 2, loss: 2.302411\n",
            "Epoch 3, loss: 2.302648\n",
            "Epoch 4, loss: 2.303774\n",
            "Epoch 5, loss: 2.303440\n",
            "Epoch 6, loss: 2.301882\n",
            "Epoch 7, loss: 2.302875\n",
            "Epoch 8, loss: 2.302444\n",
            "Epoch 9, loss: 2.303169\n",
            "Epoch 10, loss: 2.302702\n",
            "Epoch 11, loss: 2.301937\n",
            "Epoch 12, loss: 2.302646\n",
            "Epoch 13, loss: 2.302323\n",
            "Epoch 14, loss: 2.303898\n",
            "Epoch 15, loss: 2.303007\n",
            "Epoch 16, loss: 2.302404\n",
            "Epoch 17, loss: 2.302450\n",
            "Epoch 18, loss: 2.302169\n",
            "Epoch 19, loss: 2.301172\n",
            "Epoch 20, loss: 2.302872\n",
            "Epoch 21, loss: 2.302142\n",
            "Epoch 22, loss: 2.303054\n",
            "Epoch 23, loss: 2.302646\n",
            "Epoch 24, loss: 2.302529\n",
            "Epoch 25, loss: 2.302900\n",
            "Epoch 26, loss: 2.302819\n",
            "Epoch 27, loss: 2.302917\n",
            "Epoch 28, loss: 2.301996\n",
            "Epoch 29, loss: 2.302288\n",
            "Epoch 30, loss: 2.303634\n",
            "Epoch 31, loss: 2.302649\n",
            "Epoch 32, loss: 2.301769\n",
            "Epoch 33, loss: 2.303110\n",
            "Epoch 34, loss: 2.302306\n",
            "Epoch 35, loss: 2.301766\n",
            "Epoch 36, loss: 2.302731\n",
            "Epoch 37, loss: 2.302394\n",
            "Epoch 38, loss: 2.302305\n",
            "Epoch 39, loss: 2.302916\n",
            "Epoch 40, loss: 2.302185\n",
            "Epoch 41, loss: 2.302087\n",
            "Epoch 42, loss: 2.302740\n",
            "Epoch 43, loss: 2.302289\n",
            "Epoch 44, loss: 2.303197\n",
            "Epoch 45, loss: 2.302667\n",
            "Epoch 46, loss: 2.301147\n",
            "Epoch 47, loss: 2.301515\n",
            "Epoch 48, loss: 2.302159\n",
            "Epoch 49, loss: 2.303331\n",
            "Epoch 50, loss: 2.301927\n",
            "Epoch 51, loss: 2.302522\n",
            "Epoch 52, loss: 2.301697\n",
            "Epoch 53, loss: 2.302637\n",
            "Epoch 54, loss: 2.302215\n",
            "Epoch 55, loss: 2.302064\n",
            "Epoch 56, loss: 2.301275\n",
            "Epoch 57, loss: 2.303065\n",
            "Epoch 58, loss: 2.301669\n",
            "Epoch 59, loss: 2.302255\n",
            "Epoch 60, loss: 2.301814\n",
            "Epoch 61, loss: 2.302158\n",
            "Epoch 62, loss: 2.301904\n",
            "Epoch 63, loss: 2.301644\n",
            "Epoch 64, loss: 2.301483\n",
            "Epoch 65, loss: 2.301968\n",
            "Epoch 66, loss: 2.301291\n",
            "Epoch 67, loss: 2.301701\n",
            "Epoch 68, loss: 2.302161\n",
            "Epoch 69, loss: 2.301459\n",
            "Epoch 70, loss: 2.300689\n",
            "Epoch 71, loss: 2.301213\n",
            "Epoch 72, loss: 2.301704\n",
            "Epoch 73, loss: 2.301272\n",
            "Epoch 74, loss: 2.301898\n",
            "Epoch 75, loss: 2.303620\n",
            "Epoch 76, loss: 2.300972\n",
            "Epoch 77, loss: 2.302811\n",
            "Epoch 78, loss: 2.300235\n",
            "Epoch 79, loss: 2.302282\n",
            "Epoch 80, loss: 2.302031\n",
            "Epoch 81, loss: 2.302390\n",
            "Epoch 82, loss: 2.302368\n",
            "Epoch 83, loss: 2.300931\n",
            "Epoch 84, loss: 2.302473\n",
            "Epoch 85, loss: 2.301938\n",
            "Epoch 86, loss: 2.301546\n",
            "Epoch 87, loss: 2.302141\n",
            "Epoch 88, loss: 2.302003\n",
            "Epoch 89, loss: 2.301529\n",
            "Epoch 90, loss: 2.301459\n",
            "Epoch 91, loss: 2.300741\n",
            "Epoch 92, loss: 2.301938\n",
            "Epoch 93, loss: 2.301457\n",
            "Epoch 94, loss: 2.301792\n",
            "Epoch 95, loss: 2.301436\n",
            "Epoch 96, loss: 2.301435\n",
            "Epoch 97, loss: 2.301244\n",
            "Epoch 98, loss: 2.301488\n",
            "Epoch 99, loss: 2.301391\n",
            "Epoch 100, loss: 2.301482\n",
            "Epoch 101, loss: 2.301585\n",
            "Epoch 102, loss: 2.301397\n",
            "Epoch 103, loss: 2.301174\n",
            "Epoch 104, loss: 2.301860\n",
            "Epoch 105, loss: 2.302165\n",
            "Epoch 106, loss: 2.301384\n",
            "Epoch 107, loss: 2.301519\n",
            "Epoch 108, loss: 2.301991\n",
            "Epoch 109, loss: 2.301646\n",
            "Epoch 110, loss: 2.301335\n",
            "Epoch 111, loss: 2.301598\n",
            "Epoch 112, loss: 2.301672\n",
            "Epoch 113, loss: 2.300434\n",
            "Epoch 114, loss: 2.301665\n",
            "Epoch 115, loss: 2.301596\n",
            "Epoch 116, loss: 2.301265\n",
            "Epoch 117, loss: 2.300664\n",
            "Epoch 118, loss: 2.301519\n",
            "Epoch 119, loss: 2.302146\n",
            "Epoch 120, loss: 2.302532\n",
            "Epoch 121, loss: 2.300621\n",
            "Epoch 122, loss: 2.300477\n",
            "Epoch 123, loss: 2.301925\n",
            "Epoch 124, loss: 2.302667\n",
            "Epoch 125, loss: 2.300048\n",
            "Epoch 126, loss: 2.300489\n",
            "Epoch 127, loss: 2.301974\n",
            "Epoch 128, loss: 2.301220\n",
            "Epoch 129, loss: 2.300651\n",
            "Epoch 130, loss: 2.302271\n",
            "Epoch 131, loss: 2.301566\n",
            "Epoch 132, loss: 2.300931\n",
            "Epoch 133, loss: 2.301687\n",
            "Epoch 134, loss: 2.301923\n",
            "Epoch 135, loss: 2.302409\n",
            "Epoch 136, loss: 2.301575\n",
            "Epoch 137, loss: 2.301628\n",
            "Epoch 138, loss: 2.300952\n",
            "Epoch 139, loss: 2.301565\n",
            "Epoch 140, loss: 2.301655\n",
            "Epoch 141, loss: 2.302406\n",
            "Epoch 142, loss: 2.302234\n",
            "Epoch 143, loss: 2.300528\n",
            "Epoch 144, loss: 2.300282\n",
            "Epoch 145, loss: 2.301380\n",
            "Epoch 146, loss: 2.300897\n",
            "Epoch 147, loss: 2.301111\n",
            "Epoch 148, loss: 2.300678\n",
            "Epoch 149, loss: 2.299863\n",
            "Epoch 150, loss: 2.300684\n",
            "Epoch 151, loss: 2.299655\n",
            "Epoch 152, loss: 2.300022\n",
            "Epoch 153, loss: 2.300897\n",
            "Epoch 154, loss: 2.301690\n",
            "Epoch 155, loss: 2.300497\n",
            "Epoch 156, loss: 2.300899\n",
            "Epoch 157, loss: 2.301653\n",
            "Epoch 158, loss: 2.300641\n",
            "Epoch 159, loss: 2.302020\n",
            "Epoch 160, loss: 2.301496\n",
            "Epoch 161, loss: 2.300753\n",
            "Epoch 162, loss: 2.301026\n",
            "Epoch 163, loss: 2.300094\n",
            "Epoch 164, loss: 2.301191\n",
            "Epoch 165, loss: 2.299350\n",
            "Epoch 166, loss: 2.301112\n",
            "Epoch 167, loss: 2.301758\n",
            "Epoch 168, loss: 2.300923\n",
            "Epoch 169, loss: 2.301420\n",
            "Epoch 170, loss: 2.301240\n",
            "Epoch 171, loss: 2.299848\n",
            "Epoch 172, loss: 2.300946\n",
            "Epoch 173, loss: 2.301258\n",
            "Epoch 174, loss: 2.301619\n",
            "Epoch 175, loss: 2.300560\n",
            "Epoch 176, loss: 2.302250\n",
            "Epoch 177, loss: 2.300710\n",
            "Epoch 178, loss: 2.301080\n",
            "Epoch 179, loss: 2.300815\n",
            "Epoch 180, loss: 2.300920\n",
            "Epoch 181, loss: 2.300776\n",
            "Epoch 182, loss: 2.300016\n",
            "Epoch 183, loss: 2.300108\n",
            "Epoch 184, loss: 2.302240\n",
            "Epoch 185, loss: 2.300965\n",
            "Epoch 186, loss: 2.300591\n",
            "Epoch 187, loss: 2.301234\n",
            "Epoch 188, loss: 2.300852\n",
            "Epoch 189, loss: 2.301292\n",
            "Epoch 190, loss: 2.298726\n",
            "Epoch 191, loss: 2.300523\n",
            "Epoch 192, loss: 2.301138\n",
            "Epoch 193, loss: 2.299850\n",
            "Epoch 194, loss: 2.302070\n",
            "Epoch 195, loss: 2.300789\n",
            "Epoch 196, loss: 2.299780\n",
            "Epoch 197, loss: 2.300424\n",
            "Epoch 198, loss: 2.300565\n",
            "Epoch 199, loss: 2.299935\n",
            "best validation accuracy achieved: 0.245000\n",
            "rate =  0.01 ,  reg =  1e-06\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lV3rRndcxTA5"
      },
      "source": [
        "# Какой же точности мы добились на тестовых данных?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jzvHdYuuxTA6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5eb81943-b560-47e3-e274-e6e74fcad822"
      },
      "source": [
        "test_pred = best_classifier.predict(test_X)\n",
        "test_accuracy = multiclass_accuracy(test_pred, test_y)\n",
        "print('Linear softmax classifier test set accuracy: %f' % (test_accuracy, ))"
      ],
      "execution_count": 77,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Linear softmax classifier test set accuracy: 0.205000\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}