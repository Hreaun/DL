{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.0"
    },
    "colab": {
      "name": "Copy of Neural Network.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "KVggE9zc9H8t"
      ]
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KVggE9zc9H8t"
      },
      "source": [
        "# Задание 2.1 - Нейронные сети\n",
        "\n",
        "В этом задании вы реализуете и натренируете настоящую нейроную сеть своими руками!\n",
        "\n",
        "В некотором смысле это будет расширением прошлого задания - нам нужно просто составить несколько линейных классификаторов вместе!\n",
        "\n",
        "<img src=\"https://i.redd.it/n9fgba8b0qr01.png\" alt=\"Stack_more_layers\" width=\"400px\"/>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7eFXI5xk9Uuo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2f9df03f-9843-4539-f4da-0de3bf3b60ec"
      },
      "source": [
        "!git clone https://github.com/sim0nsays/dlcourse_ai.git\n",
        "!cp dlcourse_ai/assignments/assignment2/*.py .\n",
        "!./dlcourse_ai/assignments/assignment2/download_data.sh"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'dlcourse_ai'...\n",
            "remote: Enumerating objects: 357, done.\u001b[K\n",
            "remote: Total 357 (delta 0), reused 0 (delta 0), pack-reused 357\u001b[K\n",
            "Receiving objects: 100% (357/357), 740.01 KiB | 11.38 MiB/s, done.\n",
            "Resolving deltas: 100% (165/165), done.\n",
            "--2021-03-08 07:40:45--  http://ufldl.stanford.edu/housenumbers/train_32x32.mat\n",
            "Resolving ufldl.stanford.edu (ufldl.stanford.edu)... 171.64.68.10\n",
            "Connecting to ufldl.stanford.edu (ufldl.stanford.edu)|171.64.68.10|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 182040794 (174M) [text/plain]\n",
            "Saving to: ‘train_32x32.mat’\n",
            "\n",
            "train_32x32.mat     100%[===================>] 173.61M  18.6MB/s    in 13s     \n",
            "\n",
            "2021-03-08 07:40:58 (13.4 MB/s) - ‘train_32x32.mat’ saved [182040794/182040794]\n",
            "\n",
            "--2021-03-08 07:40:58--  http://ufldl.stanford.edu/housenumbers/test_32x32.mat\n",
            "Reusing existing connection to ufldl.stanford.edu:80.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 64275384 (61M) [text/plain]\n",
            "Saving to: ‘test_32x32.mat’\n",
            "\n",
            "test_32x32.mat      100%[===================>]  61.30M  27.6MB/s    in 2.2s    \n",
            "\n",
            "2021-03-08 07:41:00 (27.6 MB/s) - ‘test_32x32.mat’ saved [64275384/64275384]\n",
            "\n",
            "FINISHED --2021-03-08 07:41:00--\n",
            "Total wall clock time: 15s\n",
            "Downloaded: 2 files, 235M in 15s (15.4 MB/s)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zFnojmtU9H80"
      },
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "%matplotlib inline\n",
        "\n",
        "%load_ext autoreload\n",
        "%autoreload 2"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7JOZ1eki9H84"
      },
      "source": [
        "from dataset import load_svhn, random_split_train_val\n",
        "from gradient_check import check_layer_gradient, check_layer_param_gradient, check_model_gradient\n",
        "from layers import FullyConnectedLayer, ReLULayer\n",
        "from model import TwoLayerNet\n",
        "from trainer import Trainer, Dataset\n",
        "from optim import SGD, MomentumSGD\n",
        "from metrics import multiclass_accuracy"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dmjcw1fA9H86"
      },
      "source": [
        "# Загружаем данные\n",
        "\n",
        "И разделяем их на training и validation."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3cPS2aRY9H86"
      },
      "source": [
        "def prepare_for_neural_network(train_X, test_X):\n",
        "    train_flat = train_X.reshape(train_X.shape[0], -1).astype(np.float) / 255.0\n",
        "    test_flat = test_X.reshape(test_X.shape[0], -1).astype(np.float) / 255.0\n",
        "    \n",
        "    # Subtract mean\n",
        "    mean_image = np.mean(train_flat, axis = 0)\n",
        "    train_flat -= mean_image\n",
        "    test_flat -= mean_image\n",
        "    \n",
        "    return train_flat, test_flat\n",
        "    \n",
        "train_X, train_y, test_X, test_y = load_svhn(\"data\", max_train=10000, max_test=1000)    \n",
        "train_X, test_X = prepare_for_neural_network(train_X, test_X)\n",
        "# Split train into train and val\n",
        "train_X, train_y, val_X, val_y = random_split_train_val(train_X, train_y, num_val = 1000)"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rL9D-tnp9H87"
      },
      "source": [
        "# Как всегда, начинаем с кирпичиков\n",
        "\n",
        "Мы будем реализовывать необходимые нам слои по очереди. Каждый слой должен реализовать:\n",
        "- прямой проход (forward pass), который генерирует выход слоя по входу и запоминает необходимые данные\n",
        "- обратный проход (backward pass), который получает градиент по выходу слоя и вычисляет градиент по входу и по параметрам\n",
        "\n",
        "Начнем с ReLU, у которого параметров нет."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eaInYHfOC_14",
        "outputId": "414ccfb7-f42f-4ac6-ecb9-37246a494d30"
      },
      "source": [
        "a = np.array([[1,-2,3],\n",
        "              [-1, 2, 0.1]\n",
        "              ])\n",
        "a = np.where(a > 0, a, 0)\n",
        "print(a)\n",
        "print(np.int64(a > 0))\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[1.  0.  3. ]\n",
            " [0.  2.  0.1]]\n",
            "[[1 0 1]\n",
            " [0 1 1]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": false,
        "id": "csIMQtuu9H88",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3be10fc1-3dba-4bc2-eb77-f311a6b6b0cb"
      },
      "source": [
        "# TODO: Implement ReLULayer layer in layers.py\n",
        "# Note: you'll need to copy implementation of the gradient_check function from the previous assignment\n",
        "\n",
        "X = np.array([[1,-2,3],\n",
        "              [-1, 2, 0.1]\n",
        "              ])\n",
        "\n",
        "assert check_layer_gradient(ReLULayer(), X)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Gradient check passed!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z6rVmZY39H89"
      },
      "source": [
        "А теперь реализуем полносвязный слой (fully connected layer), у которого будет два массива параметров: W (weights) и B (bias).\n",
        "\n",
        "Все параметры наши слои будут использовать для параметров специальный класс `Param`, в котором будут храниться значения параметров и градиенты этих параметров, вычисляемые во время обратного прохода.\n",
        "\n",
        "Это даст возможность аккумулировать (суммировать) градиенты из разных частей функции потерь, например, из cross-entropy loss и regularization loss."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YEAUk8bM9H8-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8a7d0cb1-87c4-43c6-e6df-5a88d4ce8aaf"
      },
      "source": [
        "# TODO: Implement FullyConnected layer forward and backward methods\n",
        "assert check_layer_gradient(FullyConnectedLayer(3, 4), X)\n",
        "# TODO: Implement storing gradients for W and B\n",
        "assert check_layer_param_gradient(FullyConnectedLayer(3, 4), X, 'W')\n",
        "assert check_layer_param_gradient(FullyConnectedLayer(3, 4), X, 'B')"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Gradient check passed!\n",
            "Gradient check passed!\n",
            "Gradient check passed!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vmDpvVLS9H9A"
      },
      "source": [
        "## Создаем нейронную сеть\n",
        "\n",
        "Теперь мы реализуем простейшую нейронную сеть с двумя полносвязным слоями и нелинейностью ReLU. Реализуйте функцию `compute_loss_and_gradients`, она должна запустить прямой и обратный проход через оба слоя для вычисления градиентов.\n",
        "\n",
        "Не забудьте реализовать очистку градиентов в начале функции."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0Ow3OENj9H9A",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "53b25cff-b5d3-418d-f009-cb597ef94808"
      },
      "source": [
        "# TODO: In model.py, implement compute_loss_and_gradients function\n",
        "model = TwoLayerNet(n_input = train_X.shape[1], n_output = 10, hidden_layer_size = 3, reg = 0)\n",
        "loss = model.compute_loss_and_gradients(train_X[:2], train_y[:2])\n",
        "\n",
        "# TODO Now implement backward pass and aggregate all of the params\n",
        "check_model_gradient(model, train_X[:2], train_y[:2])"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Checking gradient for hidden_W\n",
            "Gradient check passed!\n",
            "Checking gradient for fully_connected_W\n",
            "Gradient check passed!\n",
            "Checking gradient for hidden_B\n",
            "Gradient check passed!\n",
            "Checking gradient for fully_connected_B\n",
            "Gradient check passed!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FQP-Gskw9H9B"
      },
      "source": [
        "Теперь добавьте к модели регуляризацию - она должна прибавляться к loss и делать свой вклад в градиенты."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ndb2V4u49H9C",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ec043485-1c8d-4024-c711-bd6c0101d98c"
      },
      "source": [
        "# TODO Now implement l2 regularization in the forward and backward pass\n",
        "model_with_reg = TwoLayerNet(n_input = train_X.shape[1], n_output = 10, hidden_layer_size = 3, reg = 1e1)\n",
        "loss_with_reg = model_with_reg.compute_loss_and_gradients(train_X[:2], train_y[:2])\n",
        "assert loss_with_reg > loss and not np.isclose(loss_with_reg, loss), \\\n",
        "    \"Loss with regularization (%2.4f) should be higher than without it (%2.4f)!\" % (loss, loss_with_reg)\n",
        "\n",
        "check_model_gradient(model_with_reg, train_X[:2], train_y[:2])"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Checking gradient for hidden_W\n",
            "Gradient check passed!\n",
            "Checking gradient for fully_connected_W\n",
            "Gradient check passed!\n",
            "Checking gradient for hidden_B\n",
            "Gradient check passed!\n",
            "Checking gradient for fully_connected_B\n",
            "Gradient check passed!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NEtooe489H9D"
      },
      "source": [
        "Также реализуем функцию предсказания (вычисления значения) модели на новых данных.\n",
        "\n",
        "Какое значение точности мы ожидаем увидеть до начала тренировки?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x5TrUq0Z9H9H",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "638f7a1c-3cdf-4d07-d59b-80095bdb9a15"
      },
      "source": [
        "# Finally, implement predict function!\n",
        "\n",
        "# TODO: Implement predict function\n",
        "# What would be the value we expect?\n",
        "print(model_with_reg.predict(train_X[:30]).shape)\n",
        "multiclass_accuracy(model_with_reg.predict(train_X[:30]), train_y[:30]) "
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(30,)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.03333333333333333"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F-c0Ph1X9H9I"
      },
      "source": [
        "# Допишем код для процесса тренировки\n",
        "\n",
        "Если все реализовано корректно, значение функции ошибки должно уменьшаться с каждой эпохой, пусть и медленно. Не беспокойтесь пока про validation accuracy."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8-OMZirY9H9J",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 460
        },
        "outputId": "2f512f29-5497-4715-9600-ff931969abef"
      },
      "source": [
        "model = TwoLayerNet(n_input = train_X.shape[1], n_output = 10, hidden_layer_size = 10, reg = 1e1)\n",
        "dataset = Dataset(train_X, train_y, val_X, val_y)\n",
        "trainer = Trainer(model, dataset, SGD(), learning_rate = 1e-2)\n",
        "\n",
        "# TODO Implement missing pieces in Trainer.fit function\n",
        "# You should expect loss to go down every epoch, even if it's slow\n",
        "loss_history, train_history, val_history = trainer.fit()"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loss: 2.301901, Train accuracy: 0.196667, val accuracy: 0.206000\n",
            "Loss: 2.301250, Train accuracy: 0.196667, val accuracy: 0.206000\n",
            "Loss: 2.301903, Train accuracy: 0.196667, val accuracy: 0.206000\n",
            "Loss: 2.303973, Train accuracy: 0.148222, val accuracy: 0.140000\n",
            "Loss: 2.301728, Train accuracy: 0.196667, val accuracy: 0.206000\n",
            "Loss: 2.302833, Train accuracy: 0.196667, val accuracy: 0.206000\n",
            "Loss: 2.302302, Train accuracy: 0.196667, val accuracy: 0.206000\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-15-1990d967a3cc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# TODO Implement missing pieces in Trainer.fit function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;31m# You should expect loss to go down every epoch, even if it's slow\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mloss_history\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_history\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_history\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/content/trainer.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    103\u001b[0m                 \u001b[0mbatch_x\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_X\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbatch_indices\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m                 \u001b[0mbatch_y\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_y\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbatch_indices\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 105\u001b[0;31m                 \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompute_loss_and_gradients\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_y\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    106\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0mparam_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparam\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/model.py\u001b[0m in \u001b[0;36mcompute_loss_and_gradients\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m     42\u001b[0m         \u001b[0;31m# by running forward and backward passes through the model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m         \u001b[0mout\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhidden_layer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m         \u001b[0mout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu_layer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m         \u001b[0mout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfully_connected_layer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/layers.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    147\u001b[0m         \u001b[0;31m# Your final implementation shouldn't have any loops\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    148\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 149\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mX\u001b[0m \u001b[0;34m@\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mW\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mB\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    150\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    151\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0md_out\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tod81wWl9H9K",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 282
        },
        "outputId": "d9816325-1376-4b7b-9503-8603a8955aa0"
      },
      "source": [
        "plt.plot(train_history)\n",
        "plt.plot(val_history)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[<matplotlib.lines.Line2D at 0x7feef45d4b90>]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 43
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gwbswJXR9H9L"
      },
      "source": [
        "# Улучшаем процесс тренировки\n",
        "\n",
        "Мы реализуем несколько ключевых оптимизаций, необходимых для тренировки современных нейросетей."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h11XikvT9H9M"
      },
      "source": [
        "## Уменьшение скорости обучения (learning rate decay)\n",
        "\n",
        "Одна из необходимых оптимизаций во время тренировки нейронных сетей - постепенное уменьшение скорости обучения по мере тренировки.\n",
        "\n",
        "Один из стандартных методов - уменьшение скорости обучения (learning rate) каждые N эпох на коэффициент d (часто называемый decay). Значения N и d, как всегда, являются гиперпараметрами и должны подбираться на основе эффективности на проверочных данных (validation data). \n",
        "\n",
        "В нашем случае N будет равным 1."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "65-JLZwO9H9M",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 545
        },
        "outputId": "14bcdf9f-1c02-4287-c45f-da9e105382f9"
      },
      "source": [
        "# TODO Implement learning rate decay inside Trainer.fit method\n",
        "# Decay should happen once per epoch\n",
        "\n",
        "model = TwoLayerNet(n_input = train_X.shape[1], n_output = 10, hidden_layer_size = 100, reg = 1e-1)\n",
        "dataset = Dataset(train_X, train_y, val_X, val_y)\n",
        "trainer = Trainer(model, dataset, SGD(), learning_rate_decay=0.99)\n",
        "\n",
        "initial_learning_rate = trainer.learning_rate\n",
        "loss_history, train_history, val_history = trainer.fit()\n",
        "\n",
        "assert trainer.learning_rate < initial_learning_rate, \"Learning rate should've been reduced\"\n",
        "assert trainer.learning_rate > 0.5*initial_learning_rate, \"Learning rate shouldn'tve been reduced that much!\""
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loss: 2.297637, Train accuracy: 0.196667, val accuracy: 0.206000\n",
            "Loss: 2.276159, Train accuracy: 0.196667, val accuracy: 0.206000\n",
            "Loss: 2.269356, Train accuracy: 0.196667, val accuracy: 0.206000\n",
            "Loss: 2.263773, Train accuracy: 0.196667, val accuracy: 0.206000\n",
            "Loss: 2.222528, Train accuracy: 0.196667, val accuracy: 0.206000\n",
            "Loss: 2.250457, Train accuracy: 0.196667, val accuracy: 0.206000\n",
            "Loss: 2.237240, Train accuracy: 0.196667, val accuracy: 0.206000\n",
            "Loss: 2.363020, Train accuracy: 0.196667, val accuracy: 0.206000\n",
            "Loss: 2.232430, Train accuracy: 0.196667, val accuracy: 0.206000\n",
            "Loss: 2.345336, Train accuracy: 0.196667, val accuracy: 0.206000\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-89-2cd0d7e60500>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0minitial_learning_rate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlearning_rate\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0mloss_history\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_history\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_history\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;32massert\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlearning_rate\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0minitial_learning_rate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"Learning rate should've been reduced\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/trainer.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    103\u001b[0m                 \u001b[0mbatch_x\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_X\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbatch_indices\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m                 \u001b[0mbatch_y\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_y\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbatch_indices\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 105\u001b[0;31m                 \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompute_loss_and_gradients\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_y\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    106\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0mparam_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparam\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/model.py\u001b[0m in \u001b[0;36mcompute_loss_and_gradients\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m     42\u001b[0m         \u001b[0;31m# by running forward and backward passes through the model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m         \u001b[0mout\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhidden_layer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m         \u001b[0mout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu_layer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m         \u001b[0mout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfully_connected_layer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/layers.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    147\u001b[0m         \u001b[0;31m# Your final implementation shouldn't have any loops\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    148\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 149\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mX\u001b[0m \u001b[0;34m@\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mW\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mB\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    150\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    151\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0md_out\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fk8CXOke9H9N"
      },
      "source": [
        "# Накопление импульса (Momentum SGD)\n",
        "\n",
        "Другой большой класс оптимизаций - использование более эффективных методов градиентного спуска. Мы реализуем один из них - накопление импульса (Momentum SGD).\n",
        "\n",
        "Этот метод хранит скорость движения, использует градиент для ее изменения на каждом шаге, и изменяет веса пропорционально значению скорости.\n",
        "(Физическая аналогия: Вместо скорости градиенты теперь будут задавать ускорение, но будет присутствовать сила трения.)\n",
        "\n",
        "```\n",
        "velocity = momentum * velocity - learning_rate * gradient \n",
        "w = w + velocity\n",
        "```\n",
        "\n",
        "`momentum` здесь коэффициент затухания, который тоже является гиперпараметром (к счастью, для него часто есть хорошее значение по умолчанию, типичный диапазон -- 0.8-0.99).\n",
        "\n",
        "Несколько полезных ссылок, где метод разбирается более подробно:  \n",
        "http://cs231n.github.io/neural-networks-3/#sgd  \n",
        "https://distill.pub/2017/momentum/"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h9R-QLAH9H9P",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 409
        },
        "outputId": "aafeebcf-5578-4b8d-bd04-d14581aa2120"
      },
      "source": [
        "# TODO: Implement MomentumSGD.update function in optim.py\n",
        "\n",
        "model = TwoLayerNet(n_input = train_X.shape[1], n_output = 10, hidden_layer_size = 100, reg = 1e-1)\n",
        "dataset = Dataset(train_X, train_y, val_X, val_y)\n",
        "trainer = Trainer(model, dataset, MomentumSGD(), learning_rate=1e-4, learning_rate_decay=0.99)\n",
        "\n",
        "# You should see even better results than before!\n",
        "loss_history, train_history, val_history = trainer.fit()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loss: 2.315165, Train accuracy: 0.196667, val accuracy: 0.206000\n",
            "Loss: 2.319746, Train accuracy: 0.196667, val accuracy: 0.206000\n",
            "Loss: 2.286963, Train accuracy: 0.196667, val accuracy: 0.206000\n",
            "Loss: 2.303713, Train accuracy: 0.196667, val accuracy: 0.206000\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-87-c381cf05523e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;31m# You should see even better results than before!\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mloss_history\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_history\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_history\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/content/trainer.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    103\u001b[0m                 \u001b[0mbatch_x\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_X\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbatch_indices\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m                 \u001b[0mbatch_y\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_y\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbatch_indices\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 105\u001b[0;31m                 \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompute_loss_and_gradients\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_y\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    106\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0mparam_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparam\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/model.py\u001b[0m in \u001b[0;36mcompute_loss_and_gradients\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m     41\u001b[0m         \u001b[0;31m# Compute loss and fill param gradients\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m         \u001b[0;31m# by running forward and backward passes through the model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 43\u001b[0;31m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhidden_layer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     44\u001b[0m         \u001b[0mout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu_layer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m         \u001b[0mout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfully_connected_layer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/layers.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    147\u001b[0m         \u001b[0;31m# Your final implementation shouldn't have any loops\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    148\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 149\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mX\u001b[0m \u001b[0;34m@\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mW\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mB\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    150\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    151\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0md_out\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p-aaib4O9H9R"
      },
      "source": [
        "# Ну что, давайте уже тренировать сеть!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5NgipHHq9H9S"
      },
      "source": [
        "## Последний тест - переобучимся (overfit) на маленьком наборе данных\n",
        "\n",
        "Хороший способ проверить, все ли реализовано корректно - переобучить сеть на маленьком наборе данных.  \n",
        "Наша модель обладает достаточной мощностью, чтобы приблизить маленький набор данных идеально, поэтому мы ожидаем, что на нем мы быстро дойдем до 100% точности на тренировочном наборе. \n",
        "\n",
        "Если этого не происходит, то где-то была допущена ошибка!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0bGi3SvE9H9S",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "52943c80-6c0c-4abd-d9e8-ea19883b6335"
      },
      "source": [
        "data_size = 15\n",
        "model = TwoLayerNet(n_input = train_X.shape[1], n_output = 10, hidden_layer_size = 100, reg = 1e-1)\n",
        "dataset = Dataset(train_X[:data_size], train_y[:data_size], val_X[:data_size], val_y[:data_size])\n",
        "trainer = Trainer(model, dataset, SGD(), learning_rate=1e-1, num_epochs=150, batch_size=5)\n",
        "\n",
        "# You should expect this to reach 1.0 training accuracy \n",
        "loss_history, train_history, val_history = trainer.fit()"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loss: 2.347405, Train accuracy: 0.200000, val accuracy: 0.133333\n",
            "Loss: 2.320409, Train accuracy: 0.200000, val accuracy: 0.133333\n",
            "Loss: 2.307884, Train accuracy: 0.200000, val accuracy: 0.133333\n",
            "Loss: 2.301262, Train accuracy: 0.266667, val accuracy: 0.000000\n",
            "Loss: 2.282862, Train accuracy: 0.200000, val accuracy: 0.133333\n",
            "Loss: 2.258532, Train accuracy: 0.200000, val accuracy: 0.133333\n",
            "Loss: 2.349622, Train accuracy: 0.200000, val accuracy: 0.133333\n",
            "Loss: 2.259465, Train accuracy: 0.200000, val accuracy: 0.133333\n",
            "Loss: 2.280375, Train accuracy: 0.400000, val accuracy: 0.000000\n",
            "Loss: 2.318970, Train accuracy: 0.400000, val accuracy: 0.000000\n",
            "Loss: 1.939739, Train accuracy: 0.400000, val accuracy: 0.000000\n",
            "Loss: 1.905268, Train accuracy: 0.400000, val accuracy: 0.000000\n",
            "Loss: 1.922693, Train accuracy: 0.400000, val accuracy: 0.000000\n",
            "Loss: 1.883892, Train accuracy: 0.333333, val accuracy: 0.000000\n",
            "Loss: 2.138371, Train accuracy: 0.333333, val accuracy: 0.000000\n",
            "Loss: 2.072113, Train accuracy: 0.333333, val accuracy: 0.000000\n",
            "Loss: 2.285011, Train accuracy: 0.400000, val accuracy: 0.000000\n",
            "Loss: 1.993635, Train accuracy: 0.400000, val accuracy: 0.000000\n",
            "Loss: 2.358192, Train accuracy: 0.400000, val accuracy: 0.000000\n",
            "Loss: 1.914681, Train accuracy: 0.400000, val accuracy: 0.000000\n",
            "Loss: 1.919923, Train accuracy: 0.400000, val accuracy: 0.000000\n",
            "Loss: 2.318406, Train accuracy: 0.400000, val accuracy: 0.066667\n",
            "Loss: 1.400390, Train accuracy: 0.400000, val accuracy: 0.066667\n",
            "Loss: 1.792370, Train accuracy: 0.400000, val accuracy: 0.000000\n",
            "Loss: 1.530701, Train accuracy: 0.400000, val accuracy: 0.000000\n",
            "Loss: 1.654157, Train accuracy: 0.466667, val accuracy: 0.000000\n",
            "Loss: 1.726573, Train accuracy: 0.466667, val accuracy: 0.000000\n",
            "Loss: 1.620950, Train accuracy: 0.466667, val accuracy: 0.000000\n",
            "Loss: 1.525007, Train accuracy: 0.466667, val accuracy: 0.066667\n",
            "Loss: 1.676151, Train accuracy: 0.533333, val accuracy: 0.066667\n",
            "Loss: 1.816995, Train accuracy: 0.466667, val accuracy: 0.000000\n",
            "Loss: 1.464910, Train accuracy: 0.466667, val accuracy: 0.000000\n",
            "Loss: 1.667683, Train accuracy: 0.600000, val accuracy: 0.000000\n",
            "Loss: 1.910560, Train accuracy: 0.533333, val accuracy: 0.000000\n",
            "Loss: 1.477351, Train accuracy: 0.533333, val accuracy: 0.066667\n",
            "Loss: 1.792694, Train accuracy: 0.600000, val accuracy: 0.066667\n",
            "Loss: 1.006097, Train accuracy: 0.600000, val accuracy: 0.066667\n",
            "Loss: 1.275989, Train accuracy: 0.666667, val accuracy: 0.066667\n",
            "Loss: 1.413102, Train accuracy: 0.600000, val accuracy: 0.066667\n",
            "Loss: 1.666649, Train accuracy: 0.666667, val accuracy: 0.000000\n",
            "Loss: 1.346804, Train accuracy: 0.666667, val accuracy: 0.066667\n",
            "Loss: 1.375653, Train accuracy: 0.666667, val accuracy: 0.066667\n",
            "Loss: 1.372000, Train accuracy: 0.666667, val accuracy: 0.066667\n",
            "Loss: 1.915935, Train accuracy: 0.666667, val accuracy: 0.066667\n",
            "Loss: 1.547068, Train accuracy: 0.666667, val accuracy: 0.066667\n",
            "Loss: 1.605139, Train accuracy: 0.733333, val accuracy: 0.066667\n",
            "Loss: 1.278481, Train accuracy: 0.800000, val accuracy: 0.066667\n",
            "Loss: 1.609511, Train accuracy: 0.733333, val accuracy: 0.000000\n",
            "Loss: 1.422427, Train accuracy: 0.733333, val accuracy: 0.066667\n",
            "Loss: 1.882240, Train accuracy: 0.733333, val accuracy: 0.066667\n",
            "Loss: 1.943410, Train accuracy: 0.733333, val accuracy: 0.066667\n",
            "Loss: 1.284176, Train accuracy: 0.733333, val accuracy: 0.066667\n",
            "Loss: 1.299853, Train accuracy: 0.733333, val accuracy: 0.133333\n",
            "Loss: 1.450447, Train accuracy: 0.733333, val accuracy: 0.133333\n",
            "Loss: 1.735915, Train accuracy: 0.733333, val accuracy: 0.133333\n",
            "Loss: 1.970962, Train accuracy: 0.733333, val accuracy: 0.066667\n",
            "Loss: 1.658359, Train accuracy: 0.733333, val accuracy: 0.066667\n",
            "Loss: 1.444063, Train accuracy: 0.733333, val accuracy: 0.066667\n",
            "Loss: 0.983488, Train accuracy: 0.733333, val accuracy: 0.066667\n",
            "Loss: 1.213739, Train accuracy: 0.733333, val accuracy: 0.066667\n",
            "Loss: 1.319049, Train accuracy: 0.733333, val accuracy: 0.066667\n",
            "Loss: 1.805887, Train accuracy: 0.733333, val accuracy: 0.066667\n",
            "Loss: 1.267031, Train accuracy: 0.733333, val accuracy: 0.066667\n",
            "Loss: 1.759995, Train accuracy: 0.733333, val accuracy: 0.066667\n",
            "Loss: 1.264091, Train accuracy: 0.733333, val accuracy: 0.066667\n",
            "Loss: 1.631119, Train accuracy: 0.733333, val accuracy: 0.066667\n",
            "Loss: 0.955109, Train accuracy: 0.733333, val accuracy: 0.066667\n",
            "Loss: 1.259718, Train accuracy: 0.733333, val accuracy: 0.066667\n",
            "Loss: 1.947491, Train accuracy: 0.733333, val accuracy: 0.066667\n",
            "Loss: 1.008937, Train accuracy: 0.733333, val accuracy: 0.066667\n",
            "Loss: 1.359208, Train accuracy: 0.733333, val accuracy: 0.066667\n",
            "Loss: 1.319908, Train accuracy: 0.733333, val accuracy: 0.066667\n",
            "Loss: 1.248254, Train accuracy: 0.800000, val accuracy: 0.066667\n",
            "Loss: 1.258861, Train accuracy: 0.800000, val accuracy: 0.066667\n",
            "Loss: 1.620941, Train accuracy: 0.800000, val accuracy: 0.066667\n",
            "Loss: 1.210925, Train accuracy: 0.866667, val accuracy: 0.066667\n",
            "Loss: 0.927852, Train accuracy: 0.866667, val accuracy: 0.066667\n",
            "Loss: 1.515485, Train accuracy: 0.866667, val accuracy: 0.066667\n",
            "Loss: 1.457845, Train accuracy: 0.866667, val accuracy: 0.066667\n",
            "Loss: 1.586391, Train accuracy: 0.933333, val accuracy: 0.066667\n",
            "Loss: 1.033594, Train accuracy: 0.933333, val accuracy: 0.066667\n",
            "Loss: 1.248737, Train accuracy: 0.866667, val accuracy: 0.066667\n",
            "Loss: 1.621755, Train accuracy: 0.933333, val accuracy: 0.066667\n",
            "Loss: 1.346586, Train accuracy: 0.933333, val accuracy: 0.066667\n",
            "Loss: 1.943837, Train accuracy: 0.933333, val accuracy: 0.066667\n",
            "Loss: 1.241888, Train accuracy: 0.933333, val accuracy: 0.066667\n",
            "Loss: 1.301453, Train accuracy: 0.933333, val accuracy: 0.066667\n",
            "Loss: 1.596102, Train accuracy: 0.933333, val accuracy: 0.066667\n",
            "Loss: 1.655652, Train accuracy: 0.933333, val accuracy: 0.066667\n",
            "Loss: 1.254548, Train accuracy: 0.933333, val accuracy: 0.066667\n",
            "Loss: 1.480622, Train accuracy: 0.933333, val accuracy: 0.066667\n",
            "Loss: 1.596973, Train accuracy: 0.933333, val accuracy: 0.066667\n",
            "Loss: 1.187435, Train accuracy: 0.933333, val accuracy: 0.066667\n",
            "Loss: 1.276691, Train accuracy: 0.933333, val accuracy: 0.066667\n",
            "Loss: 1.463777, Train accuracy: 0.933333, val accuracy: 0.066667\n",
            "Loss: 1.593712, Train accuracy: 0.933333, val accuracy: 0.066667\n",
            "Loss: 1.084228, Train accuracy: 0.933333, val accuracy: 0.066667\n",
            "Loss: 1.090772, Train accuracy: 0.933333, val accuracy: 0.066667\n",
            "Loss: 1.357789, Train accuracy: 0.933333, val accuracy: 0.066667\n",
            "Loss: 1.234136, Train accuracy: 0.933333, val accuracy: 0.066667\n",
            "Loss: 1.349143, Train accuracy: 0.933333, val accuracy: 0.066667\n",
            "Loss: 1.250146, Train accuracy: 0.933333, val accuracy: 0.066667\n",
            "Loss: 1.009300, Train accuracy: 0.933333, val accuracy: 0.000000\n",
            "Loss: 1.187317, Train accuracy: 0.933333, val accuracy: 0.066667\n",
            "Loss: 1.164523, Train accuracy: 0.933333, val accuracy: 0.066667\n",
            "Loss: 1.440744, Train accuracy: 0.933333, val accuracy: 0.000000\n",
            "Loss: 1.117520, Train accuracy: 0.933333, val accuracy: 0.000000\n",
            "Loss: 1.119609, Train accuracy: 0.933333, val accuracy: 0.000000\n",
            "Loss: 1.542602, Train accuracy: 1.000000, val accuracy: 0.000000\n",
            "Loss: 1.301749, Train accuracy: 1.000000, val accuracy: 0.000000\n",
            "Loss: 1.020703, Train accuracy: 1.000000, val accuracy: 0.000000\n",
            "Loss: 1.215974, Train accuracy: 1.000000, val accuracy: 0.000000\n",
            "Loss: 1.518074, Train accuracy: 1.000000, val accuracy: 0.000000\n",
            "Loss: 1.263460, Train accuracy: 0.933333, val accuracy: 0.000000\n",
            "Loss: 1.134013, Train accuracy: 0.933333, val accuracy: 0.000000\n",
            "Loss: 1.268742, Train accuracy: 1.000000, val accuracy: 0.000000\n",
            "Loss: 1.477929, Train accuracy: 1.000000, val accuracy: 0.000000\n",
            "Loss: 1.364618, Train accuracy: 1.000000, val accuracy: 0.000000\n",
            "Loss: 1.148355, Train accuracy: 1.000000, val accuracy: 0.000000\n",
            "Loss: 1.236593, Train accuracy: 1.000000, val accuracy: 0.000000\n",
            "Loss: 1.139823, Train accuracy: 1.000000, val accuracy: 0.000000\n",
            "Loss: 1.415895, Train accuracy: 1.000000, val accuracy: 0.000000\n",
            "Loss: 1.424922, Train accuracy: 1.000000, val accuracy: 0.000000\n",
            "Loss: 1.463755, Train accuracy: 1.000000, val accuracy: 0.000000\n",
            "Loss: 1.587879, Train accuracy: 1.000000, val accuracy: 0.000000\n",
            "Loss: 1.070720, Train accuracy: 1.000000, val accuracy: 0.000000\n",
            "Loss: 1.334238, Train accuracy: 1.000000, val accuracy: 0.000000\n",
            "Loss: 1.488354, Train accuracy: 1.000000, val accuracy: 0.000000\n",
            "Loss: 1.258736, Train accuracy: 1.000000, val accuracy: 0.000000\n",
            "Loss: 1.174324, Train accuracy: 1.000000, val accuracy: 0.000000\n",
            "Loss: 1.335986, Train accuracy: 1.000000, val accuracy: 0.000000\n",
            "Loss: 1.323730, Train accuracy: 1.000000, val accuracy: 0.000000\n",
            "Loss: 1.260214, Train accuracy: 1.000000, val accuracy: 0.000000\n",
            "Loss: 1.234124, Train accuracy: 1.000000, val accuracy: 0.000000\n",
            "Loss: 1.140789, Train accuracy: 1.000000, val accuracy: 0.000000\n",
            "Loss: 1.206724, Train accuracy: 1.000000, val accuracy: 0.000000\n",
            "Loss: 1.244107, Train accuracy: 1.000000, val accuracy: 0.000000\n",
            "Loss: 1.259171, Train accuracy: 1.000000, val accuracy: 0.000000\n",
            "Loss: 1.340617, Train accuracy: 1.000000, val accuracy: 0.000000\n",
            "Loss: 1.253713, Train accuracy: 1.000000, val accuracy: 0.000000\n",
            "Loss: 1.223716, Train accuracy: 1.000000, val accuracy: 0.000000\n",
            "Loss: 1.370739, Train accuracy: 1.000000, val accuracy: 0.000000\n",
            "Loss: 1.431264, Train accuracy: 1.000000, val accuracy: 0.000000\n",
            "Loss: 1.220904, Train accuracy: 1.000000, val accuracy: 0.000000\n",
            "Loss: 1.380599, Train accuracy: 1.000000, val accuracy: 0.000000\n",
            "Loss: 1.107054, Train accuracy: 1.000000, val accuracy: 0.000000\n",
            "Loss: 1.314145, Train accuracy: 1.000000, val accuracy: 0.000000\n",
            "Loss: 1.272770, Train accuracy: 1.000000, val accuracy: 0.000000\n",
            "Loss: 1.156313, Train accuracy: 1.000000, val accuracy: 0.000000\n",
            "Loss: 1.304988, Train accuracy: 1.000000, val accuracy: 0.000000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EWntkB_f9H9T"
      },
      "source": [
        "Теперь найдем гипепараметры, для которых этот процесс сходится быстрее.\n",
        "Если все реализовано корректно, то существуют параметры, при которых процесс сходится в **20** эпох или еще быстрее.\n",
        "Найдите их!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fPtL_ze99H9U",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "aeab70a4-1a6b-440e-da07-557699241fd7"
      },
      "source": [
        "# Now, tweak some hyper parameters and make it train to 1.0 accuracy in 20 epochs or less\n",
        "\n",
        "model = TwoLayerNet(n_input = train_X.shape[1], n_output = 10, hidden_layer_size = 980, reg = (1e-1)/3)\n",
        "dataset = Dataset(train_X[:data_size], train_y[:data_size], val_X[:data_size], val_y[:data_size])\n",
        "# TODO: Change any hyperparamers or optimizators to reach training accuracy in 20 epochs\n",
        "trainer = Trainer(model, dataset, SGD(), learning_rate=(1e-1)*3, num_epochs=20, batch_size=5)\n",
        "\n",
        "loss_history, train_history, val_history = trainer.fit()"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loss: 2.382927, Train accuracy: 0.266667, val accuracy: 0.000000\n",
            "Loss: 2.419136, Train accuracy: 0.266667, val accuracy: 0.000000\n",
            "Loss: 2.352129, Train accuracy: 0.400000, val accuracy: 0.000000\n",
            "Loss: 2.552973, Train accuracy: 0.400000, val accuracy: 0.000000\n",
            "Loss: 1.893993, Train accuracy: 0.333333, val accuracy: 0.133333\n",
            "Loss: 2.043616, Train accuracy: 0.400000, val accuracy: 0.133333\n",
            "Loss: 2.350626, Train accuracy: 0.466667, val accuracy: 0.000000\n",
            "Loss: 2.189250, Train accuracy: 0.400000, val accuracy: 0.133333\n",
            "Loss: 1.614401, Train accuracy: 0.533333, val accuracy: 0.000000\n",
            "Loss: 1.783032, Train accuracy: 0.600000, val accuracy: 0.000000\n",
            "Loss: 1.600521, Train accuracy: 0.800000, val accuracy: 0.000000\n",
            "Loss: 1.615659, Train accuracy: 0.733333, val accuracy: 0.000000\n",
            "Loss: 1.927517, Train accuracy: 0.733333, val accuracy: 0.066667\n",
            "Loss: 0.922200, Train accuracy: 0.933333, val accuracy: 0.066667\n",
            "Loss: 1.188330, Train accuracy: 0.933333, val accuracy: 0.000000\n",
            "Loss: 0.858235, Train accuracy: 0.866667, val accuracy: 0.000000\n",
            "Loss: 1.243379, Train accuracy: 0.933333, val accuracy: 0.133333\n",
            "Loss: 0.960333, Train accuracy: 0.933333, val accuracy: 0.000000\n",
            "Loss: 0.550924, Train accuracy: 1.000000, val accuracy: 0.000000\n",
            "Loss: 0.884232, Train accuracy: 1.000000, val accuracy: 0.000000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zz3DaRNL9H9V"
      },
      "source": [
        "# Итак, основное мероприятие!\n",
        "\n",
        "Натренируйте лучшую нейросеть! Можно добавлять и изменять параметры, менять количество нейронов в слоях сети и как угодно экспериментировать. \n",
        "\n",
        "Добейтесь точности лучше **60%** на validation set."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "mipN_Vog9H9W",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d9bf05cd-8219-48d8-be29-0d8360df90d3"
      },
      "source": [
        "# Let's train the best one-hidden-layer network we can\n",
        "\n",
        "learning_rates = [1e-2]\n",
        "reg_strengths = [1e-3]\n",
        "learning_rate_decays = [0.999]\n",
        "hidden_layer_sizes = [256]\n",
        "num_epochs = 120\n",
        "batch_size = 64\n",
        "\n",
        "best_classifier = None\n",
        "best_val_accuracy = 0\n",
        "best_reg = 0\n",
        "best_learning_rate = 0\n",
        "best_decay = 0\n",
        "best_hl_size = 0\n",
        "\n",
        "loss_history = []\n",
        "train_history = []\n",
        "val_history = []\n",
        "\n",
        "\n",
        "# TODO find the best hyperparameters to train the network\n",
        "# Don't hesitate to add new values to the arrays above, perform experiments, use any tricks you want\n",
        "# You should expect to get to at least 40% of valudation accuracy\n",
        "# Save loss/train/history of the best classifier to the variables above\n",
        "for learning_rate in learning_rates:\n",
        "    for reg in reg_strengths:\n",
        "        for decay in learning_rate_decays:\n",
        "            for hl_size in hidden_layer_sizes:\n",
        "                model = TwoLayerNet(n_input = train_X.shape[1], n_output = 10, hidden_layer_size=hl_size, reg=reg)\n",
        "                dataset = Dataset(train_X, train_y, val_X, val_y)\n",
        "                trainer = Trainer(model, dataset, MomentumSGD(), learning_rate=learning_rate, num_epochs=num_epochs, batch_size=batch_size, learning_rate_decay=decay)\n",
        "                loss_history_cur, train_history_cur, val_history_cur = trainer.fit()\n",
        "                if (val_history_cur[-1] > best_val_accuracy):\n",
        "                    loss_history = loss_history_cur\n",
        "                    train_history = train_history_cur\n",
        "                    val_history = val_history_cur\n",
        "                    best_reg = reg\n",
        "                    best_learning_rate = learning_rate\n",
        "                    best_decay = decay\n",
        "                    best_hl_size = hl_size\n",
        "                    best_val_accuracy = val_history_cur[-1]\n",
        "                    best_classifier = model\n",
        "\n",
        "\n",
        "\n",
        "print('reg={reg}, learning_rate={learning_rate}, decay={decay}, hl_size={hl_size}'\n",
        ".format(reg=best_reg, learning_rate=best_learning_rate, decay=best_decay, hl_size=best_hl_size))\n",
        "print('best validation accuracy achieved: %f' % best_val_accuracy)"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loss: 2.164170, Train accuracy: 0.196667, val accuracy: 0.206000\n",
            "Loss: 2.204412, Train accuracy: 0.196667, val accuracy: 0.206000\n",
            "Loss: 2.109804, Train accuracy: 0.198222, val accuracy: 0.207000\n",
            "Loss: 1.808008, Train accuracy: 0.260667, val accuracy: 0.261000\n",
            "Loss: 1.851381, Train accuracy: 0.350778, val accuracy: 0.353000\n",
            "Loss: 1.708688, Train accuracy: 0.446778, val accuracy: 0.432000\n",
            "Loss: 1.588668, Train accuracy: 0.508222, val accuracy: 0.502000\n",
            "Loss: 1.614781, Train accuracy: 0.574778, val accuracy: 0.575000\n",
            "Loss: 1.534629, Train accuracy: 0.613444, val accuracy: 0.603000\n",
            "Loss: 1.091448, Train accuracy: 0.639889, val accuracy: 0.636000\n",
            "Loss: 1.355594, Train accuracy: 0.668333, val accuracy: 0.658000\n",
            "Loss: 1.162297, Train accuracy: 0.691444, val accuracy: 0.673000\n",
            "Loss: 1.216176, Train accuracy: 0.715889, val accuracy: 0.694000\n",
            "Loss: 1.174283, Train accuracy: 0.717778, val accuracy: 0.700000\n",
            "Loss: 1.040536, Train accuracy: 0.688000, val accuracy: 0.668000\n",
            "Loss: 0.975510, Train accuracy: 0.736333, val accuracy: 0.700000\n",
            "Loss: 1.365885, Train accuracy: 0.747667, val accuracy: 0.701000\n",
            "Loss: 1.208589, Train accuracy: 0.768667, val accuracy: 0.717000\n",
            "Loss: 0.859793, Train accuracy: 0.760556, val accuracy: 0.714000\n",
            "Loss: 1.097226, Train accuracy: 0.768667, val accuracy: 0.726000\n",
            "Loss: 1.192965, Train accuracy: 0.772333, val accuracy: 0.718000\n",
            "Loss: 1.198730, Train accuracy: 0.784111, val accuracy: 0.747000\n",
            "Loss: 0.914436, Train accuracy: 0.792667, val accuracy: 0.739000\n",
            "Loss: 1.170468, Train accuracy: 0.796222, val accuracy: 0.737000\n",
            "Loss: 0.705561, Train accuracy: 0.800444, val accuracy: 0.746000\n",
            "Loss: 0.971364, Train accuracy: 0.798889, val accuracy: 0.734000\n",
            "Loss: 0.800357, Train accuracy: 0.820222, val accuracy: 0.753000\n",
            "Loss: 0.724133, Train accuracy: 0.817889, val accuracy: 0.740000\n",
            "Loss: 0.826173, Train accuracy: 0.818111, val accuracy: 0.742000\n",
            "Loss: 1.095616, Train accuracy: 0.833889, val accuracy: 0.744000\n",
            "Loss: 0.933484, Train accuracy: 0.821000, val accuracy: 0.752000\n",
            "Loss: 0.992080, Train accuracy: 0.838667, val accuracy: 0.752000\n",
            "Loss: 1.001430, Train accuracy: 0.847444, val accuracy: 0.761000\n",
            "Loss: 0.775397, Train accuracy: 0.830889, val accuracy: 0.749000\n",
            "Loss: 0.788105, Train accuracy: 0.849000, val accuracy: 0.754000\n",
            "Loss: 0.767440, Train accuracy: 0.848667, val accuracy: 0.752000\n",
            "Loss: 0.845929, Train accuracy: 0.862778, val accuracy: 0.751000\n",
            "Loss: 0.752081, Train accuracy: 0.863000, val accuracy: 0.765000\n",
            "Loss: 0.919368, Train accuracy: 0.866111, val accuracy: 0.761000\n",
            "Loss: 0.800109, Train accuracy: 0.869667, val accuracy: 0.766000\n",
            "Loss: 0.933089, Train accuracy: 0.872111, val accuracy: 0.755000\n",
            "Loss: 0.598017, Train accuracy: 0.868111, val accuracy: 0.755000\n",
            "Loss: 0.848244, Train accuracy: 0.877444, val accuracy: 0.754000\n",
            "Loss: 1.678023, Train accuracy: 0.884667, val accuracy: 0.756000\n",
            "Loss: 0.700116, Train accuracy: 0.879444, val accuracy: 0.758000\n",
            "Loss: 0.663351, Train accuracy: 0.875556, val accuracy: 0.756000\n",
            "Loss: 0.678006, Train accuracy: 0.882222, val accuracy: 0.768000\n",
            "Loss: 0.898350, Train accuracy: 0.884222, val accuracy: 0.757000\n",
            "Loss: 0.847776, Train accuracy: 0.898667, val accuracy: 0.769000\n",
            "Loss: 0.724269, Train accuracy: 0.892111, val accuracy: 0.774000\n",
            "Loss: 0.807168, Train accuracy: 0.889222, val accuracy: 0.765000\n",
            "Loss: 0.769580, Train accuracy: 0.896667, val accuracy: 0.768000\n",
            "Loss: 0.694833, Train accuracy: 0.905667, val accuracy: 0.773000\n",
            "Loss: 0.799794, Train accuracy: 0.899333, val accuracy: 0.767000\n",
            "Loss: 0.802375, Train accuracy: 0.905667, val accuracy: 0.768000\n",
            "Loss: 0.818710, Train accuracy: 0.898778, val accuracy: 0.762000\n",
            "Loss: 0.795345, Train accuracy: 0.904444, val accuracy: 0.770000\n",
            "Loss: 0.653036, Train accuracy: 0.888111, val accuracy: 0.751000\n",
            "Loss: 0.923141, Train accuracy: 0.911778, val accuracy: 0.767000\n",
            "Loss: 0.748469, Train accuracy: 0.910222, val accuracy: 0.771000\n",
            "Loss: 0.717812, Train accuracy: 0.908889, val accuracy: 0.777000\n",
            "Loss: 0.791083, Train accuracy: 0.902778, val accuracy: 0.766000\n",
            "Loss: 0.722315, Train accuracy: 0.913000, val accuracy: 0.770000\n",
            "Loss: 0.594777, Train accuracy: 0.914444, val accuracy: 0.777000\n",
            "Loss: 0.782985, Train accuracy: 0.917111, val accuracy: 0.764000\n",
            "Loss: 0.576013, Train accuracy: 0.909444, val accuracy: 0.778000\n",
            "Loss: 0.707951, Train accuracy: 0.918000, val accuracy: 0.774000\n",
            "Loss: 0.772443, Train accuracy: 0.905222, val accuracy: 0.769000\n",
            "Loss: 0.679756, Train accuracy: 0.922667, val accuracy: 0.773000\n",
            "Loss: 0.897611, Train accuracy: 0.929444, val accuracy: 0.767000\n",
            "Loss: 0.691673, Train accuracy: 0.923556, val accuracy: 0.772000\n",
            "Loss: 0.795304, Train accuracy: 0.925889, val accuracy: 0.779000\n",
            "Loss: 0.771472, Train accuracy: 0.913556, val accuracy: 0.768000\n",
            "Loss: 0.577967, Train accuracy: 0.913556, val accuracy: 0.769000\n",
            "Loss: 0.757400, Train accuracy: 0.919222, val accuracy: 0.776000\n",
            "Loss: 0.774866, Train accuracy: 0.935000, val accuracy: 0.783000\n",
            "Loss: 0.864134, Train accuracy: 0.919000, val accuracy: 0.771000\n",
            "Loss: 0.877649, Train accuracy: 0.924667, val accuracy: 0.775000\n",
            "Loss: 0.862950, Train accuracy: 0.928444, val accuracy: 0.779000\n",
            "Loss: 0.755855, Train accuracy: 0.934222, val accuracy: 0.783000\n",
            "Loss: 0.637180, Train accuracy: 0.920778, val accuracy: 0.777000\n",
            "Loss: 0.919677, Train accuracy: 0.935222, val accuracy: 0.779000\n",
            "Loss: 0.683526, Train accuracy: 0.919889, val accuracy: 0.763000\n",
            "Loss: 0.823486, Train accuracy: 0.928778, val accuracy: 0.779000\n",
            "Loss: 0.700812, Train accuracy: 0.915889, val accuracy: 0.758000\n",
            "Loss: 0.764255, Train accuracy: 0.934556, val accuracy: 0.774000\n",
            "Loss: 0.879134, Train accuracy: 0.933333, val accuracy: 0.779000\n",
            "Loss: 0.633709, Train accuracy: 0.933556, val accuracy: 0.778000\n",
            "Loss: 0.725083, Train accuracy: 0.927889, val accuracy: 0.781000\n",
            "Loss: 0.728342, Train accuracy: 0.920889, val accuracy: 0.762000\n",
            "Loss: 0.613830, Train accuracy: 0.945000, val accuracy: 0.783000\n",
            "Loss: 0.682511, Train accuracy: 0.940000, val accuracy: 0.783000\n",
            "Loss: 0.615238, Train accuracy: 0.922556, val accuracy: 0.775000\n",
            "Loss: 0.762780, Train accuracy: 0.935222, val accuracy: 0.778000\n",
            "Loss: 0.727308, Train accuracy: 0.938889, val accuracy: 0.778000\n",
            "Loss: 0.655953, Train accuracy: 0.942667, val accuracy: 0.785000\n",
            "Loss: 0.557043, Train accuracy: 0.935778, val accuracy: 0.771000\n",
            "Loss: 0.801524, Train accuracy: 0.938667, val accuracy: 0.780000\n",
            "Loss: 0.752820, Train accuracy: 0.940444, val accuracy: 0.779000\n",
            "Loss: 0.821281, Train accuracy: 0.944778, val accuracy: 0.783000\n",
            "Loss: 0.604547, Train accuracy: 0.935444, val accuracy: 0.775000\n",
            "Loss: 0.855035, Train accuracy: 0.933333, val accuracy: 0.767000\n",
            "Loss: 0.594571, Train accuracy: 0.942667, val accuracy: 0.784000\n",
            "Loss: 0.702799, Train accuracy: 0.944111, val accuracy: 0.784000\n",
            "Loss: 0.744528, Train accuracy: 0.940556, val accuracy: 0.779000\n",
            "Loss: 0.905234, Train accuracy: 0.940222, val accuracy: 0.788000\n",
            "Loss: 0.779480, Train accuracy: 0.935667, val accuracy: 0.775000\n",
            "Loss: 0.793156, Train accuracy: 0.936778, val accuracy: 0.775000\n",
            "Loss: 0.641352, Train accuracy: 0.945778, val accuracy: 0.780000\n",
            "Loss: 0.617164, Train accuracy: 0.941556, val accuracy: 0.787000\n",
            "Loss: 0.754987, Train accuracy: 0.941667, val accuracy: 0.777000\n",
            "Loss: 0.670568, Train accuracy: 0.945556, val accuracy: 0.781000\n",
            "Loss: 0.733150, Train accuracy: 0.944556, val accuracy: 0.785000\n",
            "Loss: 0.700929, Train accuracy: 0.941222, val accuracy: 0.781000\n",
            "Loss: 0.612181, Train accuracy: 0.947111, val accuracy: 0.790000\n",
            "Loss: 0.645002, Train accuracy: 0.952667, val accuracy: 0.788000\n",
            "Loss: 0.629227, Train accuracy: 0.946333, val accuracy: 0.784000\n",
            "Loss: 0.696479, Train accuracy: 0.941667, val accuracy: 0.781000\n",
            "Loss: 0.667291, Train accuracy: 0.940111, val accuracy: 0.778000\n",
            "Loss: 0.734105, Train accuracy: 0.949000, val accuracy: 0.783000\n",
            "reg=0.001, learning_rate=0.01, decay=0.999, hl_size=256\n",
            "best validation accuracy achieved: 0.783000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8Rh-EU5d9H9Y",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 461
        },
        "outputId": "08b80e9f-9179-4772-ee0c-4cfe64d1a7ca"
      },
      "source": [
        "plt.figure(figsize=(15, 7))\n",
        "plt.subplot(211)\n",
        "plt.title(\"Loss\")\n",
        "plt.plot(loss_history)\n",
        "plt.subplot(212)\n",
        "plt.title(\"Train/validation accuracy\")\n",
        "plt.plot(train_history)\n",
        "plt.plot(val_history)"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[<matplotlib.lines.Line2D at 0x7f533c3f2f90>]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 34
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 1080x504 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oMvG-_1s9H9Z"
      },
      "source": [
        "# Как обычно, посмотрим, как наша лучшая модель работает на тестовых данных"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aZVxS_Jw9H9b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3788ce6e-ebe2-46ba-b1bb-265291c4b98e"
      },
      "source": [
        "test_pred = best_classifier.predict(test_X)\n",
        "test_accuracy = multiclass_accuracy(test_pred, test_y)\n",
        "print('Neural net test set accuracy: %f' % (test_accuracy, ))"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Neural net test set accuracy: 0.774000\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}